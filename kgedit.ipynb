{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:52:39.979492Z",
     "iopub.status.busy": "2025-06-23T23:52:39.979220Z",
     "iopub.status.idle": "2025-06-23T23:53:46.964760Z",
     "shell.execute_reply": "2025-06-23T23:53:46.963700Z",
     "shell.execute_reply.started": "2025-06-23T23:52:39.979472Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyEdit'...\n",
      "remote: Enumerating objects: 8523, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 8523 (delta 7), reused 3 (delta 3), pack-reused 8502 (from 2)\u001b[K\n",
      "Receiving objects: 100% (8523/8523), 85.85 MiB | 42.24 MiB/s, done.\n",
      "Resolving deltas: 100% (5412/5412), done.\n",
      "/kaggle/working/EasyEdit\n",
      "Note: switching to 'f0ec5eb3ef6a09f637fa5344667a78f38fbeb05d'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at f0ec5eb fix bug in notebooks and caa_data.py\n",
      "Collecting datasets==1.18.3 (from -r colab_requirements.txt (line 1))\n",
      "  Downloading datasets-1.18.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting gpustat==1.1 (from -r colab_requirements.txt (line 2))\n",
      "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting hydra-core==1.1.1 (from -r colab_requirements.txt (line 3))\n",
      "  Downloading hydra_core-1.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting higher==0.2.1 (from -r colab_requirements.txt (line 4))\n",
      "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting omegaconf==2.1.1 (from -r colab_requirements.txt (line 5))\n",
      "  Downloading omegaconf-2.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting sentence-transformers==2.2.2 (from -r colab_requirements.txt (line 6))\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting openai==0.27.9 (from -r colab_requirements.txt (line 7))\n",
      "  Downloading openai-0.27.9-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting peft==0.7.1 (from -r colab_requirements.txt (line 8))\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting timm==0.9.7 (from -r colab_requirements.txt (line 9))\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting iopath==0.1.10 (from -r colab_requirements.txt (line 10))\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting fairscale==0.4.13 (from -r colab_requirements.txt (line 11))\n",
      "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat==1.1->-r colab_requirements.txt (line 2)) (12.570.86)\n",
      "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat==1.1->-r colab_requirements.txt (line 2)) (5.9.5)\n",
      "Collecting blessed>=1.17.1 (from gpustat==1.1->-r colab_requirements.txt (line 2))\n",
      "  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core==1.1.1->-r colab_requirements.txt (line 3))\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from higher==0.2.1->-r colab_requirements.txt (line 4)) (2.5.1+cu121)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.1.1->-r colab_requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (4.47.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (0.20.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (3.2.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r colab_requirements.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r colab_requirements.txt (line 8)) (0.4.5)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath==0.1.10->-r colab_requirements.txt (line 10)) (4.12.2)\n",
      "Collecting portalocker (from iopath==0.1.10->-r colab_requirements.txt (line 10))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat==1.1->-r colab_requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.2.0)\n",
      "Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: gpustat, sentence-transformers, iopath, fairscale, antlr4-python3-runtime\n",
      "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gpustat: filename=gpustat-1.1-py3-none-any.whl size=26530 sha256=5cefaf34f5b24da5001c73cd29436fe86072ed4be2153e262753ff3aa1350d3d\n",
      "  Stored in directory: /root/.cache/pip/wheels/ee/d0/2c/1e02440645c2318ba03aea99993a44a9108dc8f74de0bd370b\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=187a12b9ad65742bb0feb50f605a0b36b6a9d8ea5f58cb0658cbfb8d6d4a9d2a\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=162263f61cd5fff8aa6be84cb0309e6cd014eb0926446b538ce1f2a9b5ddb58c\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332208 sha256=1f932bc85f918bbffa6831bf36dcb691ae6436e2f438f723a3301ed2e40d72a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=285953af980426d9f2399276e8c1284fe056bb9d94aa4e9af9282621de06a335\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
      "Successfully built gpustat sentence-transformers iopath fairscale antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, blessed, iopath, hydra-core, gpustat, higher, openai, timm, sentence-transformers, peft, fairscale, datasets\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
      "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.3.0\n",
      "    Uninstalling omegaconf-2.3.0:\n",
      "      Successfully uninstalled omegaconf-2.3.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.12\n",
      "    Uninstalling timm-1.0.12:\n",
      "      Successfully uninstalled timm-1.0.12\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.3.1\n",
      "    Uninstalling sentence-transformers-3.3.1:\n",
      "      Successfully uninstalled sentence-transformers-3.3.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.14.0\n",
      "    Uninstalling peft-0.14.0:\n",
      "      Successfully uninstalled peft-0.14.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.1\n",
      "    Uninstalling datasets-3.3.1:\n",
      "      Successfully uninstalled datasets-3.3.1\n",
      "Successfully installed antlr4-python3-runtime-4.8 blessed-1.21.0 datasets-1.18.3 fairscale-0.4.13 gpustat-1.1 higher-0.2.1 hydra-core-1.1.1 iopath-0.1.10 omegaconf-2.1.1 openai-0.27.9 peft-0.7.1 portalocker-3.2.0 sentence-transformers-2.2.2 timm-0.9.7\n",
      "Collecting qwen-vl-utils\n",
      "  Downloading qwen_vl_utils-0.0.11-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Downloading av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (24.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (11.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (2025.1.31)\n",
      "Downloading qwen_vl_utils-0.0.11-py3-none-any.whl (7.6 kB)\n",
      "Downloading av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: av, qwen-vl-utils\n",
      "Successfully installed av-14.4.0 qwen-vl-utils-0.0.11\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.4.0)\n",
      "Collecting openai==1.2.0\n",
      "  Downloading openai-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (2.11.0a2)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.0) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.2.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (2.29.0)\n",
      "Downloading openai-1.2.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.9\n",
      "    Uninstalling openai-0.27.9:\n",
      "      Successfully uninstalled openai-0.27.9\n",
      "Successfully installed openai-1.2.0\n",
      "Collecting zhipuai\n",
      "  Downloading zhipuai-2.1.5.20250611-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cachetools>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (5.5.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.11.0a2)\n",
      "Requirement already satisfied: pydantic-core>=2.14.6 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.29.0)\n",
      "Collecting pyjwt<2.9.0,>=2.8.0 (from zhipuai)\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->zhipuai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->zhipuai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->zhipuai) (1.2.2)\n",
      "Downloading zhipuai-2.1.5.20250611-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyjwt, zhipuai\n",
      "  Attempting uninstall: pyjwt\n",
      "    Found existing installation: PyJWT 2.10.1\n",
      "    Uninstalling PyJWT-2.10.1:\n",
      "      Successfully uninstalled PyJWT-2.10.1\n",
      "Successfully installed pyjwt-2.8.0 zhipuai-2.1.5.20250611\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (24.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=b749d6d6e2244ba2bf1812459b4a99ef7e80ee347c7c9d4205b84b9b033c6a49\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.8\n",
      "    Uninstalling antlr4-python3-runtime-4.8:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.8\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.1.1\n",
      "    Uninstalling omegaconf-2.1.1:\n",
      "      Successfully uninstalled omegaconf-2.1.1\n",
      "  Attempting uninstall: hydra-core\n",
      "    Found existing installation: hydra-core 1.1.1\n",
      "    Uninstalling hydra-core-1.1.1:\n",
      "      Successfully uninstalled hydra-core-1.1.1\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n",
      "Collecting sentence_transformers==3.2.1\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (0.29.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==3.2.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==3.2.1) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==3.2.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers==3.2.1) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==3.2.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==3.2.1) (3.5.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers==3.2.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.2.0)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\n",
      "  Attempting uninstall: sentence_transformers\n",
      "    Found existing installation: sentence-transformers 2.2.2\n",
      "    Uninstalling sentence-transformers-2.2.2:\n",
      "      Successfully uninstalled sentence-transformers-2.2.2\n",
      "Successfully installed sentence_transformers-3.2.1\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (1.18.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.18.3\n",
      "    Uninstalling datasets-1.18.3:\n",
      "      Successfully uninstalled datasets-1.18.3\n",
      "Successfully installed datasets-3.6.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/zjunlp/EasyEdit\n",
    "%cd EasyEdit\n",
    "!git checkout f0ec5eb3ef6a09f637fa5344667a78f38fbeb05d\n",
    "!pip install -r colab_requirements.txt\n",
    "!pip install qwen-vl-utils\n",
    "!pip install av\n",
    "!pip install openai==1.2.0\n",
    "!pip install zhipuai\n",
    "!pip install hydra-core -U\n",
    "!pip install sentence_transformers==3.2.1\n",
    "!pip install -U datasets\n",
    "!pip install -q sparqlwrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:53:46.966341Z",
     "iopub.status.busy": "2025-06-23T23:53:46.966020Z",
     "iopub.status.idle": "2025-06-23T23:53:54.640391Z",
     "shell.execute_reply": "2025-06-23T23:53:54.639756Z",
     "shell.execute_reply.started": "2025-06-23T23:53:46.966310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import os.path as path\n",
    "import numpy as np\n",
    "from easyeditor import KnowEditDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:12.446040Z",
     "iopub.status.busy": "2025-06-23T23:54:12.445831Z",
     "iopub.status.idle": "2025-06-23T23:54:12.450917Z",
     "shell.execute_reply": "2025-06-23T23:54:12.450080Z",
     "shell.execute_reply.started": "2025-06-23T23:54:12.446023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='kgedit.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('KGEdit')\n",
    "\n",
    "handler = logging.FileHandler('kgedit.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('[%(asctime)s] %(name)s - %(levelname)s: %(message)s', \n",
    "                               datefmt='%H:%M:%S')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# logger = logging.getLogger('KGEdit')\n",
    "# logger.setLevel(logging.INFO)\n",
    "# handler = logging.StreamHandler(sys.stdout)\n",
    "# handler.setLevel(logging.INFO)\n",
    "# formatter = logging.Formatter('[%(asctime)s] %(name)s - %(levelname)s: %(message)s', \n",
    "#                               datefmt='%H:%M:%S')\n",
    "# handler.setFormatter(formatter)\n",
    "# logger.addHandler(handler)\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:12.452377Z",
     "iopub.status.busy": "2025-06-23T23:54:12.452065Z",
     "iopub.status.idle": "2025-06-23T23:54:12.479059Z",
     "shell.execute_reply": "2025-06-23T23:54:12.478344Z",
     "shell.execute_reply.started": "2025-06-23T23:54:12.452347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "USE_CHAT_TEMPLATE = False\n",
    "MODEL = 'qwen2.5-1.5b-instruct' # qwen2.5-1.5b-instruct qwen2.5-3b-instruct\n",
    "if 'instruct' in MODEL:\n",
    "    USE_CHAT_TEMPLATE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:12.479969Z",
     "iopub.status.busy": "2025-06-23T23:54:12.479709Z",
     "iopub.status.idle": "2025-06-23T23:54:14.196674Z",
     "shell.execute_reply": "2025-06-23T23:54:14.195868Z",
     "shell.execute_reply.started": "2025-06-23T23:54:12.479938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 14 files:   0%|                                 | 0/14 [00:00<?, ?it/s]Downloading 'benchmark/ZsRE/ZsRE-test-all.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/ZsRE/o5LGPIEg1EjkOuxgs9IOBWsg4QA=.759a1d0c7b87d4d1612f655dbbac4cb8262b2948.incomplete'\n",
      "Downloading 'benchmark/Convsent/blender_val.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/Convsent/fx9F75P0rVvXKMjHxLzGlzK0N6E=.98b69a13f438a25ac5f0d56dfc6e4f0150d30836.incomplete'\n",
      "Downloading 'benchmark/WikiBio/wikibio-train-all.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/WikiBio/8rs6DLxZ_QZwK6fLwCiUot6Qc1I=.fd11768426fe24820d7b99e11acd4db7e2277721.incomplete'\n",
      "Downloading 'benchmark/WikiBio/wikibio-test-all.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/WikiBio/Huumcl7_kkObDM_jO3_CKhFJt00=.c4503c3ae7d09e8d88ab522162db7c9dc7145d68.incomplete'\n",
      "Downloading '.gitattributes' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.98ddd117e08ef652502840374e3c926421b9010d.incomplete'\n",
      "Downloading 'README.md' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.f690f31c4f3bd8334ec8a032a30b36784414ed9b.incomplete'\n",
      "\n",
      "wikibio-train-all.json:   0%|                        | 0.00/454k [00:00<?, ?B/s]\u001b[ADownloading 'benchmark/Convsent/blender_train.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/Convsent/i-mF4aBv7Lkq4rpoc_Thnmm6zL8=.0ebd6d70e31127e3c93dd2f235bc4963e9b3bf4e6219a96c5aa91beab85aa01c.incomplete'\n",
      "\n",
      "\n",
      "blender_val.json:   0%|                             | 0.00/1.33M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "wikibio-test-all.json:   0%|                         | 0.00/310k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      ".gitattributes: 100%|██████████████████████| 2.38k/2.38k [00:00<00:00, 25.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/.gitattributes\n",
      "Fetching 14 files:   7%|█▊                       | 1/14 [00:00<00:03,  4.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "README.md: 100%|███████████████████████████| 23.8k/23.8k [00:00<00:00, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/README.md\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_train.json:   0%|                           | 0.00/24.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ZsRE-test-all.json:   0%|                           | 0.00/1.52M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'benchmark/trivia/trivia_qa_test.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/trivia/hsqfUMul1hcF0Rkn77-lciu0xCs=.bb182169de846c83a46195102df05f27a6027bfc.incomplete'\n",
      "Downloading 'benchmark/trivia/trivia_qa_train.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/trivia/Ux66o4_aGuv2cMX6Oty__PK66_Q=.a377bf2206b746e4da285adb6cf9063f64f69421.incomplete'\n",
      "\n",
      "wikibio-train-all.json: 100%|████████████████| 454k/454k [00:00<00:00, 3.29MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-train-all.json\n",
      "\n",
      "\n",
      "\n",
      "wikibio-test-all.json: 100%|█████████████████| 310k/310k [00:00<00:00, 2.28MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-test-all.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_train.json: 100%|███████████████████| 24.0M/24.0M [00:00<00:00, 174MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/Convsent/blender_train.json\n",
      "\n",
      "trivia_qa_test.json:   0%|                          | 0.00/95.1k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "blender_val.json: 100%|████████████████████| 1.33M/1.33M [00:00<00:00, 6.77MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/Convsent/blender_val.json\n",
      "\n",
      "\n",
      "trivia_qa_train.json: 100%|████████████████| 87.0k/87.0k [00:00<00:00, 13.7MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/trivia/trivia_qa_train.json\n",
      "Downloading 'benchmark/wiki_counterfact/test_cf.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_counterfact/MFXtFn6NK8VL1PB-H6pzAHNNtJk=.8b209c5ecdd63de81e3a289356864c7269065654.incomplete'\n",
      "trivia_qa_test.json: 100%|█████████████████| 95.1k/95.1k [00:00<00:00, 1.48MB/s]\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/trivia/trivia_qa_test.json\n",
      "Downloading 'benchmark/wiki_recent/recent_test.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_recent/2Oq5wtIJdPWr5VpLBoGf8aOq-7s=.ff99392090c38c986a71b8e68457ab3b540fb8ae.incomplete'\n",
      "Downloading 'benchmark/wiki_counterfact/train_cf.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_counterfact/oHHCcKqsl_RwMeOewKIysf-yTIY=.a1cde7b1dc12053bbfa70841e3e8acc53b9af02a.incomplete'\n",
      "\n",
      "test_cf.json:   0%|                                 | 0.00/2.92M [00:00<?, ?B/s]\u001b[ADownloading 'benchmark/Convsent/blender_test.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/Convsent/uG92iOpRw2ySGVaHQtkdFcCzJmY=.cf80f02b519e2fd910c59b216aa8bc81a50d8b18.incomplete'\n",
      "Downloading 'benchmark/wiki_recent/recent_train.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_recent/rZ6nU74FMB4p4evUjqJWa5ZVFBk=.d17f09a23a43b4385b5f030a03002c09270dbf85.incomplete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ZsRE-test-all.json: 100%|██████████████████| 1.52M/1.52M [00:00<00:00, 5.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ZsRE-test-all.json: 100%|██████████████████| 1.52M/1.52M [00:00<00:00, 5.86MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/ZsRE/ZsRE-test-all.json\n",
      "\n",
      "\n",
      "\n",
      "train_cf.json:   0%|                                | 0.00/1.39M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "recent_train.json:   0%|                            | 0.00/1.83M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_test.json:   0%|                            | 0.00/1.33M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "recent_train.json: 100%|███████████████████| 1.83M/1.83M [00:00<00:00, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_train.json\n",
      "\n",
      "test_cf.json: 100%|████████████████████████| 2.92M/2.92M [00:00<00:00, 11.4MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/test_cf.json\n",
      "\n",
      "\n",
      "recent_test.json: 100%|████████████████████| 4.08M/4.08M [00:00<00:00, 15.5MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_test.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_test.json: 100%|███████████████████| 1.33M/1.33M [00:00<00:00, 6.50MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/Convsent/blender_test.json\n",
      "Fetching 14 files:  21%|█████▎                   | 3/14 [00:00<00:03,  3.44it/s]\n",
      "\n",
      "\n",
      "train_cf.json: 100%|███████████████████████| 1.39M/1.39M [00:00<00:00, 5.36MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/train_cf.json\n",
      "Fetching 14 files: 100%|████████████████████████| 14/14 [00:00<00:00, 16.11it/s]\n",
      "/kaggle/working/EasyEdit/data\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "# %mkdir data\n",
    "!huggingface-cli download zjunlp/KnowEdit --repo-type dataset --local-dir /kaggle/working/EasyEdit/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.197905Z",
     "iopub.status.busy": "2025-06-23T23:54:14.197588Z",
     "iopub.status.idle": "2025-06-23T23:54:14.207213Z",
     "shell.execute_reply": "2025-06-23T23:54:14.206528Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.197882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    def __init__(self, use_wiki=True, endpoint='https://query.wikidata.org/sparql'):\n",
    "        self.logger = logging.getLogger('KGEdit.KnowledgeGraph')\n",
    "        self.logger.info('Initialising knowledge graph...')\n",
    "        self.endpoint = endpoint\n",
    "        self.local_facts = {}\n",
    "        self.cache = {}\n",
    "        self.use_wiki = use_wiki\n",
    "        self.logger.info('Knowledge graph initialised!')\n",
    "\n",
    "    def add_fact(self, subject, relation, obj):\n",
    "        self.logger.info(f'Adding fact: Subject: {subject}, Relation: {relation}, Object: {obj}')\n",
    "        key = f'{subject.lower()}|{relation.lower()}'\n",
    "        self.local_facts[key] = obj\n",
    "\n",
    "    def query(self, subject, relation):\n",
    "        # Checking local facts first\n",
    "        self.logger.info(f'Checking local facts! Subject: {subject}, relation: {relation}')\n",
    "        key = f'{subject.lower()}|{relation.lower()}'\n",
    "        if key in self.cache:\n",
    "            self.logger.info(f'Cached fact found: {self.cache[key]}')\n",
    "            return self.cache[key]\n",
    "        \n",
    "        if key in self.local_facts:\n",
    "            self.logger.info(f'Local fact found: {self.local_facts[key]}')\n",
    "            return self.local_facts[key]\n",
    "\n",
    "        if self.use_wiki:\n",
    "        # If nothing was found, send request to Wikidata\n",
    "            self.logger.info(f'Querying Wikidata: {subject} - {relation}')\n",
    "    \n",
    "            try:\n",
    "                sparql = SPARQLWrapper(self.endpoint)\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                query = f\"\"\"\n",
    "                SELECT ?object ?objectLabel WHERE {{\n",
    "                    SERVICE wikibase:mwapi {{\n",
    "                        bd:serviceParam wikibase:api \"EntitySearch\";\n",
    "                            wikibase:endpoint \"www.wikidata.org\";\n",
    "                            mwapi:search \"{subject}\";\n",
    "                            mwapi:language \"en\".\n",
    "                        ?subject wikibase:apiOutputItem mwapi:item.\n",
    "                    }}\n",
    "                    SERVICE wikibase:mwapi {{\n",
    "                        bd:serviceParam wikibase:api \"EntitySearch\";\n",
    "                            wikibase:endpoint \"www.wikidata.org\";\n",
    "                            mwapi:search \"{relation}\";\n",
    "                            mwapi:language \"en\";\n",
    "                            mwapi:type \"property\".\n",
    "                        ?property wikibase:apiOutputItem mwapi:item.\n",
    "                    }}\n",
    "                    ?property wikibase:directClaim ?wdtProperty.\n",
    "                    ?subject ?wdtProperty ?object.\n",
    "                    OPTIONAL {{\n",
    "                        ?object rdfs:label ?objectLabel.\n",
    "                        FILTER(LANG(?objectLabel) = \"en\")\n",
    "                    }}\n",
    "                }}\n",
    "                LIMIT 1\n",
    "                \"\"\"\n",
    "                sparql.setQuery(query)\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "    \n",
    "                if results[\"results\"][\"bindings\"]:\n",
    "                    result = results[\"results\"][\"bindings\"][0]\n",
    "                    obj = result.get(\"object\", {}).get(\"value\", \"\")\n",
    "                    label = result.get(\"objectLabel\", {}).get(\"value\", \"\")\n",
    "                    self.cache[key] = label\n",
    "                    self.logger.info(f'Wikidata result: {label}')\n",
    "                    return label if label else obj.split(\"/\")[-1]\n",
    "            except Exception as e:\n",
    "                self.logger.error(f'Wikidata query failed with exception: {str(e)}')\n",
    "                self.cache[key] = None\n",
    "\n",
    "        self.logger.info('Nothing found!')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.209670Z",
     "iopub.status.busy": "2025-06-23T23:54:14.209387Z",
     "iopub.status.idle": "2025-06-23T23:54:14.226111Z",
     "shell.execute_reply": "2025-06-23T23:54:14.225249Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.209641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NLIValidator:\n",
    "    def __init__(self, model_name=\"roberta-large-mnli\", device=\"cuda:0\"):\n",
    "        self.logger = logging.getLogger('KGEdit.NLIValidator')\n",
    "        self.logger.info('Initialising NLIValidator...')\n",
    "        self.device = device\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.contradiction_id = 0\n",
    "        self.logger.info('NLIValidator initialised!')\n",
    "\n",
    "    def check_contradiction(self, text1, text2):\n",
    "        self.logger.info(f'Checking contradiction: \"{text1}\" vs \"{text2}\"')\n",
    "        inputs = self.tokenizer(\n",
    "            text1, \n",
    "            text2, \n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_label = logits.argmax(dim=1).item()\n",
    "\n",
    "        is_contradiction = predicted_label == self.contradiction_id\n",
    "        self.logger.info(f'Contradiction check result: {is_contradiction}')\n",
    "        return is_contradiction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.227730Z",
     "iopub.status.busy": "2025-06-23T23:54:14.227420Z",
     "iopub.status.idle": "2025-06-23T23:54:14.250485Z",
     "shell.execute_reply": "2025-06-23T23:54:14.249773Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.227710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KGEditor:\n",
    "    def __init__(self, model_name: str, use_nli: bool = False, use_chat_template: bool = False, use_wiki: bool = True):\n",
    "        self.logger = logging.getLogger('KGEdit.KGEditor')\n",
    "        self.logger.info('Initialising KGEditor...')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.kg = KnowledgeGraph(use_wiki=use_wiki)\n",
    "        self.nli_validator = NLIValidator(device=str(self.model.device)) if use_nli else None\n",
    "        self.use_chat_template = use_chat_template\n",
    "        self.logger.info('KGEditor initialisation complete!')\n",
    "\n",
    "    def edit_knowledge(self, record):\n",
    "        self.logger.info(f'Editing knowledge: Subject: {record[\"subject\"]}; Object: {record[\"target_new\"]}; Prompt: {record[\"prompt\"]}')\n",
    "        if self.use_chat_template:\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': 'You are a helpful assistant. Given subject and object, extract relation between them from the given phrase, so that they could form a knowledge triplet.'},\n",
    "                {'role': 'user', 'content': 'Subject: Albert Einstein\\nObject: Ulm\\nPhrase: The place of birth of Albert Einstein is\\nOutput:'},\n",
    "                {'role': 'assistant', 'content': 'place of birth'},\n",
    "                {\"role\": \"user\", \"content\": \"Subject: France\\nObject: Emmanuel Macron\\nPhrase: The current head of state of France is\\nOutput:\"},\n",
    "                {'role': 'assistant', 'content': 'head of state'},\n",
    "                {\"role\": \"user\", \"content\": f'Subject: {record[\"subject\"]}\\nObject: {record[\"target_new\"]}\\nPhrase: {record[\"prompt\"]}\\nOutput:'}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "            Given subject and object, extract relation between them from the given phrase, so that they could form a knowledge triplet.\n",
    "            Return ONLY relation as a short phrase, without any additional symbols.\n",
    "    \n",
    "            Examples:\n",
    "            Subject: Albert Einstein\n",
    "            Object: Ulm\n",
    "            Phrase: The place of birth of Albert Einstein is\n",
    "            Output: place of birth\n",
    "    \n",
    "            Subject: France\n",
    "            Object: Emmanuel Macron\n",
    "            Phrase: The current head of state of France is\n",
    "            Output: head of state\n",
    "    \n",
    "            Now extract from this data:\n",
    "            Subject: {record[\"subject\"]}\n",
    "            Object: {record[\"target_new\"]}\n",
    "            Phrase: {record[\"prompt\"]}\n",
    "            Output:\n",
    "            \"\"\"\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "            # temperature=0.1\n",
    "        )\n",
    "        outputs = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)]\n",
    "        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        self.logger.info(f'Model extracted relation from sentence: \"{response}\"')\n",
    "        self.kg.add_fact(record[\"subject\"].lower(), response.lower(), record[\"target_new\"].lower())\n",
    "        self.logger.info('Knowledge added successfully!')\n",
    "\n",
    "    def process(self, query):\n",
    "        self.logger.info(f'Processing query: \"{query}\"\"')\n",
    "        entities_relations = self._extract_entities_relations(query)\n",
    "        facts = self._extract_facts(entities_relations)\n",
    "        final_answer = self._generate_response(query, facts)\n",
    "        self.logger.info(f'Generation complete! Final answer: \"{final_answer}\"')\n",
    "        return final_answer\n",
    "\n",
    "    def _extract_entities_relations(self, query):\n",
    "        self.logger.info(f'Extracting entities/relations from: {query}')      \n",
    "        if self.use_chat_template:\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': 'You are a helpful assistant. You task is to Extract ONLY the main entities and relations from the query. Return EXACTLY ONE line in the format: [ENTITY|RELATION]'},\n",
    "                {\"role\": \"user\", \"content\": f'Where was Albert Einstein born?'},\n",
    "                {'role': 'assistant', 'content': '[Albert Einstein|place of birth]'},\n",
    "                {\"role\": \"user\", \"content\": f'Who is the current president of France?'},\n",
    "                {'role': 'assistant', 'content': '[France|head of state]'},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        else:\n",
    "            text = f\"\"\"\n",
    "            Extract ONLY the main entities and relations from the query.\n",
    "            Return EXACTLY ONE line in the format: [ENTITY|RELATION]\n",
    "    \n",
    "            Examples:\n",
    "            Query: Where was Albert Einstein born?\n",
    "            Output: [Albert Einstein|place of birth]\n",
    "    \n",
    "            Query: Who is the current president of France?\n",
    "            Output: [France|head of state]\n",
    "    \n",
    "            Query: What is the capital of Germany?\n",
    "            Output: [Germany|capital]\n",
    "    \n",
    "            Now extract from this query:\n",
    "            Query: {query}\n",
    "            Output:\n",
    "            \"\"\"\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=30,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "            # temperature=0.1\n",
    "        )\n",
    "        outputs = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)]\n",
    "        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        if \"|\" in response:\n",
    "            if \"[\" not in response:\n",
    "                response = '[' + response\n",
    "            if ']' not in response:\n",
    "                response = response + ']'\n",
    "\n",
    "        self.logger.info(f\"Model extraction response: {response}\")\n",
    "\n",
    "        entities_relations = []\n",
    "        if \"[\" in response and \"|\" in response and \"]\" in response:\n",
    "            try:\n",
    "                content = response.split(\"[\")[1].split(\"]\")[0]\n",
    "                parts = content.split(\"|\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    entities_relations.append((parts[0].strip(), parts[1].strip()))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to parse extraction response. Error message: {e}\")\n",
    "\n",
    "        # Fallback if no entities found\n",
    "        if not entities_relations:\n",
    "            self.logger.warning(\"No entities/relations found, using fallback\")\n",
    "            return [(query, \"unknown\")]\n",
    "        return entities_relations\n",
    "\n",
    "    def _extract_facts(self, entities_relations):\n",
    "        self.logger.info(f'Extracting facts from KG for {len(entities_relations)} entities/relations')\n",
    "        facts = []\n",
    "        for subject, relation in entities_relations:\n",
    "            fact = self.kg.query(subject, relation)\n",
    "            if fact:\n",
    "                facts.append(f\"{subject}|{relation}|{fact}\")\n",
    "        self.logger.info(f'Extracted {len(facts)} facts')\n",
    "        return facts\n",
    "\n",
    "    def _generate_response(self, query, facts):\n",
    "        self.logger.info(f'Generating response for query: \"{query}\"')\n",
    "        if self.use_chat_template:\n",
    "            if facts:\n",
    "                messages = [\n",
    "                    {'role': 'system', 'content': 'You are taking part in scientific experiment. Please, answer a given question with respect to given facts. If no facts provided or provided facts do not suffice, use your own knowledge.'},\n",
    "                    {\"role\": \"user\", \"content\": f\"Question: {query}\\nFacts: {facts}\"}\n",
    "                ]\n",
    "            else:\n",
    "                messages = [\n",
    "                    {'role': 'system', 'content': 'You are taking part in scientific experiment. Please, answer any given question.'},\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        else:\n",
    "            if facts:\n",
    "                text = f\"\"\"Please, answer a given question with respect to given facts. If provided facts do not suffice, use your own knowledge.\n",
    "                Question: {query}\n",
    "                Facts: {facts}\n",
    "                \"\"\"\n",
    "            else:\n",
    "                text = f\"\"\"Please, answer a given question.\n",
    "                Question: {query}\n",
    "                \"\"\"\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "            \n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "            # temperature=0.1\n",
    "        )\n",
    "        outputs = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)]\n",
    "        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        self.logger.info(f'Initial response: \"{response}\"')\n",
    "\n",
    "        if facts and not self._check_response(response, facts):\n",
    "            self.logger.info(\"Response needs correction based on facts\")\n",
    "        \n",
    "            correction_prompt = (\n",
    "                f\"Original query: {query}\\n\"\\\n",
    "                f\"Initial response: {response}\\n\"\\\n",
    "                f\"Verified facts: {', '.join(facts)}\\n\"\\\n",
    "                \"Please generate a corrected response:\"\n",
    "            )\n",
    "            return self._generate_response(correction_prompt, [])\n",
    "        return response\n",
    "            \n",
    "    def _check_response(self, response, facts):\n",
    "        self.logger.info(f'Checking response for correctness. Response: \"{response}\". Facts: \"{facts}\"')\n",
    "        if not facts:\n",
    "            return True\n",
    "\n",
    "        if self.nli_validator:\n",
    "            self.logger.info(\"Using NLI validator to check correctness\")\n",
    "            for fact in facts:\n",
    "                if self.nli_validator.check_contradiction(response, fact):\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            # Simple keyword check\n",
    "            return any(fact.lower().split('|')[-1] in response.lower() for fact in facts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.251472Z",
     "iopub.status.busy": "2025-06-23T23:54:14.251223Z",
     "iopub.status.idle": "2025-06-23T23:54:14.267875Z",
     "shell.execute_reply": "2025-06-23T23:54:14.267144Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.251447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def flatten_ground_truth(ground_truth):\n",
    "    flat_list = []\n",
    "    stack = [ground_truth]\n",
    "    \n",
    "    while stack:\n",
    "        current = stack.pop()\n",
    "        if isinstance(current, list):\n",
    "            stack.extend(reversed(current))\n",
    "        else:\n",
    "            flat_list.append(str(current).lower())\n",
    "    \n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.268789Z",
     "iopub.status.busy": "2025-06-23T23:54:14.268532Z",
     "iopub.status.idle": "2025-06-23T23:54:14.285598Z",
     "shell.execute_reply": "2025-06-23T23:54:14.284855Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.268770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval_kgedit(result_path: str, logger):\n",
    "    logger.info(f'Evaluating results from: {result_path}')\n",
    "    if not os.path.exists(result_path):\n",
    "        raise FileNotFoundError(f'File {result_path} not found!')\n",
    "    \n",
    "    with open(result_path, 'r') as f:\n",
    "        datas = json.load(f)\n",
    "    \n",
    "    # Edit Success (Rewrite Accuracy)\n",
    "    edit_succ_list = []\n",
    "    for data in datas:\n",
    "        if 'rewrite_acc' in data['post']:\n",
    "            edit_succ_list.extend(data['post']['rewrite_acc'])\n",
    "    edit_succ = sum(edit_succ_list) / len(edit_succ_list) * 100 if edit_succ_list else 0\n",
    "    logger.info(f'Edit Success: {edit_succ:.2f}%')\n",
    "    \n",
    "    # Portability\n",
    "    portability_scores = []\n",
    "    portability_metrics = {}\n",
    "    for data in datas:\n",
    "        if 'portability' in data['post']:\n",
    "            for key, values in data['post']['portability'].items():\n",
    "                portability_scores.extend(values)\n",
    "                if key not in portability_metrics:\n",
    "                    portability_metrics[key] = []\n",
    "                portability_metrics[key].extend(values)\n",
    "    \n",
    "    if portability_scores:\n",
    "        overall_portability = sum(portability_scores) / len(portability_scores) * 100\n",
    "        logger.info(f'Overall Portability: {overall_portability:.2f}%')\n",
    "        for key, values in portability_metrics.items():\n",
    "            avg = sum(values) / len(values) * 100 if values else 0\n",
    "            logger.info(f'  - {key}: {avg:.2f}%')\n",
    "    \n",
    "    # Locality\n",
    "    locality_scores = []\n",
    "    locality_metrics = {}\n",
    "    for data in datas:\n",
    "        if 'locality' in data['post']:\n",
    "            for key, values in data['post']['locality'].items():\n",
    "                locality_scores.extend(values)\n",
    "                if key not in locality_metrics:\n",
    "                    locality_metrics[key] = []\n",
    "                locality_metrics[key].extend(values)\n",
    "    \n",
    "    if locality_scores:\n",
    "        overall_locality = sum(locality_scores) / len(locality_scores) * 100\n",
    "        logger.info(f'Overall Locality: {overall_locality:.2f}%')\n",
    "        for key, values in locality_metrics.items():\n",
    "            avg = sum(values) / len(values) * 100 if values else 0\n",
    "            logger.info(f'  - {key}: {avg:.2f}%')\n",
    "    \n",
    "    print(\"\\n===== Final Evaluation Results =====\")\n",
    "    print(f\"Edit Success: {edit_succ:.2f}%\")\n",
    "    \n",
    "    if portability_scores:\n",
    "        print(f\"\\nOverall Portability: {overall_portability:.2f}%\")\n",
    "        for key, values in portability_metrics.items():\n",
    "            avg = sum(values) / len(values) * 100\n",
    "            print(f\"  - {key}: {avg:.2f}%\")\n",
    "    \n",
    "    if locality_scores:\n",
    "        print(f\"\\nOverall Locality: {overall_locality:.2f}%\")\n",
    "        for key, values in locality_metrics.items():\n",
    "            avg = sum(values) / len(values) * 100\n",
    "            print(f\"  - {key}: {avg:.2f}%\")\n",
    "    \n",
    "    print(\"===================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.286664Z",
     "iopub.status.busy": "2025-06-23T23:54:14.286376Z",
     "iopub.status.idle": "2025-06-23T23:54:14.307155Z",
     "shell.execute_reply": "2025-06-23T23:54:14.306386Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.286636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_kgedit(data_path, data_size, model_name, use_nli=False, use_chat_template=False, use_wiki=True, out_path='kgedit_results.json'):\n",
    "    logger = logging.getLogger('KGEdit.run')\n",
    "    logger.info('Starting KGEdit pipeline')\n",
    "    logger.info(f'Configuration: model={model_name}, use_nli={use_nli}, use_chat_template={use_chat_template}')\n",
    "    \n",
    "    kgeditor = KGEditor(model_name, use_nli, use_chat_template, use_wiki)\n",
    "    \n",
    "    dataset = KnowEditDataset(data_path, size=data_size)\n",
    "    logger.info(f'Loaded dataset with {len(dataset)} records')\n",
    "    \n",
    "    if 'counterfact' in data_path:\n",
    "        datatype = 'counterfact'\n",
    "    elif 'recent' in data_path:\n",
    "        datatype = 'recent'\n",
    "    elif 'wikibio' in data_path:\n",
    "        datatype = 'wikibio'\n",
    "    elif 'ZsRE' in data_path:\n",
    "        datatype = 'zsre'\n",
    "    else:\n",
    "        logger.warning('Unknown dataset type, assuming counterfact format')\n",
    "        datatype = 'counterfact'\n",
    "\n",
    "    logger.info(f'Detected dataset type: {datatype}')\n",
    "\n",
    "    logger.info('Applying knowledge edits')\n",
    "    for record in tqdm(dataset, desc='Applying knowledge edits'):\n",
    "        # logger.info(f'RECORD: {record}')\n",
    "        kgeditor.edit_knowledge(record)\n",
    "\n",
    "    logger.info('Evaluating method')\n",
    "    results = []\n",
    "    for i, record in enumerate(tqdm(dataset, desc=f'Evaluating')):\n",
    "        metrics = {\n",
    "            'case_id': i,\n",
    "            'requested_rewrite': record,\n",
    "            'post': {}\n",
    "        }\n",
    "        \n",
    "        # 1. rewrite accuracy\n",
    "        response = kgeditor.process(record[\"prompt\"])\n",
    "        target_new = record[\"target_new\"].strip().lower()\n",
    "        rewrite_acc = 1.0 if target_new in response.lower() else 0.0\n",
    "        metrics[\"post\"][\"rewrite_acc\"] = [rewrite_acc]\n",
    "\n",
    "        # 2. Portability\n",
    "        metrics[\"post\"][\"portability\"] = {}\n",
    "        if datatype in ['counterfact', 'recent', 'zsre']:\n",
    "            portability_keys = {\n",
    "                'Subject_Aliasing': 'portability_s',\n",
    "                'reasoning': 'portability_r',\n",
    "                'Logical_Generalization': 'portability_l'\n",
    "            }\n",
    "            for metric_name, record_key in portability_keys.items():\n",
    "                if record_key in record and record[record_key] is not None:\n",
    "                    acc_list = []\n",
    "                    for test in record[record_key]:\n",
    "                        if test is None:\n",
    "                            continue\n",
    "                        test_response = kgeditor.process(test[\"prompt\"]).lower()\n",
    "                        ground_truths = flatten_ground_truth(test[\"ground_truth\"])\n",
    "                        correct = any(truth in test_response for truth in ground_truths)\n",
    "                        acc_list.append(1.0 if correct else 0.0)\n",
    "                    metrics[\"post\"][\"portability\"][metric_name] = acc_list\n",
    "\n",
    "        # 3. Locality\n",
    "        metrics[\"post\"][\"locality\"] = {}\n",
    "        if datatype in ['counterfact', 'recent', 'zsre']:\n",
    "            locality_keys = {\n",
    "                'Relation_Specificity': 'locality_rs',\n",
    "                'Forgetfulness': 'locality_f'\n",
    "            }\n",
    "            for metric_name, record_key in locality_keys.items():\n",
    "                if record_key in record and record[record_key] is not None:\n",
    "                    acc_list = []\n",
    "                    for test in record[record_key]:\n",
    "                        if test is None:\n",
    "                            continue\n",
    "                        test_response = kgeditor.process(test[\"prompt\"]).lower()\n",
    "                        ground_truths = flatten_ground_truth(test[\"ground_truth\"])\n",
    "                        correct = any(truth in test_response for truth in ground_truths)\n",
    "                        acc_list.append(1.0 if correct else 0.0)\n",
    "                    metrics[\"post\"][\"locality\"][metric_name] = acc_list\n",
    "        elif datatype == 'wikibio':\n",
    "            if 'locality_rs' in record and record['locality_rs'] is not None:\n",
    "                acc_list = []\n",
    "                for test in record['locality_rs']:\n",
    "                    if test is None:\n",
    "                        continue\n",
    "                    test_response = kgeditor.process(test[\"prompt\"]).lower()\n",
    "                    ground_truths = flatten_ground_truth(test[\"ground_truth\"])\n",
    "                    correct = any(truth in test_response for truth in ground_truths)\n",
    "                    acc_list.append(1.0 if correct else 0.0)\n",
    "                metrics[\"post\"][\"locality\"]['Relation_Specificity'] = acc_list\n",
    "\n",
    "        results.append(metrics)\n",
    "        \n",
    "    logger.info('Evaluating completed. Saving results.')\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    eval_kgedit(out_path, logger)\n",
    "    logger.info('KGEdit pipeline completed successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T23:54:14.308045Z",
     "iopub.status.busy": "2025-06-23T23:54:14.307783Z",
     "iopub.status.idle": "2025-06-24T00:09:10.616648Z",
     "shell.execute_reply": "2025-06-24T00:09:10.615789Z",
     "shell.execute_reply.started": "2025-06-23T23:54:14.308026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6793b14a9e5a4649a37f7e28ec028db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b5fe6c2d144e369269159c9a09b2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cebc5370234c0e88833b7600ddd95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d10a93cdd2e4498ad1c73684efc62ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5797aa653677469b9cfa01125ee5e0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a43755b5c2467ca22853a0f2faa3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfe1c7634024342968f8e6931a287b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying knowledge edits:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fc414616d6437781dc2014337a4847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Final Evaluation Results =====\n",
      "Edit Success: 72.00%\n",
      "\n",
      "Overall Portability: 35.29%\n",
      "  - reasoning: 27.27%\n",
      "  - Logical_Generalization: 60.00%\n",
      "  - Subject_Aliasing: 0.00%\n",
      "\n",
      "Overall Locality: 82.78%\n",
      "  - Relation_Specificity: 81.22%\n",
      "  - Forgetfulness: 92.86%\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "SAMPLE = 'recent'\n",
    "SAMPLE_TYPE = 'train'\n",
    "\n",
    "train_data_dirs = {\n",
    "    'wikibio': '/kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-train-all.json', # OK\n",
    "    'counterfact': '/kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/train_cf.json', # OK\n",
    "    'recent': '/kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_train.json', # OK\n",
    "    'zsre': None\n",
    "}\n",
    "test_data_dirs = {\n",
    "    'wikibio': '/kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-test-all.json', # OK\n",
    "    'counterfact': '/kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/test_cf.json', # OK\n",
    "    'recent': '/kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_test.json', # OK\n",
    "    'zsre': '/kaggle/working/EasyEdit/data/benchmark/ZsRE/ZsRE-test-all.json'\n",
    "}\n",
    "\n",
    "sample_path = test_data_dirs[SAMPLE] if SAMPLE_TYPE == 'test' else train_data_dirs[SAMPLE]\n",
    "\n",
    "qwen_models = {\n",
    "    'qwen2.5-1.5b': '/kaggle/input/qwen2.5/transformers/1.5b/1',\n",
    "    'qwen2.5-1.5b-instruct': '/kaggle/input/qwen2.5/transformers/1.5b-instruct/1',\n",
    "    'qwen2.5-3b': '/kaggle/input/qwen2.5/transformers/3b/1',\n",
    "    'qwen2.5-3b-instruct': '/kaggle/input/qwen2.5/transformers/3b-instruct/1',\n",
    "    'gpt2-xl': 'openai-community/gpt2-xl'\n",
    "}\n",
    "\n",
    "\n",
    "run_kgedit(\n",
    "    data_path=sample_path,\n",
    "    data_size=50,\n",
    "    model_name=qwen_models[MODEL],\n",
    "    use_nli=True,\n",
    "    use_chat_template=USE_CHAT_TEMPLATE,\n",
    "    use_wiki=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:50:36.237763Z",
     "iopub.status.busy": "2025-06-24T00:50:36.237316Z",
     "iopub.status.idle": "2025-06-24T00:51:43.492879Z",
     "shell.execute_reply": "2025-06-24T00:51:43.492003Z",
     "shell.execute_reply.started": "2025-06-24T00:50:36.237725Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyEdit'...\n",
      "remote: Enumerating objects: 8523, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 8523 (delta 7), reused 3 (delta 3), pack-reused 8502 (from 2)\u001b[K\n",
      "Receiving objects: 100% (8523/8523), 85.85 MiB | 42.92 MiB/s, done.\n",
      "Resolving deltas: 100% (5411/5411), done.\n",
      "/kaggle/working/EasyEdit\n",
      "Note: switching to 'f0ec5eb3ef6a09f637fa5344667a78f38fbeb05d'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at f0ec5eb fix bug in notebooks and caa_data.py\n",
      "Collecting datasets==1.18.3 (from -r colab_requirements.txt (line 1))\n",
      "  Downloading datasets-1.18.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting gpustat==1.1 (from -r colab_requirements.txt (line 2))\n",
      "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting hydra-core==1.1.1 (from -r colab_requirements.txt (line 3))\n",
      "  Downloading hydra_core-1.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting higher==0.2.1 (from -r colab_requirements.txt (line 4))\n",
      "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting omegaconf==2.1.1 (from -r colab_requirements.txt (line 5))\n",
      "  Downloading omegaconf-2.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting sentence-transformers==2.2.2 (from -r colab_requirements.txt (line 6))\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting openai==0.27.9 (from -r colab_requirements.txt (line 7))\n",
      "  Downloading openai-0.27.9-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting peft==0.7.1 (from -r colab_requirements.txt (line 8))\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting timm==0.9.7 (from -r colab_requirements.txt (line 9))\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting iopath==0.1.10 (from -r colab_requirements.txt (line 10))\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting fairscale==0.4.13 (from -r colab_requirements.txt (line 11))\n",
      "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->-r colab_requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat==1.1->-r colab_requirements.txt (line 2)) (12.570.86)\n",
      "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat==1.1->-r colab_requirements.txt (line 2)) (5.9.5)\n",
      "Collecting blessed>=1.17.1 (from gpustat==1.1->-r colab_requirements.txt (line 2))\n",
      "  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core==1.1.1->-r colab_requirements.txt (line 3))\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from higher==0.2.1->-r colab_requirements.txt (line 4)) (2.5.1+cu121)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.1.1->-r colab_requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (4.47.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (0.20.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (3.2.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r colab_requirements.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r colab_requirements.txt (line 8)) (0.4.5)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath==0.1.10->-r colab_requirements.txt (line 10)) (4.12.2)\n",
      "Collecting portalocker (from iopath==0.1.10->-r colab_requirements.txt (line 10))\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat==1.1->-r colab_requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (0.21.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2->-r colab_requirements.txt (line 6)) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->higher==0.2.1->-r colab_requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets==1.18.3->-r colab_requirements.txt (line 1)) (2024.2.0)\n",
      "Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: gpustat, sentence-transformers, iopath, fairscale, antlr4-python3-runtime\n",
      "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gpustat: filename=gpustat-1.1-py3-none-any.whl size=26530 sha256=6d35b91804903426a36110f42450dcfdf414551558b09243af826de0a748cc17\n",
      "  Stored in directory: /root/.cache/pip/wheels/ee/d0/2c/1e02440645c2318ba03aea99993a44a9108dc8f74de0bd370b\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=11f158b793cefcfd1f4011a413538fd6ed143829209648b6bc8591a11eb09df2\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=bef4ee945954858f57ad9a2879fa07d6c28a9843ba04a52622d3e9a7fb2ee9e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332208 sha256=15312fb2b494a67e135a49d8612e99d94cb00c7bbe85c4e2ea8b3b00cc5d5163\n",
      "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=a43eb0d9c51d6c9771c664a14e63b683f073037267117f6d73495b27d117a58d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
      "Successfully built gpustat sentence-transformers iopath fairscale antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, blessed, iopath, hydra-core, gpustat, higher, openai, timm, sentence-transformers, peft, fairscale, datasets\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
      "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.3.0\n",
      "    Uninstalling omegaconf-2.3.0:\n",
      "      Successfully uninstalled omegaconf-2.3.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.12\n",
      "    Uninstalling timm-1.0.12:\n",
      "      Successfully uninstalled timm-1.0.12\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.3.1\n",
      "    Uninstalling sentence-transformers-3.3.1:\n",
      "      Successfully uninstalled sentence-transformers-3.3.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.14.0\n",
      "    Uninstalling peft-0.14.0:\n",
      "      Successfully uninstalled peft-0.14.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.1\n",
      "    Uninstalling datasets-3.3.1:\n",
      "      Successfully uninstalled datasets-3.3.1\n",
      "Successfully installed antlr4-python3-runtime-4.8 blessed-1.21.0 datasets-1.18.3 fairscale-0.4.13 gpustat-1.1 higher-0.2.1 hydra-core-1.1.1 iopath-0.1.10 omegaconf-2.1.1 openai-0.27.9 peft-0.7.1 portalocker-3.2.0 sentence-transformers-2.2.2 timm-0.9.7\n",
      "Collecting qwen-vl-utils\n",
      "  Downloading qwen_vl_utils-0.0.11-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Downloading av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (24.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (11.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (2025.1.31)\n",
      "Downloading qwen_vl_utils-0.0.11-py3-none-any.whl (7.6 kB)\n",
      "Downloading av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: av, qwen-vl-utils\n",
      "Successfully installed av-14.4.0 qwen-vl-utils-0.0.11\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.4.0)\n",
      "Collecting openai==1.2.0\n",
      "  Downloading openai-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (2.11.0a2)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.0) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.2.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (2.29.0)\n",
      "Downloading openai-1.2.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.9\n",
      "    Uninstalling openai-0.27.9:\n",
      "      Successfully uninstalled openai-0.27.9\n",
      "Successfully installed openai-1.2.0\n",
      "Collecting zhipuai\n",
      "  Downloading zhipuai-2.1.5.20250611-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cachetools>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (5.5.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.11.0a2)\n",
      "Requirement already satisfied: pydantic-core>=2.14.6 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.29.0)\n",
      "Collecting pyjwt<2.9.0,>=2.8.0 (from zhipuai)\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->zhipuai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->zhipuai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->zhipuai) (1.2.2)\n",
      "Downloading zhipuai-2.1.5.20250611-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyjwt, zhipuai\n",
      "  Attempting uninstall: pyjwt\n",
      "    Found existing installation: PyJWT 2.10.1\n",
      "    Uninstalling PyJWT-2.10.1:\n",
      "      Successfully uninstalled PyJWT-2.10.1\n",
      "Successfully installed pyjwt-2.8.0 zhipuai-2.1.5.20250611\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (24.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=a94c41e877a00cfe537a301ff1d9e86ecc2f1fdb2b542b2d29add088c547d7a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.8\n",
      "    Uninstalling antlr4-python3-runtime-4.8:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.8\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.1.1\n",
      "    Uninstalling omegaconf-2.1.1:\n",
      "      Successfully uninstalled omegaconf-2.1.1\n",
      "  Attempting uninstall: hydra-core\n",
      "    Found existing installation: hydra-core 1.1.1\n",
      "    Uninstalling hydra-core-1.1.1:\n",
      "      Successfully uninstalled hydra-core-1.1.1\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n",
      "Collecting sentence_transformers==3.2.1\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (0.29.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==3.2.1) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==3.2.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==3.2.1) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==3.2.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers==3.2.1) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==3.2.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==3.2.1) (3.5.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers==3.2.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers==3.2.1) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers==3.2.1) (2024.2.0)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence_transformers\n",
      "  Attempting uninstall: sentence_transformers\n",
      "    Found existing installation: sentence-transformers 2.2.2\n",
      "    Uninstalling sentence-transformers-2.2.2:\n",
      "      Successfully uninstalled sentence-transformers-2.2.2\n",
      "Successfully installed sentence_transformers-3.2.1\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (1.18.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.18.3\n",
      "    Uninstalling datasets-1.18.3:\n",
      "      Successfully uninstalled datasets-1.18.3\n",
      "Successfully installed datasets-3.6.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/zjunlp/EasyEdit\n",
    "%cd EasyEdit\n",
    "!git checkout f0ec5eb3ef6a09f637fa5344667a78f38fbeb05d\n",
    "!pip install -r colab_requirements.txt\n",
    "!pip install qwen-vl-utils\n",
    "!pip install av\n",
    "!pip install openai==1.2.0\n",
    "!pip install zhipuai\n",
    "!pip install hydra-core -U\n",
    "!pip install sentence_transformers==3.2.1\n",
    "!pip install -U datasets\n",
    "!pip install -q sparqlwrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:51:43.494509Z",
     "iopub.status.busy": "2025-06-24T00:51:43.494190Z",
     "iopub.status.idle": "2025-06-24T00:52:09.132705Z",
     "shell.execute_reply": "2025-06-24T00:52:09.132039Z",
     "shell.execute_reply.started": "2025-06-24T00:51:43.494483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from easyeditor import KnowEditDataset, ZsreDataset, EditTrainer # , BaseEditor\n",
    "from easyeditor import (\n",
    "    FTHyperParams, \n",
    "    IKEHyperParams, \n",
    "    KNHyperParams, \n",
    "    MEMITHyperParams, \n",
    "    ROMEHyperParams, \n",
    "    LoRAHyperParams,\n",
    "    MELOHyperParams,\n",
    "    MENDHyperParams,\n",
    "    MENDTrainingHparams,\n",
    "    SERACHparams\n",
    "    )\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import os.path as path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:09.134308Z",
     "iopub.status.busy": "2025-06-24T00:52:09.133979Z",
     "iopub.status.idle": "2025-06-24T00:52:09.137930Z",
     "shell.execute_reply": "2025-06-24T00:52:09.137066Z",
     "shell.execute_reply.started": "2025-06-24T00:52:09.134270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "USE_CHAT_TEMPLATE = False\n",
    "MODEL = 'qwen2.5-1.5b-instruct' # gpt2-xl, qwen2.5-1.5b\n",
    "if 'instruct' in MODEL:\n",
    "    USE_CHAT_TEMPLATE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:09.139464Z",
     "iopub.status.busy": "2025-06-24T00:52:09.139269Z",
     "iopub.status.idle": "2025-06-24T00:52:09.159078Z",
     "shell.execute_reply": "2025-06-24T00:52:09.158346Z",
     "shell.execute_reply.started": "2025-06-24T00:52:09.139447Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# generate_fast\n",
    "\n",
    "import unicodedata\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from easyeditor.util.logit_lens import LogitLens\n",
    "\n",
    "def generate_fast(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    n_gen_per_prompt: int = 1,\n",
    "    top_k: int = 5,\n",
    "    max_out_len: int = 200,\n",
    "    vanilla_generation=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Changed function with added ability to use chat template generation\n",
    "    \"\"\"\n",
    "\n",
    "    if USE_CHAT_TEMPLATE and hasattr(tok, \"apply_chat_template\"):\n",
    "        # print('applying chat template!')\n",
    "        formatted_prompts = []\n",
    "        for prompt in prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            try:\n",
    "                formatted_prompt = tok.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                formatted_prompts.append(formatted_prompt)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying chat template: {e}\")\n",
    "                formatted_prompts.append(prompt)\n",
    "        prompts = formatted_prompts\n",
    "\n",
    "    # Unroll prompts and tokenize\n",
    "    inp = [prompt for prompt in prompts for _ in range(n_gen_per_prompt)]\n",
    "    inp_tok = tok(inp, padding=True, return_tensors=\"pt\").to(\n",
    "        next(model.parameters()).device\n",
    "    )\n",
    "    input_ids, attention_mask = inp_tok[\"input_ids\"], inp_tok[\"attention_mask\"]\n",
    "    if vanilla_generation:\n",
    "        gen_txt = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_out_len\n",
    "        )\n",
    "        txt = [tok.decode(x, skip_special_tokens=True) for x in gen_txt.detach().cpu().numpy().tolist()]\n",
    "        txt = [\n",
    "            unicodedata.normalize(\"NFKD\", x)\n",
    "            .replace(\"\\n\\n\", \" \")\n",
    "            .replace(\"<|endoftext|>\", \"\")\n",
    "            for x in txt\n",
    "        ]\n",
    "        return txt\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    # Setup storage of fast generation with attention caches.\n",
    "    # `cur_context` is used to define the range of inputs that are not yet\n",
    "    # stored in `past_key_values`. At each step, we are generating the\n",
    "    # next token for the index at `cur_context.stop + 1`.\n",
    "    past_key_values, cur_context = None, slice(0, attention_mask.sum(1).min().item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while input_ids.size(1) < max_out_len:  # while not exceeding max output length\n",
    "            model_out = model(\n",
    "                input_ids=input_ids[:, cur_context],\n",
    "                attention_mask=None if 'llama' in model.name_or_path.lower() or 'baichuan' in model.name_or_path.lower() or 'internlm' in model.name_or_path.lower() else attention_mask[:, cur_context],\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            if type(model_out) is torch.Tensor:\n",
    "                logits = model_out\n",
    "            else:\n",
    "                logits = model_out.logits\n",
    "            past_key_values = model_out.past_key_values\n",
    "            softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
    "\n",
    "            # Top-k sampling\n",
    "            tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
    "            softmax_out_top_k = torch.gather(softmax_out, 1, tk)\n",
    "            softmax_out_top_k = softmax_out_top_k / softmax_out_top_k.sum(1)[:, None]\n",
    "            new_tok_indices = torch.multinomial(softmax_out_top_k, 1)\n",
    "            new_toks = torch.gather(tk, 1, new_tok_indices)\n",
    "\n",
    "            # If we're currently generating the continuation for the last token in `input_ids`,\n",
    "            # create a new index so we can insert the new token\n",
    "            if cur_context.stop == input_ids.size(1):\n",
    "                attention_mask = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_zeros(batch_size, 1)], dim=1\n",
    "                )\n",
    "                input_ids = torch.cat(\n",
    "                    [\n",
    "                        input_ids,\n",
    "                        input_ids.new_ones(batch_size, 1) * tok.pad_token_id,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            last_non_masked = attention_mask.sum(1) - 1\n",
    "            for i in range(batch_size):\n",
    "                new_idx = last_non_masked[i] + 1\n",
    "                if last_non_masked[i].item() + 1 != cur_context.stop:\n",
    "                    continue\n",
    "\n",
    "                # Stop generating if we've already maxed out for this prompt\n",
    "                if new_idx < max_out_len:\n",
    "                    input_ids[i][new_idx] = new_toks[i]\n",
    "                    attention_mask[i][new_idx] = 1\n",
    "\n",
    "            cur_context = slice(cur_context.stop, cur_context.stop + 1)\n",
    "    txt = [tok.decode(x, skip_special_tokens=True) for x in input_ids.detach().cpu().numpy().tolist()]\n",
    "    txt = [\n",
    "        unicodedata.normalize(\"NFKD\", x)\n",
    "        .replace(\"\\n\\n\", \" \")\n",
    "        .replace(\"<|endoftext|>\", \"\")\n",
    "        for x in txt\n",
    "    ]\n",
    "\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:09.160145Z",
     "iopub.status.busy": "2025-06-24T00:52:09.159874Z",
     "iopub.status.idle": "2025-06-24T00:52:09.229829Z",
     "shell.execute_reply": "2025-06-24T00:52:09.229022Z",
     "shell.execute_reply.started": "2025-06-24T00:52:09.160112Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_utils.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "import typing\n",
    "# from ..util.generate import generate_fast\n",
    "import torch.nn.functional as F\n",
    "from easyeditor.trainer import *\n",
    "from sklearn.metrics import f1_score\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import time\n",
    "import regex\n",
    "import string\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return regex.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def llm_judge(question, ground_truth, prediction, api_key):\n",
    "    content_template = \"\"\"\n",
    "Your job is to look at a question, a gold target, and a predicted answer, and then assign a grade of either [\"CORRECT\", \"INCORRECT\"].\n",
    "\n",
    "The following are examples of CORRECT predicted answers.\n",
    "```\n",
    "Question: What are the names of Barack Obama's children?\n",
    "Gold target: Malia Obama and Sasha Obama\n",
    "Predicted answer 1: sasha and malia obama\n",
    "Predicted answer 2: Malia and Sasha Obama are the names of Barack Obama's children.\n",
    "```\n",
    "These predicted answers are all CORRECT because:\n",
    "    - They fully contain the important information in the gold target.\n",
    "    - They do not contain any information that contradicts the gold target.\n",
    "\n",
    "The following are examples of INCORRECT predicted answers.\n",
    "```\n",
    "Question: What are the names of Barack Obama's children?\n",
    "Gold target: Malia and Sasha\n",
    "Predicted answer 1: Malia.\n",
    "Predicted answer 2: Malia, Sasha, and Susan.\n",
    "Predicted answer 3: Malia and Sasha, Malia and Sasha, Malia and Sasha, Malia and Sasha (repeated answer)\n",
    "```\n",
    "These predicted answers are all INCORRECT because:\n",
    "    - A factual statement in the answer contradicts the gold target or contain repeated answer.\n",
    "\n",
    "\n",
    "Here is a sample. Simply reply with either CORRECT or INCORRECT.\n",
    "\n",
    "```\n",
    "Question: {question}\n",
    "Gold target: {target}\n",
    "Predicted answer: {predicted_answer}\n",
    "```\n",
    "\n",
    "According to the gold target, please grade the predicted answer of this question as one of:\n",
    "A: CORRECT\n",
    "B: INCORRECT\n",
    "\n",
    "Just return the letters \"A\" or \"B\", with no text around it.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    content = content_template.format(\n",
    "        question=question,\n",
    "        target=ground_truth,\n",
    "        predicted_answer=prediction,\n",
    "    )\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    llm_ans = completion.choices[0].message.content\n",
    "    llm_score = 1.0 if llm_ans == \"A\" else 0.0\n",
    "    time.sleep(1) # avoid high rate of request\n",
    "    return llm_score\n",
    "\n",
    "def test_prediction_acc_LLM_judge(model, tok, hparams, prompts, targets, device, locality=False):\n",
    "    # generation & truncation\n",
    "    all_score = []\n",
    "    all_response = []\n",
    "    if isinstance(prompts, str):\n",
    "        prompts, targets = [prompts, ], [targets, ]\n",
    "    for prompt in prompts:\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        text = tok.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        prompt_tok = tok(\n",
    "            text,\n",
    "            padding = True,\n",
    "            truncation = True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(f\"cuda:{device}\")\n",
    "        # add a template\n",
    "        gen_tokens = model.generate(\n",
    "            input_ids=prompt_tok['input_ids'],\n",
    "            attention_mask=prompt_tok['attention_mask'],\n",
    "            max_new_tokens=512,\n",
    "            stop_strings=[\".\", \"\\n\", tok.eos_token],\n",
    "            tokenizer=tok,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        # decode and process\n",
    "        if isinstance(model, T5ForConditionalGeneration):\n",
    "            trunc_gen_tokens = gen_tokens[0]  # encoder-decoder model only provied generated content after prompt\n",
    "        else:\n",
    "            trunc_gen_tokens = gen_tokens[0][prompt_tok['input_ids'].shape[1]:]  # decoder-only model provied generated content containing prompt\n",
    "        # if locality:\n",
    "        #     ans = trunc_gen_tokens.detach().cpu().numpy().tolist()\n",
    "        #     all_response.append(ans)\n",
    "        # else:\n",
    "        gen_content = tok.decode(trunc_gen_tokens)\n",
    "        suffixes_to_remove = [\".\", \"\\n\", tok.eos_token]\n",
    "        for suffix in suffixes_to_remove:\n",
    "            if gen_content.endswith(suffix):\n",
    "                gen_content = gen_content[:-len(suffix)]\n",
    "        # LLM-as-a-Judge\n",
    "        if hparams.evaluation_type == \"generate-text\":\n",
    "            all_response.append(gen_content)\n",
    "        elif hparams.evaluation_type == \"LLM-judge\" and hasattr(hparams, 'api_key') and hparams.api_key:\n",
    "            LLM_Score = llm_judge(prompts, targets, gen_content, hparams.api_key)\n",
    "            all_score.append(LLM_Score)\n",
    "            all_response.append(gen_content)\n",
    "        else:\n",
    "            # the user do not provide api key, using exact match as an alternative\n",
    "            EM_Score = float(exact_match_score(gen_content, targets))\n",
    "            all_score.append(EM_Score)\n",
    "            all_response.append(gen_content)\n",
    "    \n",
    "    if len(all_score) > 0:\n",
    "        return all_score, all_response\n",
    "    else:\n",
    "        return all_response\n",
    "\n",
    "def test_batch_prediction_acc(model, tok, hparams, prompts, target, device, locality=False):\n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=hparams.max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_tok)\n",
    "        if type(outputs) is torch.Tensor:\n",
    "            logits = outputs\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "\n",
    "        if tok.padding_side == 'left':\n",
    "            ans = torch.argmax(logits, dim=-1)[:, -1].squeeze()\n",
    "        else:\n",
    "            last_non_masked = prompt_tok[\"attention_mask\"].sum(1) - 1\n",
    "            to_gather = last_non_masked.unsqueeze(1).repeat(1, logits.size(-1)).unsqueeze(1)\n",
    "            gathered = torch.gather(logits, 1, to_gather).squeeze(1)\n",
    "            ans = torch.argmax(gathered, dim=1)\n",
    "\n",
    "        ans = ans.squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "        if locality:\n",
    "            return ans\n",
    "\n",
    "        return np.mean(np.equal(ans, target))\n",
    "\n",
    "def test_seq2seq_batch_prediction_acc(model, tok, hparams, prompts, targets, device, locality=False):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts,targets = [prompts,], [targets,]\n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=hparams.max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "\n",
    "    trg_tok = tok(\n",
    "        targets,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=hparams.max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "\n",
    "    prompt_tok['decoder_input_ids'] = trg_tok['input_ids']\n",
    "    prompt_tok['decoder_attention_mask'] = trg_tok['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_tok)\n",
    "        if type(outputs) is torch.Tensor:\n",
    "            logits = outputs\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "\n",
    "        assert logits.size(1) == trg_tok['input_ids'].size(1)\n",
    "        ans = torch.argmax(logits, dim=-1)\n",
    "        if locality:\n",
    "            answers = ans.squeeze().detach().cpu().numpy().tolist()\n",
    "            return answers if type(answers[0]) is list else [answers,]\n",
    "        return torch.mean((trg_tok['input_ids'][:,:-1] == ans[:,:-1]).float(), dim=-1).detach().cpu().numpy().tolist()\n",
    "\n",
    "def test_prediction_acc(model, tok, hparams, prompts, targets, device, locality=False, vanilla_generation=False):\n",
    "    if vanilla_generation:\n",
    "        if isinstance(prompts, str):\n",
    "            prompts, targets = [prompts, ], [targets, ]\n",
    "        results = []\n",
    "        for prompt, target_new in zip(prompts, targets):\n",
    "            target_new_tokens = tok.encode(target_new, add_special_tokens=False)\n",
    "            prompt_tok = tok(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(f\"cuda:{device}\")\n",
    "            gen_token = model.generate(\n",
    "                input_ids=prompt_tok['input_ids'],\n",
    "                attention_mask=prompt_tok['attention_mask'],\n",
    "                max_new_tokens=len(target_new_tokens),\n",
    "                pad_token_id=tok.eos_token_id,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            if locality:\n",
    "                results.append(gen_token.detach().cpu().numpy().tolist()[0][-len(target_new_tokens):])\n",
    "            else:\n",
    "                results.append(np.mean(np.equal(target_new_tokens, gen_token.detach().cpu().numpy().tolist()[0][-len(target_new_tokens):])))\n",
    "        return results\n",
    "\n",
    "    if isinstance(prompts, str):\n",
    "        prompts,targets = [prompts,], [targets,]\n",
    "    if not locality and hasattr(hparams, 'use_chat_template') and hparams.use_chat_template:\n",
    "        prompts = [[{\"role\":\"user\", \"content\":m}] for m in prompts]\n",
    "        prompts=tok.apply_chat_template(prompts,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        tokenize=False)\n",
    "    prompt_target = [prompt + ' ' + target for prompt, target in zip(prompts,targets)]\n",
    "    max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "    before_padding_side = tok.padding_side\n",
    "    tok.padding_side = 'left'\n",
    "    prompt_target_tok = tok(\n",
    "        prompt_target,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max(hparams.max_length, max_prompt_len),\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max(hparams.max_length, max_prompt_len),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tok.padding_side = before_padding_side\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "    prompt_len = [x+y for x,y in zip(num_pad_toks,num_prompt_toks)]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_target_tok)\n",
    "        if type(outputs) is torch.Tensor:\n",
    "            logits = outputs\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "        answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "        answers = slice_list(answers,prompt_len,left=True)\n",
    "        labels = slice_list(labels,prompt_len,left=False)\n",
    "        if locality:\n",
    "            return answers if type(answers[0]) is list else [answers,]\n",
    "        if isinstance(answers[0], list):\n",
    "            res = []\n",
    "            for ans,label in zip(answers,labels):\n",
    "                temp_acc = np.mean(np.equal(ans, label))\n",
    "                if np.isnan(temp_acc):\n",
    "                    continue\n",
    "                res.append(temp_acc)\n",
    "            return res\n",
    "        else:\n",
    "            return [np.mean(np.equal(answers, labels))]\n",
    "\n",
    "def test_generation_quality_serac(\n",
    "    model,\n",
    "    tok,\n",
    "    prefixes: typing.List[str],\n",
    "    max_out_len: int,       \n",
    "):\n",
    "    #only single case\n",
    "    prompt_tok = tok(\n",
    "        prefixes,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    prompt_tok_length=len(prompt_tok['input_ids'])\n",
    "    gen_texts=model.generate(**prompt_tok,max_new_tokens=256)\n",
    "    if isinstance(model,SERAC):\n",
    "        gen_texts=tok.decode(gen_texts[prompt_tok_length:])\n",
    "        gen_texts=[gen_texts]\n",
    "        print(len(gen_texts))\n",
    "    else:\n",
    "        gen_texts=tok.decode(gen_texts[prompt_tok_length:])\n",
    "        gen_texts=[gen_texts]\n",
    "        print(len(gen_texts))      \n",
    "    ngram_entropy = n_gram_entropy(gen_texts, return_list=True)\n",
    "\n",
    "\n",
    "    ret = {\n",
    "        \"ngram_entropy\": ngram_entropy\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "def test_generation_quality(\n",
    "    model,\n",
    "    tok,\n",
    "    prefixes: typing.List[str],\n",
    "    max_out_len: int,\n",
    "    vanilla_generation: bool = False,\n",
    "):\n",
    "    gen_texts = generate_fast(\n",
    "        model,\n",
    "        tok,\n",
    "        prefixes,\n",
    "        n_gen_per_prompt=1,\n",
    "        max_out_len=max_out_len,\n",
    "        vanilla_generation=vanilla_generation,\n",
    "    )\n",
    "\n",
    "    ngram_entropy = n_gram_entropy(gen_texts)\n",
    "    ret = {\n",
    "        \"ngram_entropy\": ngram_entropy,\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "def n_gram_entropy(gen_texts, agg=\"arith\"):\n",
    "    assert agg in [\"arith\", \"geom\"]\n",
    "\n",
    "    return (scipy.stats.mstats.gmean if agg == \"geom\" else np.mean)(\n",
    "        [compute_n_gram_entropy(txt) for txt in gen_texts]\n",
    "    ).item()\n",
    "\n",
    "def compute_n_gram_entropy(sentence, ns=None, weights=None, agg=\"arith\"):\n",
    "    if ns is None:\n",
    "        ns = [2, 3]\n",
    "    if weights is None:\n",
    "        weights = [2 / 3, 4 / 3]\n",
    "    assert agg in [\"arith\", \"geom\"]\n",
    "\n",
    "    entropy_list = []\n",
    "    for n in ns:\n",
    "        fdist = compute_freq(sentence, n)\n",
    "        freqs = np.array([freq for _, freq in fdist.items()])\n",
    "        freqs = freqs / freqs.sum()\n",
    "\n",
    "        entropy_list.append(np.sum(-freqs * np.log(freqs) / np.log(2)))\n",
    "\n",
    "    entropy_list = np.array(entropy_list) * np.array(weights)\n",
    "\n",
    "    return (scipy.stats.mstats.gmean if agg == \"geom\" else np.mean)(entropy_list)\n",
    "\n",
    "def compute_freq(sentence, n=2):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    ngrams = nltk.ngrams(tokens, n)\n",
    "    return nltk.FreqDist(ngrams)\n",
    "\n",
    "def PPL(\n",
    "    model,\n",
    "    tok,\n",
    "    prompt: typing.Union[str, typing.List[str]],\n",
    "    target_new: typing.Union[str, typing.List[str]],\n",
    "    device,\n",
    "):\n",
    "    if isinstance(prompt, str):\n",
    "        prompt,target_new = [prompt,], [target_new,]\n",
    "    full_prompt = [f\"{p} {l}\" for p, l in zip(prompt, target_new)]\n",
    "    prompt_ids = tok(list(prompt), return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_ids]\n",
    "    tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    for i in range(len(prompt)):\n",
    "        tokens[\"labels\"][i][:num_prompt_toks[i]] = -100\n",
    "    tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = -100 # What is this doing?\n",
    "    batch = {f\"{k1}\" : v1 for k1, v1 in tokens.items()}\n",
    "    input_ids = batch[\"input_ids\"][:, :1024]#.to(device)\n",
    "    if \"labels\" not in batch:\n",
    "        target_ids = batch[\"input_ids\"][:, :1024].clone()\n",
    "    else:\n",
    "        target_ids = batch[\"labels\"][:, :1024].clone()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids.to(device), labels=target_ids.to(device))\n",
    "        nll = outputs.loss\n",
    "    ppl = torch.exp(nll)#.clip(0, 100)\n",
    "    return ppl.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def OOD_PPL(\n",
    "        model,\n",
    "        tok,\n",
    "        prompt: typing.Union[str, typing.List[str]],\n",
    "        target_new: typing.Union[str, typing.List[str]],\n",
    "        device,\n",
    "        threshold=0.8\n",
    "):\n",
    "    if isinstance(prompt, str):\n",
    "        prompt, target_new = [prompt, ], [target_new, ]\n",
    "\n",
    "    full_prompt = [f\"{p}\" for p, l in zip(prompt, target_new)]\n",
    "    tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    tokens[\"labels\"] = tokens['input_ids'].clone()\n",
    "    tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = -100\n",
    "    batch = {f\"{k1}\": v1 for k1, v1 in tokens.items()}\n",
    "    input_ids = batch[\"input_ids\"][:, :1024]  # .to(device)\n",
    "    target_ids = batch[\"labels\"][:, :1024]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids.to(device), labels=target_ids.to(device)).logits\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = target_ids.to(device)[:, 1:].contiguous()\n",
    "\n",
    "        log_probs = -nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "        if shift_labels.dim() == log_probs.dim() - 1:\n",
    "            shift_labels = shift_labels.unsqueeze(-1)\n",
    "\n",
    "        padding_mask = shift_labels.eq(-100)\n",
    "\n",
    "        # In case the ignore_index is -100, the gather will fail, so we replace labels by 0. The padding_mask\n",
    "        # will ignore them in any case.\n",
    "        shift_labels = torch.clamp(shift_labels, min=0)\n",
    "\n",
    "        nll_loss = log_probs.gather(dim=-1, index=shift_labels)\n",
    "        nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "\n",
    "        threshold = -np.log(threshold)\n",
    "\n",
    "        return len(nll_loss[nll_loss < threshold]) / len(nll_loss.view(-1))\n",
    "\n",
    "def verify_answer(model_answer, correct_answer):\n",
    "    if type(correct_answer) is str:\n",
    "        correct_answer = [[correct_answer]]\n",
    "    for answer in correct_answer:\n",
    "        if True not in [possible_answer in model_answer for possible_answer in answer]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def answer_match(\n",
    "    model,\n",
    "    tok,\n",
    "    prompt: str,\n",
    "    target_new: str,\n",
    "    device,\n",
    "):\n",
    "    inputs = tok.encode(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs, temperature=0, max_new_tokens=30)\n",
    "    predict = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return verify_answer(predict,target_new)\n",
    "\n",
    "def slice_list(matrix,start_indices,left):\n",
    "    if isinstance(matrix[0], list):\n",
    "        if left:\n",
    "            return [row[start_index-1:-1] for row, start_index in zip(matrix, start_indices)]\n",
    "        else:\n",
    "            return [row[start_index:] for row, start_index in zip(matrix, start_indices)]\n",
    "    else:\n",
    "        if left:\n",
    "            return matrix[start_indices[0]-1:-1]\n",
    "        else:\n",
    "            return matrix[start_indices[0]:]\n",
    "\n",
    "def gather_log_probs(logits, labels):\n",
    "    # print(f\"labels.shape: {labels.shape} , logits.shape[:-1] :{logits.shape[:-1]}\")\n",
    "    assert labels.dim() == logits.dim() - 1\n",
    "    assert labels.shape == logits.shape[:-1]\n",
    "    return logits.log_softmax(-1).gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def masked_mean(values, mask):\n",
    "    assert mask.dtype == torch.bool\n",
    "    assert values.shape == mask.shape\n",
    "    return (values * mask.float()).sum() / mask.sum().float()\n",
    "\n",
    "def mask_hf_labels(labels, null_token=0):\n",
    "    valid_mask = labels != -100\n",
    "    valid_labels = labels.masked_fill(~valid_mask, null_token)\n",
    "    return valid_mask, valid_labels\n",
    "\n",
    "def es(pre_logits, edit_logits, q_mask, labels, same_mask):\n",
    "    \n",
    "    _, targ = mask_hf_labels(labels)\n",
    "\n",
    "    pos_mask = same_mask.unsqueeze(-1) * q_mask \n",
    "    neg_mask = (~same_mask).unsqueeze(-1) * q_mask \n",
    "        \n",
    "    pre_token_log_probs = gather_log_probs(pre_logits, targ)\n",
    "    edit_token_log_probs = gather_log_probs(edit_logits, targ)\n",
    "\n",
    "    mean_pos_pre = masked_mean(pre_token_log_probs, pos_mask)\n",
    "    mean_pos_edit = masked_mean(edit_token_log_probs, pos_mask)\n",
    "    mean_neg_edit = masked_mean(edit_token_log_probs, neg_mask)\n",
    "\n",
    "    z_sent = (mean_pos_edit - mean_neg_edit).sigmoid()\n",
    "    z_topic_raw = (mean_pos_edit - mean_pos_pre).exp()\n",
    "    z_topic = min(1, z_topic_raw)\n",
    "\n",
    "    es_sent = z_sent * z_topic\n",
    "    return es_sent\n",
    "\n",
    "def es_per_icl(example, pre_logits, edit_logits):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pre_q_mask = example[\"outer_pre\"][\"q_mask\"]\n",
    "        edit_q_mask = example[\"outer_edit\"][\"q_mask\"]\n",
    "        \n",
    "        pre_labels = example[\"outer_pre\"][\"labels\"]\n",
    "        edit_labels = example[\"outer_edit\"][\"labels\"]\n",
    "        \n",
    "        pre_mask, pre_targ = mask_hf_labels(pre_labels)\n",
    "        edit_mask, edit_targ = mask_hf_labels(edit_labels)\n",
    "        \n",
    "        same_per_mask = example[\"same_per_mask\"]\n",
    "\n",
    "        pre_pos_mask = same_per_mask.unsqueeze(-1) * pre_q_mask \n",
    "        pre_neg_mask = (~same_per_mask).unsqueeze(-1) * pre_q_mask \n",
    "        edit_pos_mask = same_per_mask.unsqueeze(-1) * edit_q_mask \n",
    "        edit_neg_mask = (~same_per_mask).unsqueeze(-1) * edit_q_mask \n",
    "        \n",
    "        pre_token_log_probs = gather_log_probs(pre_logits, pre_targ)\n",
    "        edit_token_log_probs = gather_log_probs(edit_logits, edit_targ)\n",
    "\n",
    "        mean_pos_pre = masked_mean(pre_token_log_probs, pre_pos_mask)\n",
    "        mean_pos_edit = masked_mean(edit_token_log_probs, edit_pos_mask)\n",
    "        mean_neg_edit = masked_mean(edit_token_log_probs, edit_neg_mask)\n",
    "\n",
    "        z_per = (mean_pos_edit - mean_neg_edit).sigmoid()\n",
    "        z_topic_raw = (mean_pos_edit - mean_pos_pre).exp()\n",
    "        z_topic = min(1, z_topic_raw)\n",
    "\n",
    "        es_per = z_per * z_topic\n",
    "        return {\n",
    "            \"acc_per\": es_per,\n",
    "            \"z_per\": z_per,\n",
    "            \"z_topic\": z_topic,\n",
    "            \"z_topic_raw\": z_topic_raw,\n",
    "            \"correct_probs\": mean_pos_edit,\n",
    "            \"wrong_probs\": mean_neg_edit,\n",
    "        }\n",
    "\n",
    "def per_generation(\n",
    "    model,\n",
    "    tok,\n",
    "    max_out_len: int,\n",
    "    target_per, \n",
    "    device,\n",
    "    edited_model=None,\n",
    "    IKE=False,\n",
    "    **kwargs\n",
    "    ):\n",
    "    def generate_text(query, model, tokenizer):\n",
    "        input_text = query\n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": max_out_len,\n",
    "            \"temperature\": 0,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "        src_input_ids = tokenizer(input_text).input_ids\n",
    "        input_ids = torch.tensor([src_input_ids], dtype=torch.long, device=device)\n",
    "        outputs = model.generate(input_ids, **generation_config)\n",
    "        response = tokenizer.decode(outputs[0][len(src_input_ids) :], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def clean_text(text):\n",
    "        return text.strip().split(\"\\n\")[0]\n",
    "    \n",
    "    if IKE:\n",
    "        pre_text = clean_text(generate_text(kwargs[\"pre_q\"], model, tok))\n",
    "        edit_text = clean_text(generate_text(kwargs[\"edit_q\"], model, tok))\n",
    "\n",
    "    else:\n",
    "        assert edited_model is not None\n",
    "        pre_text = clean_text(generate_text(kwargs[\"inner_q\"], model, tok))\n",
    "        edit_text = clean_text(generate_text(kwargs[\"inner_q\"], edited_model.model, tok))\n",
    "\n",
    "    ngram_pre_text = n_gram_entropy([pre_text])\n",
    "    ngram_edit_text = n_gram_entropy([edit_text])\n",
    "    coherent = ngram_pre_text >= 3.5 and ngram_edit_text >= 3.5\n",
    "    \n",
    "    result = {\n",
    "        \"pre_text\": pre_text,\n",
    "        \"edit_text\": edit_text,\n",
    "        \"ngram_pre_text\": ngram_pre_text,\n",
    "        \"ngram_edit_text\": ngram_edit_text,\n",
    "        \"coherent\": coherent,\n",
    "        \"target_per\": target_per,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def kl_loc_loss(pre, post, mask=None):\n",
    "    \n",
    "    pre = pre.to(torch.float32).contiguous()\n",
    "    post = post[:,-pre.shape[1]:,:].to(torch.float32).contiguous()\n",
    "    \n",
    "    sequence = pre.dim() == 3\n",
    "    pre_ = pre.view(-1, pre.shape[-1])\n",
    "    post_ = post.view(pre_.shape)\n",
    "    assert pre_.shape[0] == post_.shape[0]\n",
    "\n",
    "    if not sequence:\n",
    "        if pre_.shape[-1] == 1:  # No masking needed for binary classification\n",
    "            return (pre.sigmoid() * (F.logsigmoid(pre) - F.logsigmoid(post))).mean() + (\n",
    "                (-pre).sigmoid() * (F.logsigmoid(-pre) - F.logsigmoid(-post))\n",
    "            ).mean()\n",
    "    else:  # We have sequences of predictions; masking needed\n",
    "        # print(\"sequence\")\n",
    "        if pre_.shape[-1] > 1:\n",
    "            assert mask is not None\n",
    "            mask_ = mask.view(pre_.shape[0])\n",
    "            kl = (pre_.softmax(-1) * (pre_.log_softmax(-1) - post_.log_softmax(-1))).sum(-1)\n",
    "            return (kl * mask_).sum() / mask_.sum()\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def F1(model, tok, hparams, prompts, targets, device, locality=False, vanilla_generation=True):\n",
    "    if vanilla_generation:\n",
    "        target_new_tokens = tok.encode(targets, add_special_tokens=False)\n",
    "        prompt_tok = tok(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        gen_token = model.generate(\n",
    "            input_ids=prompt_tok['input_ids'],\n",
    "            attention_mask=prompt_tok['attention_mask'],\n",
    "            max_new_tokens=len(target_new_tokens),\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            use_cache=False,\n",
    "\n",
    "        )\n",
    "        return f1_score(target_new_tokens, gen_token.detach().cpu().numpy().tolist()[0][-len(target_new_tokens):], average='macro')\n",
    "    if isinstance(prompts, str):\n",
    "        prompts,targets = [prompts,], [targets,]\n",
    "    prompt_target = [prompt + ' ' + target for prompt, target in zip(prompts,targets)]\n",
    "    max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "    prompt_target_tok = tok(\n",
    "        prompt_target,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max(hparams.max_length, max_prompt_len),\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(f\"cuda:{device}\")\n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max(hparams.max_length, max_prompt_len),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "    prompt_len = [x+y for x,y in zip(num_pad_toks,num_prompt_toks)]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_target_tok)\n",
    "        if type(outputs) is torch.Tensor:\n",
    "            logits = outputs\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "        answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "        answers = slice_list(answers,prompt_len,left=True)\n",
    "        labels = slice_list(labels,prompt_len,left=False)\n",
    "\n",
    "        return f1_score(answers, labels, average='macro')\n",
    "\n",
    "def test_instance_change(model, tok, max_length, prompts, targets, device, P = None):\n",
    "    demo1_str = \"Whether FrancoAngeli belongs to category publisher? Yes\\nWhether And Other Stories belongs to category people? No\\n\"\n",
    "    if P is None:\n",
    "        prompts = demo1_str +prompts\n",
    "    else:\n",
    "        prompts = P + demo1_str + prompts\n",
    "\n",
    "    if isinstance(prompts, str):\n",
    "        prompts,targets = [prompts,], [targets,]\n",
    "    prompt_target = [prompt + ' ' + target for prompt, target in zip(prompts,targets)]\n",
    "    max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max(max_length, max_prompt_len),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        pre_edit_outputs = model.generate(\n",
    "            input_ids=prompt_tok['input_ids'].to(f\"cuda:{device}\"),\n",
    "            attention_mask=prompt_tok['attention_mask'].to(f\"cuda:{device}\"),\n",
    "            max_new_tokens=2,\n",
    "            pad_token_id=tok.eos_token_id\n",
    "        )\n",
    "\n",
    "        model_response = [tok.decode(x, skip_special_tokens=True) for x in pre_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "        answer = model_response[0][model_response[0].rfind('?')+2:]\n",
    "        # print(model_response[0], answer)\n",
    "\n",
    "        if \"yes\" in answer.lower():\n",
    "            return np.ones(1)\n",
    "        else:\n",
    "            if \"no\" not in answer.lower():\n",
    "                print(f\"entity error in define yes or no: {answer}\")\n",
    "                return np.array([-1.0])\n",
    "            return np.zeros(1)\n",
    "\n",
    "def test_concept_gen(model, tok, max_length, prompts, targets, device):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts,targets = [prompts,], [targets,]\n",
    "    prompts = [prompt + ' ' for prompt in prompts]\n",
    "    prompt_target = [prompt + ' ' + target for prompt, target in zip(prompts,targets)]\n",
    "    max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "    prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max(max_length, max_prompt_len),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        pre_edit_outputs = model.generate(\n",
    "            input_ids=prompt_tok['input_ids'].to(f\"cuda:{device}\"),\n",
    "            attention_mask=prompt_tok['attention_mask'].to(f\"cuda:{device}\"),\n",
    "            max_new_tokens=40,\n",
    "            pad_token_id=tok.eos_token_id\n",
    "        )\n",
    "\n",
    "        model_response = [tok.decode(x, skip_special_tokens=True) for x in pre_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "        answer = model_response[0][len(prompts[0]):]\n",
    "        return answer\n",
    "\n",
    "\n",
    "def test_safety_gen(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        test_prompt, \n",
    "        cuda,\n",
    "        max_tokens = 1624, \n",
    "        max_output_tokens=600):\n",
    "    tokenizer.padding_side = 'left'\n",
    "    # if input_tokens (at least 1024) + output_tokens (at least 600) < 1624, truncate the input length (from right to left, as harmful questions typically appear on the right)\n",
    "    if max_tokens < 1624:\n",
    "        only_response = []\n",
    "        for item in test_prompt:\n",
    "            input = tokenizer([item,], return_tensors=\"pt\", padding=True, truncation=True).to(f\"cuda:{cuda}\")\n",
    "            if input[\"input_ids\"].size(-1) > max_tokens-max_output_tokens:\n",
    "                input = {k: v[:, -(max_tokens - max_output_tokens):] for k, v in input.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**input, max_new_tokens=max_output_tokens)\n",
    "                texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "                texts = texts[0]\n",
    "            if input[\"input_ids\"].size(-1) > max_tokens-max_output_tokens:\n",
    "                max_overlap_len = min(len(item), len(texts))\n",
    "                overlap = next((item[-i:] for i in range(max_overlap_len, 0, -1) if item[-i:] == texts[:i]), \"\")\n",
    "            else:\n",
    "                overlap = item\n",
    "            only_response.append(texts[len(overlap)+1:].lstrip())\n",
    "        return only_response\n",
    "    else:\n",
    "        input = tokenizer(test_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(f\"cuda:{cuda}\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**input, max_new_tokens=max_output_tokens)\n",
    "            texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            only_response = [out[len(test_prompt[index])+1:] for index, out in enumerate(texts)]\n",
    "        return only_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:09.230811Z",
     "iopub.status.busy": "2025-06-24T00:52:09.230611Z",
     "iopub.status.idle": "2025-06-24T00:52:09.265049Z",
     "shell.execute_reply": "2025-06-24T00:52:09.264353Z",
     "shell.execute_reply.started": "2025-06-24T00:52:09.230794Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate.py\n",
    "\n",
    "\"\"\"\n",
    "Contains evaluation utilities for pytorch-based rewriting methods.\n",
    "To use, simply call `compute_rewrite_quality_zsre` with the\n",
    "appropriate arguments, which returns a dictionary containing them.\n",
    "\"\"\"\n",
    "from easyeditor.models.melo.melo import LORA\n",
    "\n",
    "import typing\n",
    "from itertools import chain\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from easyeditor.util import HyperParams\n",
    "from easyeditor.evaluate.evaluate_utils import (\n",
    "    test_seq2seq_batch_prediction_acc, \n",
    "    test_batch_prediction_acc, \n",
    "    test_prediction_acc,\n",
    "    test_prediction_acc_LLM_judge,\n",
    "    # test_generation_quality, \n",
    "    test_concept_gen,\n",
    "    test_safety_gen,\n",
    "    test_instance_change,\n",
    "    PPL,\n",
    "    OOD_PPL,\n",
    "    kl_loc_loss,\n",
    "    es,\n",
    "    es_per_icl,\n",
    "    per_generation,\n",
    "    F1\n",
    ")\n",
    "\n",
    "def compute_edit_quality(\n",
    "    model,\n",
    "    model_name,\n",
    "    hparams: HyperParams,\n",
    "    tok: AutoTokenizer,\n",
    "    record: typing.Dict,\n",
    "    device,\n",
    "    eval_metric: str = 'token_em',\n",
    "    test_generation = False\n",
    ") -> typing.Dict:\n",
    "    \"\"\"\n",
    "    Given a rewritten model, computes generalization and specificity metrics for\n",
    "    the desired rewrite (passed in via the CounterFact dataset record). Returns a\n",
    "    dictionary containing those metrics.\n",
    "\n",
    "    :param model: Rewritten model\n",
    "    :param tok: Tokenizer\n",
    "    :param record: CounterFact dataset record\n",
    "    :paran snips: ???\n",
    "    :param vec: ???\n",
    "    :return: Dictionary containing rewriting metrics\n",
    "    \"\"\"\n",
    "    if isinstance(model,LORA):\n",
    "        model=model.model\n",
    "    # First, unpack rewrite evaluation record.\n",
    "    target_new, ground_truth = (\n",
    "        record[x] for x in [\"target_new\", \"ground_truth\"]\n",
    "    )\n",
    "\n",
    "    rewrite_prompts = record[\"prompt\"]\n",
    "    rephrase_prompts = record[\"rephrase_prompt\"] if 'rephrase_prompt' in record.keys() else None\n",
    "    ret = compute_rewrite_or_rephrase_quality(model, model_name, hparams, tok,\n",
    "                                              rewrite_prompts, target_new, device=device, eval_metric=eval_metric)\n",
    "\n",
    "    ret['locality'] = {}\n",
    "    ret['portability'] = {}\n",
    "    if rephrase_prompts is not None:\n",
    "        ret.update(\n",
    "            compute_rewrite_or_rephrase_quality(model, model_name, hparams, tok,\n",
    "                                                rephrase_prompts, target_new, device=device, test_rephrase=True, eval_metric=eval_metric)\n",
    "        )\n",
    "\n",
    "    if 'locality' in record.keys() and any(record['locality']):\n",
    "        for locality_key in record['locality'].keys():\n",
    "            ret['locality'].update(\n",
    "                compute_locality_quality(model, model_name, hparams, tok, locality_key,\n",
    "                                         record['locality'][locality_key]['prompt'],\n",
    "                                         record['locality'][locality_key]['ground_truth'], device=device)\n",
    "            )\n",
    "    if 'portability' in record.keys() and any(record['portability']):\n",
    "        for portability_key in record['portability'].keys():\n",
    "            ret['portability'].update(\n",
    "                compute_portability_quality(model, model_name, hparams, tok, portability_key,\n",
    "                                            record['portability'][portability_key]['prompt'],\n",
    "                                            record['portability'][portability_key]['ground_truth'], device=device)\n",
    "            )\n",
    "    if test_generation:\n",
    "        if hparams.alg_name == 'GRACE':\n",
    "            ret['fluency'] = test_generation_quality(model=model,tok=tok,prefixes=rewrite_prompts if isinstance(rewrite_prompts,list) else [rewrite_prompts,], max_out_len=100, vanilla_generation=True)\n",
    "        else:\n",
    "            ret['fluency'] = test_generation_quality(model=model,tok=tok,prefixes=rewrite_prompts if isinstance(rewrite_prompts,list) else [rewrite_prompts,], max_out_len=100, vanilla_generation=False)\n",
    "    return ret\n",
    "\n",
    "def compute_rewrite_or_rephrase_quality(\n",
    "    model,\n",
    "    model_name,\n",
    "    hparams: HyperParams,\n",
    "    tok: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    target_new: str,\n",
    "    device,\n",
    "    test_rephrase: bool = False,\n",
    "    eval_metric: str = 'token_em'\n",
    ") -> typing.Dict:\n",
    "    \n",
    "    if not test_rephrase:\n",
    "        key = 'rewrite'\n",
    "    else:\n",
    "        key = 'rephrase'\n",
    "    # using real-world evaluation: autoregressive decoding, natural stop criteria, LLM-as-a-Judge\n",
    "    if hasattr(hparams, 'evaluation_type') and hparams.evaluation_type == \"LLM-judge\":\n",
    "        acc, gen_content = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, target_new, device, locality=False)\n",
    "        ret = {\n",
    "            f\"{key}_acc\": acc,\n",
    "            f\"{key}_gen_content\": gen_content\n",
    "        }\n",
    "    elif hasattr(hparams, 'evaluation_type') and hparams.evaluation_type == \"generate-text\":\n",
    "        gen_content_model = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, target_new, device, locality=False)\n",
    "        ret = {\n",
    "            f\"{key}_gen_content\": gen_content_model\n",
    "        }\n",
    "    else:  # traditional evaluation \n",
    "        if eval_metric == 'ppl':\n",
    "            ppl = PPL(model, tok, prompt, target_new, device)\n",
    "            ret = {\n",
    "                f\"{key}_ppl\": ppl\n",
    "            }\n",
    "        elif eval_metric == 'ood_ppl':\n",
    "            ans = OOD_PPL(model, tok, prompt, target_new, device)\n",
    "            ret = {\n",
    "                f\"ood_acc\": ans\n",
    "            }\n",
    "        elif hparams.alg_name==\"GRACE\":\n",
    "            # ppl = PPL(model, tok, prompt, target_new, device)\n",
    "            if 't5' in model_name.lower():\n",
    "                acc = test_seq2seq_batch_prediction_acc(model, tok, hparams, prompt, target_new, device)\n",
    "            else:\n",
    "                acc = test_prediction_acc(model, tok, hparams, prompt, target_new, device, vanilla_generation=True)\n",
    "            f1 = F1(model,tok,hparams,prompt,target_new,device, vanilla_generation=True)\n",
    "            ret = {\n",
    "                f\"{key}_acc\": acc,\n",
    "                # f\"{key}_PPL\": ppl,\n",
    "                f\"{key}_F1\":f1     \n",
    "            }        \n",
    "        else:  # teacher-forcing evaluation\n",
    "            if 't5' in model_name.lower():\n",
    "                acc = test_seq2seq_batch_prediction_acc(model, tok, hparams, prompt, target_new, device)\n",
    "            else:\n",
    "                acc = test_prediction_acc(model, tok, hparams, prompt, target_new, device)\n",
    "            ret = {\n",
    "                f\"{key}_acc\": acc\n",
    "            }\n",
    "    return ret\n",
    "\n",
    "def compute_locality_quality(\n",
    "    model,\n",
    "    model_name,\n",
    "    hparams: HyperParams,\n",
    "    tok: AutoTokenizer,\n",
    "    locality_key: str,\n",
    "    prompt: typing.Union[str, List[str]],\n",
    "    locality_ground_truth: typing.Union[str, List[str]],\n",
    "    device,\n",
    ") -> typing.Dict:\n",
    "\n",
    "    # using real-world evaluation: autoregressive decoding, natural stop criteria, LLM-as-a-Judge\n",
    "    if hasattr(hparams, 'evaluation_type'):\n",
    "        loc_tokens = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, locality_ground_truth, device, locality=True)\n",
    "    else:  # traditional evaluation \n",
    "        if 't5' in model_name.lower():\n",
    "            loc_tokens = test_seq2seq_batch_prediction_acc(model, tok, hparams, prompt, locality_ground_truth, device, locality=True)\n",
    "        else:\n",
    "            loc_tokens = test_prediction_acc(model, tok, hparams, prompt, locality_ground_truth, device, locality=True, vanilla_generation=hparams.alg_name=='GRACE')\n",
    "        if type(loc_tokens) is not list:\n",
    "            loc_tokens = [loc_tokens,]\n",
    "\n",
    "    ret = {\n",
    "        f\"{locality_key}_output\": loc_tokens\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "def compute_portability_quality(\n",
    "    model,\n",
    "    model_name,\n",
    "    hparams: HyperParams,\n",
    "    tok: AutoTokenizer,\n",
    "    portability_key: str,\n",
    "    prompt: typing.Union[str, List[str]],\n",
    "    ground_truth: typing.Union[str, List[str]],\n",
    "    device,\n",
    ") -> typing.Dict:\n",
    "    # using real-world evaluation: autoregressive decoding, natural stop criteria, LLM-as-a-Judge\n",
    "    if hasattr(hparams, 'evaluation_type') and hparams.evaluation_type == \"LLM-judge\":\n",
    "        portability_correct = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, ground_truth, device, locality=False)\n",
    "    elif hasattr(hparams, 'evaluation_type') and hparams.evaluation_type == \"generate-text\":\n",
    "        portability_correct = test_prediction_acc_LLM_judge(model, tok, hparams, prompt, ground_truth, device, locality=False)\n",
    "    else:  # traditional evaluation\n",
    "        if 't5' in model_name.lower():\n",
    "            portability_correct = test_seq2seq_batch_prediction_acc(model, tok, hparams, prompt, ground_truth, device)\n",
    "        else:\n",
    "            portability_correct = test_prediction_acc(model, tok, hparams, prompt, ground_truth, device, vanilla_generation=hparams.alg_name=='GRACE')\n",
    "\n",
    "    ret = {\n",
    "        f\"{portability_key}_acc\": portability_correct\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "def compute_icl_edit_quality(\n",
    "        model,\n",
    "        model_name,\n",
    "        hparams: HyperParams,\n",
    "        tok: AutoTokenizer,\n",
    "        icl_examples,\n",
    "        record: typing.Dict,\n",
    "        device,\n",
    "        pre_edit: bool = False,\n",
    "        test_generation = False\n",
    ") -> typing.Dict:\n",
    "    \"\"\"\n",
    "    Given a rewritten model, computes generalization and specificity metrics for\n",
    "    the desired rewrite (passed in via the CounterFact dataset record). Returns a\n",
    "    dictionary containing those metrics.\n",
    "\n",
    "    :param model: Rewritten model\n",
    "    :param tok: Tokenizer\n",
    "    :param record: CounterFact dataset record\n",
    "    :param snips: ???\n",
    "    :param vec: ???\n",
    "    :return: Dictionary containing rewriting metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # First, unpack rewrite evaluation record.\n",
    "    target_new, ground_truth = (\n",
    "        record[x] for x in [\"target_new\", \"ground_truth\"]\n",
    "    )\n",
    "    prompt = record[\"prompt\"]\n",
    "    rephrase = record[\"rephrase_prompt\"] if 'rephrase_prompt' in record.keys() else None\n",
    "    new_fact = f'New Fact: {prompt} {target_new}\\nPrompt: {prompt}'\n",
    "\n",
    "    if pre_edit:\n",
    "        edit_acc = icl_lm_eval(model, model_name, hparams, tok, icl_examples,\n",
    "                               target_new, prompt)\n",
    "    else:\n",
    "        edit_acc = icl_lm_eval(model, model_name, hparams, tok, icl_examples,\n",
    "                               target_new, new_fact)\n",
    "    ret = {\n",
    "        f\"rewrite_acc\": [edit_acc]\n",
    "    }\n",
    "    ret['locality'] = {}\n",
    "    ret['portability'] = {}\n",
    "    if rephrase is not None:\n",
    "        rephrase_acc = icl_lm_eval(model, model_name, hparams, tok, icl_examples,\n",
    "                                   target_new, f'New Fact: {prompt} {target_new}\\nPrompt: {rephrase}')\n",
    "        ret['rephrase_acc'] = rephrase_acc\n",
    "\n",
    "    if 'locality' in record.keys() and any(record['locality']):\n",
    "        for locality_key in record['locality'].keys():\n",
    "            if isinstance(record['locality'][locality_key]['ground_truth'], list):\n",
    "                pre_neighbor = []\n",
    "                post_neighbor = []\n",
    "                for x_a, x_p in zip(record['locality'][locality_key]['ground_truth'],\n",
    "                                    record['locality'][locality_key]['prompt']):\n",
    "                    tmp_pre_neighbor = icl_lm_eval(model, model_name, hparams, tok, [''], x_a,\n",
    "                                                   f\"{x_p}\", neighborhood=True)\n",
    "                    tmp_post_neighbor = icl_lm_eval(model, model_name, hparams, tok, icl_examples, x_a,\n",
    "                                                    f\"New Fact: {prompt} {target_new}\\nPrompt: {x_p}\",\n",
    "                                                    neighborhood=True)\n",
    "                    if type(tmp_pre_neighbor) is not list:\n",
    "                        tmp_pre_neighbor = [tmp_pre_neighbor, ]\n",
    "                    if type(tmp_post_neighbor) is not list:\n",
    "                        tmp_post_neighbor = [tmp_post_neighbor, ]\n",
    "                    assert len(tmp_pre_neighbor) == len(tmp_post_neighbor)\n",
    "                    pre_neighbor.append(tmp_pre_neighbor)\n",
    "                    post_neighbor.append(tmp_post_neighbor)\n",
    "                res = []\n",
    "                for ans, label in zip(pre_neighbor, post_neighbor):\n",
    "                    temp_acc = np.mean(np.equal(ans, label))\n",
    "                    if np.isnan(temp_acc):\n",
    "                        continue\n",
    "                    res.append(temp_acc)\n",
    "                ret['locality'][f'{locality_key}_acc'] = res\n",
    "            else:\n",
    "                pre_neighbor = icl_lm_eval(model, model_name, hparams, tok, [''],\n",
    "                                           record['locality'][locality_key]['ground_truth'],\n",
    "                                           f\"{record['locality'][locality_key]['prompt']}\",\n",
    "                                           neighborhood=True)\n",
    "                post_neighbor = icl_lm_eval(model, model_name, hparams, tok, icl_examples,\n",
    "                                            record['locality'][locality_key]['ground_truth'],\n",
    "                                            f\"New Fact: {prompt} {target_new}\\nPrompt: {record['locality'][locality_key]['prompt']}\",\n",
    "                                            neighborhood=True)\n",
    "                if type(pre_neighbor) is not list:\n",
    "                    pre_neighbor = [pre_neighbor, ]\n",
    "                if type(post_neighbor) is not list:\n",
    "                    post_neighbor = [post_neighbor, ]\n",
    "                assert len(pre_neighbor) == len(post_neighbor)\n",
    "\n",
    "                ret['locality'][f'{locality_key}_acc'] = np.mean(np.equal(pre_neighbor, post_neighbor))\n",
    "    # Form a list of lists of prefixes to test.\n",
    "    if 'portability' in record.keys() and any(record['portability']):\n",
    "        for portability_key in record['portability'].keys():\n",
    "            if pre_edit:\n",
    "                icl_input = ['']\n",
    "                x_prefix = \"\"\n",
    "            else:\n",
    "                icl_input = icl_examples\n",
    "                x_prefix = f\"New Fact: {prompt} {target_new}\\nPrompt: \"\n",
    "            if isinstance(record['portability'][portability_key]['ground_truth'], list):\n",
    "                portability_acc = []\n",
    "                for x_a, x_p in zip(record['portability'][portability_key]['ground_truth'],\n",
    "                                    record['portability'][portability_key]['prompt']):\n",
    "                    tmp_portability_acc = icl_lm_eval(model, model_name, hparams, tok, icl_input, x_a,\n",
    "                                                      f\"{x_prefix}{x_p}\")\n",
    "                portability_acc.append(tmp_portability_acc)\n",
    "            else:\n",
    "                portability_acc = icl_lm_eval(model, model_name, hparams, tok, icl_input,\n",
    "                                              record['portability'][portability_key]['ground_truth'],\n",
    "                                              f\"{x_prefix}{record['portability'][portability_key]['prompt']}\")\n",
    "            ret['portability'][f'{portability_key}_acc'] = portability_acc\n",
    "\n",
    "    if test_generation:\n",
    "        ret['fluency'] = test_generation_quality(model=model,tok=tok, prefixes=new_fact if isinstance(new_fact,list) else [new_fact,], max_out_len=100, vanilla_generation=False)\n",
    "    return ret\n",
    "\n",
    "def icl_lm_eval(\n",
    "        model,\n",
    "        model_name,\n",
    "        hparams: HyperParams,\n",
    "        tokenizer,\n",
    "        icl_examples,\n",
    "        target,\n",
    "        x,\n",
    "        neighborhood=False\n",
    ")-> typing.Dict:\n",
    "    device = torch.device(f'cuda:{hparams.device}')\n",
    "    if 't5' in model_name.lower():\n",
    "        target_len = len(tokenizer.encode(target))\n",
    "        target_ids = tokenizer(f'{x} {target}', return_tensors='pt')['input_ids'].to(device)\n",
    "        encodings = tokenizer(''.join(icl_examples), return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, labels=target_ids).logits\n",
    "            ans = torch.argmax(logits, dim=-1)[:,-target_len:-1].squeeze()\n",
    "            target_ids = target_ids[:,-target_len:-1]\n",
    "            if neighborhood:\n",
    "                return ans.squeeze().detach().cpu().numpy().tolist()\n",
    "            return torch.mean((ans == target_ids.to(ans.device).squeeze()).float(), dim=-1).detach().cpu().numpy().tolist()\n",
    "    elif 'llama' in model_name.lower():\n",
    "        target_ids = tokenizer(target, return_tensors='pt')['input_ids'].to(device)\n",
    "        encodings = tokenizer(''.join(icl_examples) + f'{x} {target}', return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        ans = torch.argmax(logits, dim=-1)[:,-target_ids.size(1):-1].squeeze()\n",
    "        target_ids = target_ids[:,1:]\n",
    "        if neighborhood:\n",
    "            return ans.squeeze().detach().cpu().numpy().tolist()\n",
    "        return torch.mean((ans == target_ids.to(ans.device).squeeze()).float(), dim=-1).detach().cpu().numpy().tolist()\n",
    "    else:\n",
    "        target_ids = tokenizer(' ' + target + '\\n', return_tensors='pt')['input_ids'].to(device)\n",
    "        encodings = tokenizer(''.join(icl_examples) + f'{x} {target}', return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        ans = torch.argmax(logits, dim=-1)[:,-target_ids.size(1):-1].squeeze()\n",
    "        target_ids = target_ids[:,:-1]\n",
    "        if neighborhood:\n",
    "            return ans.squeeze().detach().cpu().numpy().tolist()\n",
    "        return torch.mean((ans == target_ids.to(ans.device).squeeze()).float(), dim=-1).detach().cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:09.266313Z",
     "iopub.status.busy": "2025-06-24T00:52:09.266008Z",
     "iopub.status.idle": "2025-06-24T00:52:09.329010Z",
     "shell.execute_reply": "2025-06-24T00:52:09.328133Z",
     "shell.execute_reply.started": "2025-06-24T00:52:09.266284Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# editor.py\n",
    "\n",
    "from typing import Optional, Union, List, Tuple, Dict\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from easyeditor.models.melo.melo import LORA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, BitsAndBytesConfig\n",
    "from transformers import LlamaTokenizer,PreTrainedTokenizerFast, LlamaTokenizerFast\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from easyeditor.util.globals import *\n",
    "from easyeditor.editors.utils import _chunks, _prepare_requests, summary_metrics\n",
    "from easyeditor.editors.batch_editor import BatchEditor\n",
    "# from easyeditor.evaluate import compute_edit_quality, compute_icl_edit_quality, compute_sent_metric\n",
    "from easyeditor.util import nethook\n",
    "from easyeditor.util.hparams import HyperParams\n",
    "from easyeditor.util.alg_dict import *\n",
    "# from ..evaluate.evaluate_utils import test_generation_quality\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "def make_logs():\n",
    "\n",
    "    f_h, s_h = get_handler('logs', log_name='run.log')\n",
    "    LOG.addHandler(f_h)\n",
    "    LOG.addHandler(s_h)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    if seed >= 10000:\n",
    "        raise ValueError(\"seed number should be less than 10000\")\n",
    "    if torch.distributed.is_initialized():\n",
    "        rank = torch.distributed.get_rank()\n",
    "    else:\n",
    "        rank = 0\n",
    "    seed = (rank * 100000) + seed\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "  \n",
    "class BaseEditor:\n",
    "    \"\"\"Base editor for all methods\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_hparams(cls, hparams: HyperParams):\n",
    "        return cls(hparams)\n",
    "\n",
    "    def __init__(self, hparams: HyperParams):\n",
    "        assert hparams is not None, 'Error: hparams is None.'\n",
    "        self.model_name = hparams.model_name\n",
    "        self.apply_algo = ALG_DICT[hparams.alg_name]\n",
    "        self.alg_name = hparams.alg_name\n",
    "        make_logs()\n",
    "        LOG.info(\"Instantiating model\")\n",
    "\n",
    "        if type(self.model_name) is str:\n",
    "            device_map = 'auto' if hparams.model_parallel else None\n",
    "            torch_dtype = torch.float16 if hasattr(hparams, 'fp16') and hparams.fp16 else torch.float32\n",
    "            \n",
    "            # QLoRA configuration\n",
    "            if hparams.alg_name == 'QLoRA':\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=(hparams.quantization_bit == 4),\n",
    "                    bnb_4bit_use_double_quant=hparams.double_quant,\n",
    "                    bnb_4bit_quant_type=hparams.quant_type,\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "                )\n",
    "                model_kwargs = {\n",
    "                    \"quantization_config\": bnb_config,\n",
    "                    \"torch_dtype\": torch_dtype,\n",
    "                    \"device_map\": {'': hparams.device}\n",
    "                }\n",
    "            else:\n",
    "                model_kwargs = {\n",
    "                    \"torch_dtype\": torch_dtype,\n",
    "                    \"device_map\": device_map\n",
    "                }\n",
    "\n",
    "            if 't5' in self.model_name.lower():\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.model_name, **model_kwargs)\n",
    "                self.tok = T5Tokenizer.from_pretrained(self.model_name)\n",
    "            elif 'chatglm-api' in self.model_name.lower():\n",
    "                self.model, self.tok = None, None\n",
    "                self.hparams = hparams\n",
    "                return\n",
    "            elif 'gpt-3.5' in self.model_name.lower():\n",
    "                self.model, self.tok = None, None\n",
    "            elif 'gpt' in self.model_name.lower():\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name, **model_kwargs)\n",
    "                self.tok = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "                self.tok.pad_token_id = self.tok.eos_token_id\n",
    "            elif 'llama' in self.model_name.lower():\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name, **model_kwargs)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                self.tok.pad_token_id = self.tok.eos_token_id\n",
    "            elif 'baichuan' in self.model_name.lower():\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name, **model_kwargs, trust_remote_code=True)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name,trust_remote_code=True)\n",
    "                self.tok.pad_token_id = self.tok.eos_token_id\n",
    "            elif 'chatglm' in self.model_name.lower():\n",
    "                self.model = AutoModel.from_pretrained(self.model_name,trust_remote_code=True, **model_kwargs)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name,trust_remote_code=True)\n",
    "                if 'chatglm2'in self.model_name.lower(): \n",
    "                    self.tok.unk_token_id = 64787\n",
    "                else: \n",
    "                    self.tok.pad_token_id = self.tok.eos_token_id\n",
    "            elif 'internlm' in self.model_name.lower():\n",
    "                self.model = AutoModel.from_pretrained(self.model_name,trust_remote_code=True, **model_kwargs)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name,trust_remote_code=True)\n",
    "                self.tok.pad_token_id = self.tok.eos_token_id\n",
    "            elif 'qwen2' in self.model_name.lower():\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name,trust_remote_code=True, torch_dtype=torch_dtype if hparams.alg_name not in ['MEND'] else torch.bfloat16, device_map=device_map)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name, eos_token='<|endoftext|>', pad_token='<|endoftext|>',unk_token='<|endoftext|>', trust_remote_code=True)\n",
    "            elif 'qwen' in self.model_name.lower():\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name,fp32=False,trust_remote_code=True, **model_kwargs)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name, eos_token='<|endoftext|>', pad_token='<|endoftext|>',unk_token='<|endoftext|>', trust_remote_code=True)\n",
    "            elif 'mistral' in self.model_name.lower():\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name, **model_kwargs)\n",
    "                self.tok = AutoTokenizer.from_pretrained(self.model_name)\n",
    "                self.tok.pad_token_id = self.tok.eos_token_id\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            if self.tok is not None and (isinstance(self.tok, GPT2Tokenizer) or isinstance(self.tok, GPT2TokenizerFast) or isinstance(self.tok, LlamaTokenizer) or isinstance(self.tok, LlamaTokenizerFast) or isinstance(self.tok, PreTrainedTokenizerFast)) and (hparams.alg_name not in ['ROME', 'MEMIT', 'EMMET', 'R-ROME','AlphaEdit','CORE']):\n",
    "                LOG.info('AutoRegressive Model detected, set the padding side of Tokenizer to left...')\n",
    "                self.tok.padding_side = 'left'\n",
    "            if self.tok is not None and ('mistral' in self.model_name.lower() or 'llama' in self.model_name.lower() or 'qwen' in self.model_name.lower()) and (hparams.alg_name in ['ROME', 'MEMIT', 'EMMET', 'R-ROME','AlphaEdit', 'CORE']):\n",
    "                LOG.info('AutoRegressive Model detected, set the padding side of Tokenizer to right...')\n",
    "                self.tok.padding_side = 'right'\n",
    "        else:\n",
    "            self.model, self.tok = self.model_name\n",
    "\n",
    "        if hparams.model_parallel: \n",
    "            hparams.device = str(self.model.device).split(\":\")[1]\n",
    "        if not hparams.model_parallel and hasattr(hparams, 'device') and hparams.alg_name != 'QLoRA':\n",
    "            self.model.to(f'cuda:{hparams.device}')\n",
    "\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def edit(self,\n",
    "             prompts: Union[str, List[str]],\n",
    "             target_new: Union[str, List[str]],\n",
    "             ground_truth: Optional[Union[str, List[str]]] = None,\n",
    "             target_neg: Optional[Union[str, List[str]]] = None,\n",
    "             rephrase_prompts: Optional[Union[str, List[str]]] = None,\n",
    "             locality_inputs:  Optional[Dict] = None,\n",
    "             portability_inputs: Optional[Dict] = None,\n",
    "             sequential_edit=False,\n",
    "             verbose=True,\n",
    "             **kwargs\n",
    "             ):\n",
    "        \"\"\"\n",
    "        `prompts`: list or str\n",
    "            the prompts to edit\n",
    "        `ground_truth`: str\n",
    "            the ground truth / expected output\n",
    "        `locality_inputs`: dict\n",
    "            for locality\n",
    "        \"\"\"\n",
    "        test_generation = kwargs.pop('test_generation', False)\n",
    "\n",
    "        if isinstance(prompts, List):\n",
    "            assert len(prompts) == len(target_new)\n",
    "        else:\n",
    "            prompts, target_new = [prompts,], [target_new,]\n",
    "\n",
    "        if hasattr(self.hparams, 'batch_size') and not BatchEditor.is_batchable_method(self.alg_name):  # For Singleton Editing, bs=1\n",
    "            assert self.hparams.batch_size == 1, 'Single Editing: batch_size should be set to 1'\n",
    "\n",
    "        if ground_truth is not None:\n",
    "            ground_truth = [ground_truth,] if isinstance(ground_truth, str) else ground_truth\n",
    "        else:# Default ground truth is <|endoftext|>\n",
    "            ground_truth = ['<|endoftext|>'] * (len(prompts))\n",
    "\n",
    "        if \"requests\" in kwargs.keys():\n",
    "            requests = kwargs[\"requests\"]\n",
    "        else:\n",
    "            requests = _prepare_requests(prompts, target_new, ground_truth, target_neg, rephrase_prompts, locality_inputs, portability_inputs, **kwargs)\n",
    "\n",
    "        return self.edit_requests(requests, sequential_edit, verbose, test_generation=test_generation, **kwargs)\n",
    "\n",
    "    def batch_edit(self,\n",
    "                   prompts: List[str],\n",
    "                   target_new: List[str],\n",
    "                   ground_truth: Optional[List[str]] = None,\n",
    "                   target_neg: Optional[List[str]] = None,\n",
    "                   rephrase_prompts: Optional[List[str]] = None,\n",
    "                   locality_inputs: Optional[Dict] = None,\n",
    "                   portability_inputs: Optional[Dict] = None,\n",
    "                   sequential_edit=False,\n",
    "                   verbose=True,\n",
    "                   **kwargs\n",
    "                   ):\n",
    "        \"\"\"\n",
    "        `prompts`: list or str\n",
    "            the prompts to edit\n",
    "        `ground_truth`: str\n",
    "            the ground truth / expected output\n",
    "        \"\"\"\n",
    "        assert len(prompts) == len(target_new)\n",
    "        test_generation = kwargs['test_generation'] if 'test_generation' in kwargs.keys() else False\n",
    "        if ground_truth is not None:\n",
    "            if isinstance(ground_truth, str):\n",
    "                ground_truth = [ground_truth,]\n",
    "            else:\n",
    "                assert len(ground_truth) == len(prompts)\n",
    "        else: # Default ground truth is <|endoftext|>\n",
    "            ground_truth = ['<|endoftext|>' for _ in range(len(prompts))]\n",
    "\n",
    "\n",
    "        assert BatchEditor.is_batchable_method(self.alg_name), f'The Method {self.alg_name} can not batch edit examples.'\n",
    "\n",
    "        requests = _prepare_requests(prompts, target_new, ground_truth, target_neg, rephrase_prompts, locality_inputs, portability_inputs, **kwargs)\n",
    "\n",
    "        assert hasattr(self.hparams, 'batch_size'), f'Method {self.alg_name} found, pls specify the batch_size....'\n",
    "        all_metrics = []\n",
    "        for record_chunks in _chunks(requests, self.hparams.batch_size):\n",
    "            start = time()\n",
    "\n",
    "            edited_model, weights_copy = self.apply_algo(\n",
    "                self.model,\n",
    "                self.tok,\n",
    "                record_chunks,\n",
    "                self.hparams,\n",
    "                copy=False,\n",
    "                return_orig_weights=True\n",
    "            )\n",
    "            exec_time = time() - start\n",
    "            LOG.info(f\"Execution editing took {exec_time}\")\n",
    "\n",
    "            start = time()\n",
    "            chunk_metrics = []\n",
    "            for i, request in enumerate(record_chunks):\n",
    "\n",
    "                metrics = {\n",
    "                    'case_id': i,\n",
    "                    \"requested_rewrite\": request,\n",
    "                    \"time\": exec_time,\n",
    "                    \"post\": compute_edit_quality(edited_model, self.model_name, self.hparams, self.tok, request, self.hparams.device, test_generation=test_generation),\n",
    "                }\n",
    "\n",
    "                chunk_metrics.append(metrics)\n",
    "\n",
    "            if sequential_edit:\n",
    "                self.model = edited_model\n",
    "            else:\n",
    "                if self.alg_name == 'KN' or self.alg_name == 'GRACE' or self.alg_name == 'WISE':\n",
    "                    with torch.no_grad():\n",
    "                        weights_copy()\n",
    "                elif self.alg_name == 'LoRA' or self.alg_name == 'QLoRA' or self.alg_name == 'DPO':\n",
    "                    edited_model.unload()\n",
    "                    del self.model.peft_config\n",
    "                elif self.alg_name == 'MELO':\n",
    "                    self.model = edited_model\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        for k, v in weights_copy.items():\n",
    "                            nethook.get_parameter(self.model, k)[...] = v.to(f\"cuda:{self.hparams.device}\")\n",
    "\n",
    "            for i, request in enumerate(record_chunks):\n",
    "                chunk_metrics[i][\"pre\"] = compute_edit_quality(self.model, self.model_name, self.hparams, self.tok, request, self.hparams.device, test_generation=test_generation)\n",
    "\n",
    "                if verbose:\n",
    "                    LOG.info(\n",
    "                        f\"{i} editing: {request['prompt']} -> {request['target_new']}  \\n {chunk_metrics[i]}\"\n",
    "                    )\n",
    "\n",
    "            LOG.info(f\"Evaluation took {time() - start}\")\n",
    "            all_metrics.extend(chunk_metrics)\n",
    "        return all_metrics, edited_model, weights_copy\n",
    "\n",
    "    def edit_requests(self,\n",
    "             requests,\n",
    "             sequential_edit=False,\n",
    "             verbose=True,\n",
    "             test_generation=False,\n",
    "             **kwargs\n",
    "             ):\n",
    "        \"\"\"\n",
    "        `prompts`: list or str\n",
    "            the prompts to edit\n",
    "        `ground_truth`: str\n",
    "            the ground truth / expected output\n",
    "        `locality_inputs`: dict\n",
    "            for locality\n",
    "        \"\"\"\n",
    "        eval_metric= kwargs['eval_metric'] if 'eval_metric' in kwargs.keys() else 'exact match'\n",
    "        if hasattr(self.hparams, 'batch_size'):  # For Singleton Editing, bs=1\n",
    "            assert self.hparams.batch_size == 1, 'Single Editing: batch_size should be set to 1'\n",
    "        all_metrics = []\n",
    "        if 'pre_edit' in kwargs and kwargs['pre_edit'] is not None:\n",
    "            metrics = kwargs['pre_edit']\n",
    "            all_metrics = metrics\n",
    "        else:\n",
    "            for i, request in enumerate(tqdm(requests)):\n",
    "                if self.alg_name == 'IKE':\n",
    "                    assert 'train_ds' in kwargs.keys(), print('IKE need train_ds(For getting In-Context prompt)')\n",
    "                    metrics = {\"pre\": compute_icl_edit_quality(self.model, self.model_name, self.hparams, self.tok, [''], request, self.hparams.device, pre_edit=True)}\n",
    "                else:\n",
    "                    metrics = {\"pre\": compute_edit_quality(self.model, self.model_name, self.hparams, self.tok, request, self.hparams.device, eval_metric=eval_metric, test_generation=test_generation)}\n",
    "                all_metrics.append(metrics)\n",
    "            if 'pre_file' in kwargs and kwargs['pre_file'] is not None:\n",
    "                json.dump(all_metrics, open(kwargs['pre_file'], 'w'), indent=4)\n",
    "\n",
    "        def edit_func(request):\n",
    "            if self.alg_name == 'IKE' or self.alg_name == 'ICE':\n",
    "                edited_model, weights_copy, icl_examples = self.model, {}, self.apply_algo(\n",
    "                    self.model,\n",
    "                    self.tok,\n",
    "                    [request],\n",
    "                    self.hparams,\n",
    "                    copy=False,\n",
    "                    return_orig_weights=True,\n",
    "                    keep_original_weight=False,\n",
    "                    train_ds=kwargs['train_ds'] if self.alg_name == 'IKE' else None\n",
    "                )\n",
    "            else:\n",
    "                edited_model, weights_copy = self.apply_algo(\n",
    "                    self.model,\n",
    "                    self.tok,\n",
    "                    [request],\n",
    "                    self.hparams,\n",
    "                    copy=False,\n",
    "                    return_orig_weights=True,\n",
    "                    keep_original_weight=False,\n",
    "                    train_ds=kwargs['train_ds'] if self.alg_name == 'IKE' else None\n",
    "                )\n",
    "                icl_examples = None\n",
    "            return edited_model, weights_copy, icl_examples\n",
    "\n",
    "        def edit_evaluation(all_metrics, request, edited_model, idx, test_generation, icl_examples, **kwargs):\n",
    "            eval_metric= kwargs['eval_metric'] if 'eval_metric' in kwargs.keys() else 'exact match'\n",
    "            if self.alg_name == 'IKE':\n",
    "                all_metrics[idx].update({\n",
    "                    'case_id': idx,\n",
    "                    \"requested_rewrite\": request,\n",
    "                    \"post\": compute_icl_edit_quality(self.model, self.model_name, self.hparams, self.tok, icl_examples, request, self.hparams.device ,test_generation=test_generation),\n",
    "                })\n",
    "                if \"metric_kwargs\" in kwargs:\n",
    "                    all_metrics[idx].update(compute_sent_metric(self.model, edited_model, self.model_name, self.hparams, self.tok,metric_kwargs=kwargs[\"metric_kwargs\"][idx], device=self.hparams.device))\n",
    "    \n",
    "            else:\n",
    "                all_metrics[idx].update({\n",
    "                    'case_id': idx,\n",
    "                    \"requested_rewrite\": request,\n",
    "                    \"post\": compute_edit_quality(edited_model, self.model_name, self.hparams, self.tok, request, self.hparams.device, eval_metric=eval_metric, test_generation=test_generation),\n",
    "                })\n",
    "                if \"metric_kwargs\" in kwargs:\n",
    "                    all_metrics[idx].update(compute_sent_metric(self.model, edited_model, self.model_name, self.hparams, self.tok,metric_kwargs=kwargs[\"metric_kwargs\"][idx], device=self.hparams.device))\n",
    "                if 'locality' in all_metrics[idx]['post'].keys() and not hasattr(self.hparams, 'evaluation_type'):\n",
    "                    for locality_key in request['locality'].keys():\n",
    "                        locality_result = []\n",
    "                        if hasattr(self.hparams, 'evaluation_type'):\n",
    "                            locality_result.append(float(all_metrics[idx]['post']['locality'][f'{locality_key}_output']==all_metrics[idx]['pre']['locality'][f'{locality_key}_output']))\n",
    "                        else:\n",
    "                            for ans, label in zip(all_metrics[idx]['post']['locality'][f'{locality_key}_output'], all_metrics[idx]['pre']['locality'][f'{locality_key}_output']):\n",
    "                                locality_result.append(np.mean(np.equal(ans, label)))\n",
    "                        all_metrics[idx]['post']['locality'][f'{locality_key}_acc'] = locality_result\n",
    "                        all_metrics[idx]['post']['locality'].pop(f'{locality_key}_output')\n",
    "                    all_metrics[idx]['pre'].pop('locality')\n",
    "\n",
    "            if verbose:\n",
    "                LOG.info(f\"{idx} editing: {request['prompt']} -> {request['target_new']}  \\n\\n {all_metrics[idx]}\")\n",
    "\n",
    "\n",
    "        if sequential_edit:\n",
    "            for i, request in enumerate(tqdm(requests, total=len(requests))):\n",
    "                edited_model, weights_copy, icl_examples = edit_func(request)\n",
    "            if self.alg_name == 'LoRA' or self.alg_name == 'QLoRA' or self.alg_name == 'DPO':\n",
    "                self.model = edited_model\n",
    "            if self.alg_name == 'WISE' and hasattr(self.hparams, 'save_path') and self.hparams.save_path:\n",
    "                print(\"Start saving the WISE model!\")\n",
    "                edited_model.save(self.hparams.save_path)\n",
    "            for i, request in enumerate(requests):\n",
    "                edit_evaluation(all_metrics, request, edited_model, i, test_generation, icl_examples, **kwargs)\n",
    "        else:\n",
    "            for i, request in enumerate(tqdm(requests, total=len(requests))):\n",
    "                edited_model, weights_copy, icl_examples = edit_func(request)\n",
    "                edit_evaluation(all_metrics, request, edited_model, i, test_generation, icl_examples, **kwargs)\n",
    "                if self.alg_name == 'KN' or self.alg_name == 'GRACE' or self.alg_name == 'WISE':\n",
    "                    with torch.no_grad():\n",
    "                        weights_copy()\n",
    "                elif self.alg_name == 'LoRA' or self.alg_name == 'QLoRA' or self.alg_name == 'DPO':\n",
    "                    edited_model.unload()\n",
    "                    del self.model.peft_config\n",
    "                elif self.alg_name == 'MELO':\n",
    "                    self.model = edited_model\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        for k, v in weights_copy.items():\n",
    "                            nethook.get_parameter(self.model, k)[...] = v.to(f\"cuda:{self.hparams.device}\")\n",
    "\n",
    "\n",
    "        if isinstance(edited_model, LORA):\n",
    "            edited_model = edited_model.model\n",
    "        if not hasattr(self.hparams, 'evaluation_type') or self.hparams.evaluation_type != \"generate-text\":\n",
    "            summary_metrics(all_metrics)\n",
    "\n",
    "        return all_metrics, edited_model, weights_copy\n",
    "\n",
    "    def normal_edit(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        target_new: List[str],\n",
    "        sequential_edit=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `prompts`: list or str\n",
    "            the prompts to edit\n",
    "        `ground_truth`: str\n",
    "            the ground truth / expected output\n",
    "        \"\"\"\n",
    "        assert len(prompts) == len(target_new)\n",
    "        ground_truth = ['<|endoftext|>' for _ in range(len(prompts))]\n",
    "\n",
    "\n",
    "        assert BatchEditor.is_batchable_method(self.alg_name), f'The Method {self.alg_name} can not batch edit examples.'\n",
    "\n",
    "        requests = _prepare_requests(prompts, target_new, ground_truth)\n",
    "\n",
    "        assert hasattr(self.hparams, 'batch_size'), f'Method {self.alg_name} found, pls specify the batch_size....'\n",
    "\n",
    "        # print(f\"[editor.py][batch_edit] `batch_size`={self.hparams.batch_size}\")\n",
    "        # for epc in range(epoch):\n",
    "        #     print(f\"[editor.py][batch_edit] `Epoch` = {epc+1}\")\n",
    "        #     for record_chunks in self._chunks(requests, self.hparams.batch_size):\n",
    "        start = time()\n",
    "\n",
    "        edited_model, weights_copy = self.apply_algo(\n",
    "            self.model,\n",
    "            self.tok,\n",
    "            requests,  # record_chunks -> requests\n",
    "            self.hparams,\n",
    "            copy=False,\n",
    "            return_orig_weights=True,\n",
    "            keep_original_weight=False,\n",
    "        )\n",
    "        exec_time = time() - start\n",
    "        LOG.info(f\"Execution editing took {exec_time}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k, v in weights_copy.items():\n",
    "                nethook.get_parameter(self.model, k)[...] = v.to(f\"cuda:{self.hparams.device}\")\n",
    "\n",
    "        return None, edited_model, weights_copy\n",
    "    \n",
    "    def generate_edit(\n",
    "        self,\n",
    "        prompts: Union[str, List[str]],\n",
    "        target_new: Union[str, List[str]],\n",
    "        ground_truth: Optional[Union[str, List[str]]] = None,\n",
    "        target_neg: Optional[Union[str, List[str]]] = None,\n",
    "        rephrase_prompts: Optional[Union[str, List[str]]] = None,\n",
    "        locality_inputs:  Optional[Dict] = None,\n",
    "        portability_inputs: Optional[Dict] = None,\n",
    "        sequential_edit=False,\n",
    "        verbose=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        eval_metric= kwargs['eval_metric'] if 'eval_metric' in kwargs.keys() else 'exact match'\n",
    "        test_generation = kwargs.pop('test_generation', False)\n",
    "\n",
    "        assert len(prompts) == len(target_new)\n",
    "\n",
    "        if hasattr(self.hparams, 'batch_size'):\n",
    "            assert self.hparams.batch_size == 1, 'Single Editing: batch_size should be set to 1'\n",
    "        \n",
    "        if \"requests\" in kwargs.keys():\n",
    "            requests = kwargs[\"requests\"]\n",
    "        else:\n",
    "            requests = _prepare_requests(prompts, target_new, ground_truth, target_neg, rephrase_prompts, locality_inputs, portability_inputs, **kwargs)\n",
    "        \n",
    "        def text_generate(\n",
    "            model,\n",
    "            model_name,\n",
    "            hparams: HyperParams,\n",
    "            tok: AutoTokenizer,\n",
    "            query,\n",
    "            device,\n",
    "            eval_metric: str = 'token_em',\n",
    "            test_generation = False\n",
    "        ):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "            text = self.tok.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            model_inputs = tok.encode(text, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "            template_length = len(model_inputs[0])\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=model_inputs,\n",
    "                max_new_tokens=512\n",
    "            )\n",
    "            trimmed_generated_ids = generated_ids[0][template_length:]\n",
    "            response = tok.decode(trimmed_generated_ids, skip_special_tokens=True)\n",
    "            return response\n",
    "\n",
    "        all_results = []\n",
    "        if 'pre_edit' in kwargs and kwargs['pre_edit'] is not None:\n",
    "            results = kwargs['pre_edit']\n",
    "            all_results = results\n",
    "        else:\n",
    "            for i, request in enumerate(tqdm(requests)):\n",
    "                results = {}\n",
    "                results['pre'] = {}\n",
    "                results['pre']['rewrite_ans'] = text_generate(self.model, self.model_name, self.hparams, self.tok, request['prompt'], self.hparams.device, eval_metric=eval_metric, test_generation=test_generation)\n",
    "                results['pre']['rephrase_ans'] = text_generate(self.model, self.model_name, self.hparams, self.tok, request['rephrase_prompt'], self.hparams.device, eval_metric=eval_metric, test_generation=test_generation)\n",
    "                por_results = []\n",
    "                for pr in request['portability']['por_hop']['prompt']:\n",
    "                    por_results.append(text_generate(self.model, self.model_name, self.hparams, self.tok, pr, self.hparams.device, eval_metric=eval_metric, test_generation=test_generation))\n",
    "                if 'locality' in request.keys() and 'loc_hop' in request['locality'].keys():\n",
    "                    loc_results = []\n",
    "                    for pr in request['locality']['loc_hop']['prompt']:\n",
    "                        loc_results.append(text_generate(self.model, self.model_name, self.hparams, self.tok, pr, self.hparams.device, eval_metric=eval_metric, test_generation=test_generation))\n",
    "                    results['pre']['locality_ans'] = loc_results\n",
    "                results['pre']['portability_ans'] = por_results\n",
    "                all_results.append(results)\n",
    "            if 'pre_file' in kwargs and kwargs['pre_file'] is not None:\n",
    "                json.dump(all_results, open(kwargs['pre_file'], 'w'), indent=4)\n",
    "\n",
    "        def edit_func(request):\n",
    "            if self.alg_name == 'IKE':\n",
    "                edited_model, weights_copy, icl_examples = self.model, {}, self.apply_algo(\n",
    "                    self.model,\n",
    "                    self.tok,\n",
    "                    [request],\n",
    "                    self.hparams,\n",
    "                    copy=False,\n",
    "                    return_orig_weights=True,\n",
    "                    keep_original_weight=False,\n",
    "                    train_ds=kwargs['train_ds'] if self.alg_name == 'IKE' else None\n",
    "                )\n",
    "            else:\n",
    "                edited_model, weights_copy = self.apply_algo(\n",
    "                    self.model,\n",
    "                    self.tok,\n",
    "                    [request],\n",
    "                    self.hparams,\n",
    "                    copy=False,\n",
    "                    return_orig_weights=True,\n",
    "                    keep_original_weight=False,\n",
    "                    train_ds=kwargs['train_ds'] if self.alg_name == 'IKE' else None\n",
    "                )\n",
    "                icl_examples = None\n",
    "            return edited_model, weights_copy, icl_examples\n",
    "        \n",
    "        def post_edit_results(all_results, request, edited_model, idx, eval_metric, test_generation, icl_examples, **kwargs):\n",
    "            if self.alg_name == 'IKE':\n",
    "                all_results[idx].update({\n",
    "                    'case_id': idx,\n",
    "                    \"requested_rewrite\": request,\n",
    "                    \"post\": compute_icl_edit_quality(self.model, self.model_name, self.hparams, self.tok, icl_examples, request, self.hparams.device),\n",
    "                })\n",
    "            else:\n",
    "                results_post = {}\n",
    "                results_post['rewrite_ans'] = text_generate(edited_model, self.model_name, self.hparams, self.tok, request['prompt'], self.hparams.device, eval_metric=eval_metric, test_generation=test_generation)\n",
    "                results_post['rephrase_ans'] = text_generate(edited_model, self.model_name, self.hparams, self.tok, request['rephrase_prompt'], self.hparams.device, eval_metric=eval_metric, test_generation=test_generation)\n",
    "                por_results = []\n",
    "                for pr in request['portability']['por_hop']['prompt']:\n",
    "                    por_results.append(text_generate(edited_model, self.model_name, self.hparams, self.tok, pr, self.hparams.device, eval_metric=eval_metric, test_generation=test_generation))\n",
    "                if 'locality' in request.keys() and 'loc_hop' in request['locality'].keys():\n",
    "                    loc_results = []\n",
    "                    for pr in request['locality']['loc_hop']['prompt']:\n",
    "                        loc_results.append(text_generate(edited_model, self.model_name, self.hparams, self.tok, pr, self.hparams.device, eval_metric=eval_metric, test_generation=test_generation))\n",
    "                    results_post['locality_ans'] = loc_results\n",
    "                results_post['portability_ans'] = por_results\n",
    "                if test_generation:\n",
    "                    if self.hparams.alg_name == 'GRACE':\n",
    "                        results_post['fluency'] = test_generation_quality(model=edited_model,tok=self.tok,prefixes=request['prompt'] if isinstance(request['prompt'],list) else [request['prompt'],], max_out_len=100, vanilla_generation=True)\n",
    "                    else:\n",
    "                        results_post['fluency'] = test_generation_quality(model=edited_model,tok=self.tok,prefixes=request['prompt'] if isinstance(request['prompt'],list) else [request['prompt'],], max_out_len=100, vanilla_generation=False)\n",
    "                all_results[idx].update({\n",
    "                    'case_id': idx,\n",
    "                    \"requested_rewrite\": request,\n",
    "                    \"post\": results_post\n",
    "                })\n",
    "            if verbose:\n",
    "                LOG.info(f\"{idx} editing: {request['prompt']} -> {request['target_new']}\")\n",
    "\n",
    "        if sequential_edit:\n",
    "            for i, request in enumerate(tqdm(requests, total=len(requests))):\n",
    "                edited_model, weights_copy, icl_examples = edit_func(request)\n",
    "            for i, request in enumerate(requests):\n",
    "                post_edit_results(all_results, request, edited_model, i, eval_metric, test_generation, icl_examples, **kwargs)\n",
    "        else:\n",
    "            for i, request in enumerate(tqdm(requests, total=len(requests))):\n",
    "                edited_model, weights_copy, icl_examples = edit_func(request)\n",
    "                post_edit_results(all_results, request, edited_model, i, eval_metric, test_generation, icl_examples, **kwargs)\n",
    "                if self.alg_name == 'KN' or self.alg_name == 'GRACE' or self.alg_name == 'WISE':\n",
    "                    with torch.no_grad():\n",
    "                        weights_copy()\n",
    "                elif self.alg_name == 'LoRA' or self.alg_name == 'QLoRA' or self.alg_name == 'DPO':\n",
    "                    edited_model.unload()\n",
    "                    del self.model.peft_config\n",
    "                elif self.alg_name == 'MELO':\n",
    "                    self.model = edited_model\n",
    "                elif self.alg_name == 'LoRA' or self.alg_name == 'QLoRA' or self.alg_name == 'DPO':\n",
    "                    self.model = edited_model\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        for k, v in weights_copy.items():\n",
    "                            nethook.get_parameter(self.model, k)[...] = v.to(f\"cuda:{self.hparams.device}\")\n",
    "\n",
    "        if isinstance(edited_model, LORA):\n",
    "            edited_model = edited_model.model\n",
    "        if len(all_results) != 0:\n",
    "            summary_metrics(all_results)\n",
    "\n",
    "        return all_results, edited_model, weights_copy\n",
    "\n",
    "    def deep_edit(\n",
    "        self,\n",
    "        datasets\n",
    "    ):\n",
    "        metrics = self.apply_algo(datasets, self.hparams)\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:09.336737Z",
     "iopub.status.busy": "2025-06-24T00:52:09.336465Z",
     "iopub.status.idle": "2025-06-24T00:52:11.174731Z",
     "shell.execute_reply": "2025-06-24T00:52:11.173831Z",
     "shell.execute_reply.started": "2025-06-24T00:52:09.336710Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 14 files:   0%|                                 | 0/14 [00:00<?, ?it/s]Downloading 'benchmark/WikiBio/wikibio-train-all.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/WikiBio/8rs6DLxZ_QZwK6fLwCiUot6Qc1I=.fd11768426fe24820d7b99e11acd4db7e2277721.incomplete'\n",
      "Downloading '.gitattributes' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.98ddd117e08ef652502840374e3c926421b9010d.incomplete'\n",
      "Downloading 'benchmark/Convsent/blender_train.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/Convsent/i-mF4aBv7Lkq4rpoc_Thnmm6zL8=.0ebd6d70e31127e3c93dd2f235bc4963e9b3bf4e6219a96c5aa91beab85aa01c.incomplete'\n",
      "Downloading 'README.md' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.f690f31c4f3bd8334ec8a032a30b36784414ed9b.incomplete'\n",
      "Downloading 'benchmark/WikiBio/wikibio-test-all.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/WikiBio/Huumcl7_kkObDM_jO3_CKhFJt00=.c4503c3ae7d09e8d88ab522162db7c9dc7145d68.incomplete'\n",
      "Downloading 'benchmark/Convsent/blender_val.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/Convsent/fx9F75P0rVvXKMjHxLzGlzK0N6E=.98b69a13f438a25ac5f0d56dfc6e4f0150d30836.incomplete'\n",
      "Downloading 'benchmark/ZsRE/ZsRE-test-all.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/ZsRE/o5LGPIEg1EjkOuxgs9IOBWsg4QA=.759a1d0c7b87d4d1612f655dbbac4cb8262b2948.incomplete'\n",
      "\n",
      "blender_train.json:   0%|                           | 0.00/24.0M [00:00<?, ?B/s]\u001b[ADownloading 'benchmark/Convsent/blender_test.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/Convsent/uG92iOpRw2ySGVaHQtkdFcCzJmY=.cf80f02b519e2fd910c59b216aa8bc81a50d8b18.incomplete'\n",
      "\n",
      "\n",
      "wikibio-train-all.json:   0%|                        | 0.00/454k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "README.md: 100%|███████████████████████████| 23.8k/23.8k [00:00<00:00, 85.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/README.md\n",
      "\n",
      "\n",
      "\n",
      "wikibio-test-all.json:   0%|                         | 0.00/310k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "blender_val.json:   0%|                             | 0.00/1.33M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".gitattributes: 100%|██████████████████████| 2.38k/2.38k [00:00<00:00, 21.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/.gitattributes\n",
      "Fetching 14 files:   7%|█▊                       | 1/14 [00:00<00:03,  4.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ZsRE-test-all.json:   0%|                           | 0.00/1.52M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_test.json:   0%|                            | 0.00/1.33M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "blender_train.json: 100%|███████████████████| 24.0M/24.0M [00:00<00:00, 159MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/Convsent/blender_train.json\n",
      "Downloading 'benchmark/trivia/trivia_qa_test.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/trivia/hsqfUMul1hcF0Rkn77-lciu0xCs=.bb182169de846c83a46195102df05f27a6027bfc.incomplete'\n",
      "\n",
      "\n",
      "ZsRE-test-all.json: 100%|██████████████████| 1.52M/1.52M [00:00<00:00, 18.8MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/ZsRE/ZsRE-test-all.json\n",
      "wikibio-train-all.json: 100%|████████████████| 454k/454k [00:00<00:00, 3.33MB/s]\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-train-all.json\n",
      "\n",
      "\n",
      "\n",
      "wikibio-test-all.json: 100%|█████████████████| 310k/310k [00:00<00:00, 2.40MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading 'benchmark/trivia/trivia_qa_train.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/trivia/Ux66o4_aGuv2cMX6Oty__PK66_Q=.a377bf2206b746e4da285adb6cf9063f64f69421.incomplete'\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-test-all.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_val.json: 100%|████████████████████| 1.33M/1.33M [00:00<00:00, 9.52MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/Convsent/blender_val.json\n",
      "Downloading 'benchmark/wiki_counterfact/test_cf.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_counterfact/MFXtFn6NK8VL1PB-H6pzAHNNtJk=.8b209c5ecdd63de81e3a289356864c7269065654.incomplete'\n",
      "\n",
      "trivia_qa_train.json:   0%|                         | 0.00/87.0k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "blender_test.json: 100%|███████████████████| 1.33M/1.33M [00:00<00:00, 6.79MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/Convsent/blender_test.json\n",
      "Fetching 14 files:  21%|█████▎                   | 3/14 [00:00<00:01,  6.57it/s]Downloading 'benchmark/wiki_counterfact/train_cf.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_counterfact/oHHCcKqsl_RwMeOewKIysf-yTIY=.a1cde7b1dc12053bbfa70841e3e8acc53b9af02a.incomplete'\n",
      "trivia_qa_train.json: 100%|████████████████| 87.0k/87.0k [00:00<00:00, 1.33MB/s]\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/trivia/trivia_qa_train.json\n",
      "\n",
      "trivia_qa_test.json: 100%|█████████████████| 95.1k/95.1k [00:00<00:00, 12.2MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/trivia/trivia_qa_test.json\n",
      "Downloading 'benchmark/wiki_recent/recent_test.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_recent/2Oq5wtIJdPWr5VpLBoGf8aOq-7s=.ff99392090c38c986a71b8e68457ab3b540fb8ae.incomplete'\n",
      "Downloading 'benchmark/wiki_recent/recent_train.json' to '/kaggle/working/EasyEdit/data/.cache/huggingface/download/benchmark/wiki_recent/rZ6nU74FMB4p4evUjqJWa5ZVFBk=.d17f09a23a43b4385b5f030a03002c09270dbf85.incomplete'\n",
      "\n",
      "test_cf.json:   0%|                                 | 0.00/2.92M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "train_cf.json:   0%|                                | 0.00/1.39M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "recent_test.json:   0%|                             | 0.00/4.08M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "recent_train.json:   0%|                            | 0.00/1.83M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "train_cf.json: 100%|███████████████████████| 1.39M/1.39M [00:00<00:00, 10.6MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/train_cf.json\n",
      "\n",
      "test_cf.json: 100%|████████████████████████| 2.92M/2.92M [00:00<00:00, 10.5MB/s]\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/test_cf.json\n",
      "Fetching 14 files:  79%|██████████████████▊     | 11/14 [00:00<00:00, 15.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "recent_train.json: 100%|███████████████████| 1.83M/1.83M [00:00<00:00, 7.03MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "recent_train.json: 100%|███████████████████| 1.83M/1.83M [00:00<00:00, 6.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_train.json\n",
      "recent_test.json: 100%|████████████████████| 4.08M/4.08M [00:00<00:00, 12.2MB/s]\n",
      "Download complete. Moving file to /kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_test.json\n",
      "Fetching 14 files: 100%|████████████████████████| 14/14 [00:01<00:00, 13.79it/s]\n",
      "/kaggle/working/EasyEdit/data\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "# %mkdir data\n",
    "!huggingface-cli download zjunlp/KnowEdit --repo-type dataset --local-dir /kaggle/working/EasyEdit/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:11.176190Z",
     "iopub.status.busy": "2025-06-24T00:52:11.175873Z",
     "iopub.status.idle": "2025-06-24T00:52:11.188687Z",
     "shell.execute_reply": "2025-06-24T00:52:11.187685Z",
     "shell.execute_reply.started": "2025-06-24T00:52:11.176165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EDITING_METHOD = 'MELO' # FT, ICE, LoRA, MELO\n",
    "is_ice = False\n",
    "if EDITING_METHOD == 'ICE':\n",
    "    is_ice = True\n",
    "\n",
    "if EDITING_METHOD != 'KGEdit':\n",
    "    if EDITING_METHOD == 'ICE':\n",
    "        EDITING_METHOD = 'IKE'\n",
    "        is_ice = True\n",
    "    \n",
    "    if MODEL == 'gpt2-xl':\n",
    "        with open(f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}/gpt2-xl.yaml', 'r', encoding='utf-8') as f:\n",
    "            config = f.read()\n",
    "        \n",
    "        if EDITING_METHOD == 'SERAC':\n",
    "            config = re.sub(r'(?<=model_name: )\\./hugging_cache', 'openai-community', config)\n",
    "            config = re.sub(r'(?<=small_name: )\\./hugging_cache', 'openai-community', config)\n",
    "            config = re.sub(r'(?<=cls_name: )\\./hugging_cache', 'distilbert', config)\n",
    "            config = re.sub(r'(?<=archive: )\\./results/models/SERAC/gpt2-xl', '/kaggle/working/EasyEdit/results/models/SERAC/gpt2-xl_bak', config)\n",
    "            \n",
    "        elif EDITING_METHOD == 'MEND':\n",
    "            with open(f'/kaggle/working/EasyEdit/hparams/TRAINING/{EDITING_METHOD}/gpt2-xl.yaml', 'r', encoding='utf-8') as tf:\n",
    "                training_config = tf.read()\n",
    "            config = re.sub(r'(?<=model_name: )\\./hugging_cache', 'openai-community', config)\n",
    "            training_config = re.sub(r'(?<=model_name: )\\./hugging_cache', 'openai-community', training_config)\n",
    "            training_config = re.sub(r'(?<=tokenizer_name: )\\./hugging_cache', 'openai-community', training_config)\n",
    "            config = re.sub(r'(?<=archive: )\\./results/models/MEND/gpt2-xl', '/kaggle/working/EasyEdit/results/models/MEND/gpt2-xl_bak', config)\n",
    "        \n",
    "            with open(f'/kaggle/working/EasyEdit/hparams/TRAINING/{EDITING_METHOD}/gpt2-xl.yaml', 'w', encoding='utf-8') as tf:\n",
    "                tf.write(training_config)\n",
    "        \n",
    "        elif EDITING_METHOD == 'MELO':\n",
    "            with open(f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}/gpt2-xl.yaml', 'r', encoding='utf-8') as f:\n",
    "                config = f.read()\n",
    "            config = re.sub(r'\\./hugging_cache/gpt2-xl', 'openai-community/gpt2-xl', config)\n",
    "            config = re.sub(r'\\./hugging_cache/distilbert-base-cased', 'distilbert/distilbert-base-cased', config)\n",
    "            \n",
    "        else:\n",
    "            config = re.sub(r'(?<=model_name: \").+(?=/gpt2-xl\")', 'openai-community', config)\n",
    "            \n",
    "    elif 'qwen' in MODEL:\n",
    "        model_size = re.search(r'(?<=-)(\\d\\.)?\\db', MODEL).group(0)\n",
    "        model_type = '-instruct'\n",
    "        if model_type not in MODEL:\n",
    "            model_type = ''\n",
    "        if EDITING_METHOD == 'MELO':\n",
    "            with open(f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}/gpt2-xl.yaml', 'r', encoding='utf-8') as f:\n",
    "                config = f.read()\n",
    "            config = re.sub(r'\\./hugging_cache/gpt2-xl', f'/kaggle/input/qwen2.5/transformers/{model_size}{model_type}/1', config)\n",
    "            config = re.sub(r'\\./hugging_cache/distilbert-base-cased', 'distilbert/distilbert-base-cased', config)\n",
    "            config = re.sub(r'(?<=class_name: ).+(?=\\n)', 'Qwen2ForCausalLM', config)\n",
    "            config = re.sub(r'(?<=tokenizer_class: ).+(?=\\n)', 'Qwen2Tokenizer', config)\n",
    "            config = re.sub(r'(?<=grace_layer: )transformer.h.35.mlp.c_fc', 'model.layers.8.mlp.down_proj', config)\n",
    "            config = re.sub(r'transformer.h.36.mlp.c_fc', 'model.layers.10.mlp.gate_proj\\n    - model.layers.10.mlp.up_proj', config)\n",
    "            config = re.sub(r'transformer.h.37.mlp.c_fc', 'model.layers.11.mlp.gate_proj\\n    - model.layers.11.mlp.up_proj', config)\n",
    "        else:\n",
    "            with open(f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}/qwen2.5-7b.yaml', 'r', encoding='utf-8') as f:\n",
    "                config = f.read()\n",
    "            config = re.sub(r'(?<=\\nmodel_name: \").+(?=\")', f'/kaggle/input/qwen2.5/transformers/{model_size}{model_type}/1', config)\n",
    "        \n",
    "    config = re.sub(r'(?<=device: )\\d+', '0', config)\n",
    "    config = re.sub(r'(?<=batch_size: )\\d+', '1', config)\n",
    "    \n",
    "    if is_ice is True:\n",
    "        EDITING_METHOD = 'ICE'\n",
    "        os.makedirs(f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}', exist_ok=True)\n",
    "    \n",
    "    with open(f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}/{MODEL}.yaml', 'w', encoding='utf-8') as f:\n",
    "        f.write(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-16T18:49:02.295179Z",
     "iopub.status.busy": "2025-06-16T18:49:02.294846Z",
     "iopub.status.idle": "2025-06-16T18:49:02.299544Z",
     "shell.execute_reply": "2025-06-16T18:49:02.298804Z",
     "shell.execute_reply.started": "2025-06-16T18:49:02.295147Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:11.189836Z",
     "iopub.status.busy": "2025-06-24T00:52:11.189598Z",
     "iopub.status.idle": "2025-06-24T00:52:11.213452Z",
     "shell.execute_reply": "2025-06-24T00:52:11.212589Z",
     "shell.execute_reply.started": "2025-06-24T00:52:11.189817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval(result_path: str):\n",
    "    if not path.exists(result_path):\n",
    "        raise FileNotFoundError(f'File {result_path} not found!')\n",
    "    \n",
    "    with open(result_path,'r') as file:\n",
    "        datas=json.load(file)\n",
    "    #data_rome_counterfact['post'].keys()  dict_keys(['rewrite_acc', 'locality', 'portability'])\n",
    "    Edit_Succ_list=[data_rome_counterfact['post']['rewrite_acc'][0] for data_rome_counterfact in datas]\n",
    "    Edit_Succ=sum(Edit_Succ_list)/len(Edit_Succ_list)*100\n",
    "    print('Edit_Succ:',Edit_Succ)\n",
    "    \n",
    "    Portability_list=[]\n",
    "    for data_rome_counterfact in datas:\n",
    "        case_list=[]\n",
    "        for key in data_rome_counterfact['post']['portability'].keys():\n",
    "            case_list.append(sum(data_rome_counterfact['post']['portability'][key])/len(data_rome_counterfact['post']['portability'][key])*100)\n",
    "        if len(case_list) != 0:\n",
    "            Portability_list.append(np.mean(case_list))\n",
    "    Overall_portability = np.mean(Portability_list)\n",
    "    print('Overall_portability:',Overall_portability)\n",
    "\n",
    "    Locality_list=[]\n",
    "    for data_rome_counterfact in datas:\n",
    "        case_list=[]\n",
    "        for key in data_rome_counterfact['post']['locality'].keys():\n",
    "            case_list.append(sum(data_rome_counterfact['post']['locality'][key])/len(data_rome_counterfact['post']['locality'][key])*100)\n",
    "        if len(case_list) != 0:\n",
    "            Locality_list.append(np.mean(case_list))\n",
    "    Overall_locality = np.mean(Locality_list)\n",
    "    print('Overall_locality:',Overall_locality)\n",
    "    \n",
    "    Fluency_list=[x['post']['fluency']['ngram_entropy'] for x in datas]\n",
    "    Fluency=sum(Fluency_list)/len(Fluency_list)*100\n",
    "    print('Fluency:',Fluency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:11.215064Z",
     "iopub.status.busy": "2025-06-24T00:52:11.214718Z",
     "iopub.status.idle": "2025-06-24T00:52:11.237180Z",
     "shell.execute_reply": "2025-06-24T00:52:11.236310Z",
     "shell.execute_reply.started": "2025-06-24T00:52:11.215031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_edit(\n",
    "    hparams_file: str,\n",
    "    data_dir: str,\n",
    "    editing_method: str = 'ROME',\n",
    "    ds_size=None,\n",
    "    metrics_save_dir: str = './output',\n",
    "    datatype=None, # counterfact, recent, zsre or wikibio\n",
    "    pre_file=None, #pre-edit file name\n",
    "    train_data_path=None, # Used for IKE\n",
    "    use_chat_template=False\n",
    "):\n",
    "        \n",
    "    print('--Knowledge Editing Pipeline started...--')\n",
    "    editing_methods = {\n",
    "        'FT': FTHyperParams,\n",
    "        'ICE': IKEHyperParams,\n",
    "        'LoRA': LoRAHyperParams,\n",
    "        'MELO': MELOHyperParams,\n",
    "    }\n",
    "    if editing_method not in editing_methods:\n",
    "        raise NotImplementedError('Unknown editing method! The following methods are supported:\\n' + ';\\n'.join([method for method in editing_methods]))\n",
    "    \n",
    "    editing_hparams = editing_methods[editing_method]\n",
    "    if EDITING_METHOD == 'MEND':\n",
    "        print('--Additional Training for MEND--')\n",
    "        training_hparams = MENDTrainingHparams.from_hparams('/kaggle/working/EasyEdit/hparams/TRAINING/MEND/gpt2-xl.yaml')\n",
    "        train_ds = ZsreDataset('/kaggle/working/EasyEdit/data/zsre/zsre_mend_train_10000.json', config=training_hparams)\n",
    "        eval_ds = ZsreDataset('/kaggle/working/EasyEdit/data/zsre/zsre_mend_eval.json', config=training_hparams)\n",
    "\n",
    "        trainer = EditTrainer(\n",
    "            config=training_hparams,\n",
    "            train_set=train_ds,\n",
    "            val_set=eval_ds\n",
    "        )\n",
    "\n",
    "        trainer.run()\n",
    "\n",
    "    print('--Initialising dataset--')\n",
    "\n",
    "    datas = KnowEditDataset(data_dir, size=ds_size)\n",
    "    \n",
    "    if datatype == 'counterfact' or datatype == 'recent' or datatype == 'zsre':\n",
    "        prompts=[data['prompt'] for data in datas]\n",
    "        subjects=[data['subject'] for data in datas]\n",
    "        target_new = [data['target_new'] for data in datas]\n",
    "        \n",
    "        portability_r =[data['portability_r'] for data in datas]\n",
    "        portability_s =[data['portability_s'] for data in datas]\n",
    "        portability_l =[data['portability_l'] for data in datas]\n",
    "\n",
    "        portability_reasoning_prompts=[]\n",
    "        portability_reasoning_ans=[]\n",
    "        portability_Logical_Generalization_prompts=[]\n",
    "        portability_Logical_Generalization_ans=[]\n",
    "        portability_Subject_Aliasing_prompts=[]\n",
    "        portability_Subject_Aliasing_ans=[]\n",
    "        \n",
    "        portability_data = [portability_r,portability_s,portability_l]\n",
    "        portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]\n",
    "        portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]\n",
    "        \n",
    "        for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):\n",
    "            for item in data:\n",
    "                if item is None:\n",
    "                    portable_prompts.append(None)\n",
    "                    portable_answers.append(None)\n",
    "                else:\n",
    "                    temp_prompts = []\n",
    "                    temp_answers = []\n",
    "                    for pr in item:\n",
    "                        prompt=pr[\"prompt\"]\n",
    "                        an=pr[\"ground_truth\"]\n",
    "                        while isinstance(an,list):\n",
    "                            an = an[0]\n",
    "                        if an.strip() ==\"\":\n",
    "                            continue\n",
    "                        temp_prompts.append(prompt)\n",
    "                        temp_answers.append(an)\n",
    "                    portable_prompts.append(temp_prompts)\n",
    "                    portable_answers.append(temp_answers)\n",
    "        assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)\n",
    "        \n",
    "        locality_rs = [data['locality_rs'] for data in datas]\n",
    "        locality_f = [data['locality_f'] for data in datas]\n",
    "        locality_Relation_Specificity_prompts=[]\n",
    "        locality_Relation_Specificity_ans=[]\n",
    "        locality_Forgetfulness_prompts=[]        \n",
    "        locality_Forgetfulness_ans=[]\n",
    "        \n",
    "        locality_data = [locality_rs, locality_f]\n",
    "        locality_prompts = [locality_Relation_Specificity_prompts,locality_Forgetfulness_prompts]\n",
    "        locality_answers = [locality_Relation_Specificity_ans,locality_Forgetfulness_ans]\n",
    "        \n",
    "        for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "            for item in data:\n",
    "                if item is None:\n",
    "                    local_prompts.append(None)\n",
    "                    local_answers.append(None)\n",
    "                else:\n",
    "                    temp_prompts = []\n",
    "                    temp_answers = []\n",
    "                    for pr in item:\n",
    "                        prompt=pr[\"prompt\"]\n",
    "                        an=pr[\"ground_truth\"]\n",
    "                        while isinstance(an,list):\n",
    "                            an = an[0]\n",
    "                        if an.strip() ==\"\":\n",
    "                            continue\n",
    "                        temp_prompts.append(prompt)\n",
    "                        temp_answers.append(an)\n",
    "                    local_prompts.append(temp_prompts)\n",
    "                    local_answers.append(temp_answers)\n",
    "        assert len(prompts) == len(locality_Relation_Specificity_prompts) == len(locality_Forgetfulness_prompts)\n",
    "        \n",
    "        locality_inputs = {}\n",
    "        portability_inputs = {}\n",
    "        \n",
    "        locality_inputs = {\n",
    "            'Relation_Specificity':{\n",
    "                'prompt': locality_Relation_Specificity_prompts,\n",
    "                'ground_truth': locality_Relation_Specificity_ans\n",
    "            },\n",
    "            'Forgetfulness':{\n",
    "                'prompt': locality_Forgetfulness_prompts,\n",
    "                'ground_truth': locality_Forgetfulness_ans\n",
    "            }\n",
    "        }\n",
    "        portability_inputs = {\n",
    "            'Subject_Aliasing':{\n",
    "                'prompt': portability_Subject_Aliasing_prompts,\n",
    "                'ground_truth': portability_Subject_Aliasing_ans\n",
    "            },\n",
    "            'reasoning':{\n",
    "                'prompt': portability_reasoning_prompts,\n",
    "                'ground_truth': portability_reasoning_ans           \n",
    "            },\n",
    "            'Logical_Generalization':{\n",
    "                'prompt': portability_Logical_Generalization_prompts,\n",
    "                'ground_truth': portability_Logical_Generalization_ans           \n",
    "            }\n",
    "        }\n",
    "\n",
    "    if datatype == 'wikibio':\n",
    "        prompts=[data['prompt'] for data in datas]\n",
    "        subjects=[data['subject'] for data in datas]\n",
    "        target_new = [data['target_new'] for data in datas]\n",
    "        \n",
    "        locality_rs = [data['locality_rs'] for data in datas]\n",
    "        locality_f = [data['locality_f'] for data in datas]\n",
    "        locality_Relation_Specificity_prompts=[]\n",
    "        locality_Relation_Specificity_ans=[]\n",
    "        \n",
    "        locality_data = [locality_rs]\n",
    "        locality_prompts = [locality_Relation_Specificity_prompts]\n",
    "        locality_answers = [locality_Relation_Specificity_ans]\n",
    "        for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "            for item in data:\n",
    "                if item is None:\n",
    "                    local_prompts.append(None)\n",
    "                    local_answers.append(None)\n",
    "                else:\n",
    "                    temp_prompts = []\n",
    "                    temp_answers = []\n",
    "                    for pr in item:\n",
    "                        prompt=pr[\"prompt\"]\n",
    "                        an=pr[\"ground_truth\"]\n",
    "                        while isinstance(an,list):\n",
    "                            an = an[0]\n",
    "                        if an.strip() ==\"\":\n",
    "                            continue\n",
    "                        temp_prompts.append(prompt)\n",
    "                        temp_answers.append(an)\n",
    "                    local_prompts.append(temp_prompts)\n",
    "                    local_answers.append(temp_answers)\n",
    "        assert len(prompts) == len(locality_Relation_Specificity_prompts)\n",
    "        portability_inputs = None\n",
    "        locality_inputs = {}\n",
    "        locality_inputs = {\n",
    "            'Relation_Specificity':{\n",
    "                'prompt': locality_Relation_Specificity_prompts,\n",
    "                'ground_truth': locality_Relation_Specificity_ans\n",
    "            }\n",
    "        }\n",
    "\n",
    "    print('--Dataset initialised. Collecting editing hyperparameters--')\n",
    "\n",
    "    hparams = None\n",
    "    if editing_method != 'KGEdit':\n",
    "        hparams = editing_hparams.from_hparams(hparams_file)\n",
    "        \n",
    "    if pre_file is None:\n",
    "        if editing_method != 'KGEdit':\n",
    "            pre_file = f\"./{hparams.model_name.split('/')[-1]}_{datatype}_pre_edit.json\"\n",
    "        else:\n",
    "            pre_file = f\"./{MODEL}_{datatype}_pre_edit.json\"\n",
    "    print(f'pre edit file: {pre_file}')\n",
    "\n",
    "    if pre_file is not None and os.path.exists(pre_file):\n",
    "        with open(pre_file,'r', encoding='utf-8') as pre_f:\n",
    "            pre_edit = json.load(pre_f)\n",
    "        assert len(pre_edit) == len(prompts)\n",
    "    else:\n",
    "        pre_edit = None\n",
    "\n",
    "    if editing_method == 'IKE':\n",
    "        train_ds = KnowEditDataset(train_data_path)\n",
    "        sentence_model = SentenceTransformer(hparams.sentence_model_name).to(f'cuda:{hparams.device}')\n",
    "        encode_ike_facts(sentence_model, train_ds, hparams)\n",
    "    elif editing_method == 'ICE':\n",
    "        hparams.use_icl_examples = False\n",
    "        train_ds = None\n",
    "    else:\n",
    "        train_ds = None\n",
    "    print('--Preparation complete. Running Knowledge Editing--')\n",
    "    \n",
    "    if editing_method == 'KGEdit':\n",
    "        editor = KGEditor(hparams, use_nli=use_nli, use_chat_template=use_chat_template)\n",
    "        # metrics, edited_model = editor.edit_requests(\n",
    "        # requests=requests, \n",
    "        # **kwargs\n",
    "        # )\n",
    "    else:\n",
    "        editor = BaseEditor.from_hparams(hparams)\n",
    "    metrics, edited_model, _ = editor.edit(\n",
    "        prompts=prompts,\n",
    "        target_new=target_new,\n",
    "        subject=subjects,\n",
    "        locality_inputs=locality_inputs,\n",
    "        portability_inputs=portability_inputs,\n",
    "        train_ds=train_ds,\n",
    "        keep_original_weight=True,\n",
    "        pre_file=pre_file,\n",
    "        pre_edit=pre_edit,\n",
    "        test_generation=True,\n",
    "    )\n",
    "    print('--Knowledge edited successfully. Saving results--')\n",
    "\n",
    "    if not os.path.exists(metrics_save_dir):\n",
    "        os.makedirs(metrics_save_dir)\n",
    "    result_path = os.path.join(metrics_save_dir, f'{editing_method}_{datatype}_{hparams.model_name.split(\"/\")[-1]}_results.json')\n",
    "    with open(result_path, 'w', encoding='utf-8') as res_f:\n",
    "        json.dump(metrics, res_f, indent=4)\n",
    "    print('--Saving complete.\\tEvaluating results--')\n",
    "    eval(result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T00:52:11.238397Z",
     "iopub.status.busy": "2025-06-24T00:52:11.238086Z",
     "iopub.status.idle": "2025-06-24T01:03:40.357911Z",
     "shell.execute_reply": "2025-06-24T01:03:40.357039Z",
     "shell.execute_reply.started": "2025-06-24T00:52:11.238369Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Knowledge Editing Pipeline started...--\n",
      "--Initialising dataset--\n",
      "--Dataset initialised. Collecting editing hyperparameters--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 00:52:11,278 - __main__ - INFO - Instantiating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre edit file: ./1_recent_pre_edit.json\n",
      "--Preparation complete. Running Knowledge Editing--\n",
      "We are creating the logger files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 00:52:26,678 - __main__ - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "100%|██████████| 50/50 [02:43<00:00,  3.28s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]/kaggle/working/EasyEdit/easyeditor/models/melo/peft_egg/src/peft/tuners/melo.py:254: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Grace Layer is found: model.layers.8.mlp.down_proj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c9efeb12a74e5c98bceacadd687f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d319d114e6d425a8929fd1c1c0920d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e4e95be6734a268f6bd261834ed30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d52fa9d10344342ba1bbef90e9f2715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 00:55:25,282 - __main__ - INFO - 0 editing: The gender of Ingrīda Amantova is -> female  \n",
      "\n",
      " {'pre': {'rewrite_acc': [1.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.23880373256265}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'The gender of Ingrīda Amantova is', 'target_new': 'female', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Ingrīda Amantova is', 'The name of the country of citizenship of Ingrīda Amantova is', 'The name of the alma mater of Ingrīda Amantova is', 'The occupation of Ingrīda Amantova is', 'The name of the award Ingrīda Amantova won is'], 'ground_truth': ['Cēsis', 'Soviet Union', 'University of Latvia', 'luger', 'Cross of Recognition']}}, 'subject': 'Ingrīda Amantova'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.198130696140149}}}\n",
      "  2%|▏         | 1/50 [00:12<10:22, 12.70s/it]2025-06-24 00:55:35,182 - __main__ - INFO - 1 editing: The name of the ethnic group which Sohayla Eldeeb is associated with is -> Egyptians  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.125441678093181}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the ethnic group which Sohayla Eldeeb is associated with is', 'target_new': 'Egyptians', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Sohayla Eldeeb is', 'The name of the country of citizenship of Sohayla Eldeeb is', 'The name of the alma mater of Sohayla Eldeeb is', 'The occupation of Sohayla Eldeeb is'], 'ground_truth': ['female', 'United States of America', 'Stanford University', 'climate activist']}, 'Forgetfulness': {'prompt': ['The name of the ethnic group which Sohayla Eldeeb is associated with, which is not Egyptians, is'], 'ground_truth': ['Egyptians']}}, 'subject': 'Sohayla Eldeeb'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.140028506889584}}}\n",
      "  4%|▍         | 2/50 [00:22<08:50, 11.05s/it]2025-06-24 00:55:45,363 - __main__ - INFO - 2 editing: The name of the league which Billy White plays in is -> NCAA Division I men's basketball  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}, 'fluency': {'ngram_entropy': 6.232700656886531}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'The name of the league which Billy White plays in is', 'target_new': \"NCAA Division I men's basketball\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Billy White is', 'The place of birth of Billy White is', 'The name of the country of citizenship of Billy White is', 'The name of the sports team which Billy White is a member of is', 'The name of the alma mater of Billy White is', 'The occupation of Billy White is'], 'ground_truth': ['male', 'Las Vegas', 'United States of America', \"San Diego State Aztecs men's basketball\", 'San Diego State University', 'basketball player']}, 'Forgetfulness': {'prompt': [\"The name of the league which Billy White plays in, which is not NCAA Division I men's basketball, is\"], 'ground_truth': [\"NCAA Division I men's basketball\"]}}, 'subject': 'Billy White'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.369474227970857}}}\n",
      "  6%|▌         | 3/50 [00:32<08:20, 10.66s/it]2025-06-24 00:55:55,511 - __main__ - INFO - 3 editing: The gender of Alfhild Tamm is -> female  \n",
      "\n",
      " {'pre': {'rewrite_acc': [1.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.2855707098924745}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'The gender of Alfhild Tamm is', 'target_new': 'female', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the father of Alfhild Tamm is', 'The names of the siblings of Alfhild Tamm are', 'The place of birth of Alfhild Tamm is', 'The place of death of Alfhild Tamm is', 'The place of burial of Alfhild Tamm is', 'The name of the country of citizenship of Alfhild Tamm is', 'The occupation of Alfhild Tamm is', 'The name of the award Alfhild Tamm won is'], 'ground_truth': ['Klas Oscar Sebastian Tamm', 'Hildegard Tamm', 'Tveta församling', 'Lidingö församling', 'Norra begravningsplatsen', 'Sweden', 'psychiatrist', 'Honorary Doctor at Karolinska Institutet']}}, 'subject': 'Alfhild Tamm'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.321360426226673}}}\n",
      "  8%|▊         | 4/50 [00:42<08:00, 10.46s/it]2025-06-24 00:56:05,631 - __main__ - INFO - 4 editing: The names of the cast members of Purgatory are -> Sam Shepard  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.2477882090230175}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'The names of the cast members of Purgatory are', 'target_new': 'Sam Shepard', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of Purgatory is', 'The name of the composer of Purgatory is'], 'ground_truth': ['Uli Edel', 'Brad Fiedel']}, 'Forgetfulness': {'prompt': ['The names of the cast members of Purgatory, which is not Sam Shepard, is'], 'ground_truth': ['Sam Shepard']}}, 'subject': 'Purgatory'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.301496530413857}}}\n",
      " 10%|█         | 5/50 [00:53<07:45, 10.33s/it]2025-06-24 00:56:15,767 - __main__ - INFO - 5 editing: The number of children Adolf Miethe has is -> 2  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.266407887971818}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'The number of children Adolf Miethe has is', 'target_new': '2', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Adolf Miethe is', 'The name of the father of Adolf Miethe is', 'The name of the child of Adolf Miethe is', 'The gender of Adolf Miethe is', 'The place of birth of Adolf Miethe is', 'The place of death of Adolf Miethe is', 'The name of the country of citizenship of Adolf Miethe is', 'The name of the position held by Adolf Miethe is', 'The name of the alma mater of Adolf Miethe is', 'The occupation of Adolf Miethe is', 'The name of the employer of Adolf Miethe is', 'The name of the field of work of Adolf Miethe is', 'The name of the award Adolf Miethe won is', 'The name of the religion which Adolf Miethe is associated with is'], 'ground_truth': ['Karoline Miethe', 'Albert Miethe', 'Käthe Miethe', 'male', 'Potsdam', 'Berlin', 'Kingdom of Prussia', 'wissenschaftlicher Assistent', 'Hermann-von-Helmholtz-Gymnasium', 'photographer', 'Technische Hochschule Berlin', 'color photography', 'honorary citizenship', 'Lutheranism']}}, 'subject': 'Adolf Miethe'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.113752009703365}}}\n",
      " 12%|█▏        | 6/50 [01:03<07:31, 10.27s/it]2025-06-24 00:56:25,629 - __main__ - INFO - 6 editing: The name of the sports team which Adel Taarabt is a member of is -> Genoa CFC  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 5.649564708748887}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The name of the sports team which Adel Taarabt is a member of is', 'target_new': 'Genoa CFC', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Adel Taarabt is', 'The place of birth of Adel Taarabt is', 'The name of the country of citizenship of Adel Taarabt is', 'The occupation of Adel Taarabt is', 'The name of the league which Adel Taarabt plays in is', 'The name of the religion which Adel Taarabt is associated with is'], 'ground_truth': ['male', 'Fez', 'Morocco', 'association football player', 'Premier League', 'Islam']}, 'Forgetfulness': {'prompt': ['The name of the sports team which Adel Taarabt is a member of, which is not Genoa CFC, is'], 'ground_truth': ['Queens Park Rangers F.C.']}}, 'subject': 'Adel Taarabt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 5.938190756200209}}}\n",
      " 14%|█▍        | 7/50 [01:13<07:15, 10.13s/it]2025-06-24 00:56:35,683 - __main__ - INFO - 7 editing: The eye color of Dorthe Damsgaard is -> blue  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.2177412561522525}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'The eye color of Dorthe Damsgaard is', 'target_new': 'blue', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Dorthe Damsgaard is', 'The place of birth of Dorthe Damsgaard is', 'The name of the country of citizenship of Dorthe Damsgaard is', 'The occupation of Dorthe Damsgaard is'], 'ground_truth': ['female', 'Copenhagen', 'Denmark', 'pornographic actor']}}, 'subject': 'Dorthe Damsgaard'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.192530285688118}}}\n",
      " 16%|█▌        | 8/50 [01:23<07:04, 10.11s/it]2025-06-24 00:56:45,708 - __main__ - INFO - 8 editing: The name of the country which Lac Otelnuk is associated with is -> Canada  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.0]}, 'fluency': {'ngram_entropy': 6.35698923801468}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The name of the country which Lac Otelnuk is associated with is', 'target_new': 'Canada', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country Lac Otelnuk is associated with is', 'The name of the currency in the country Lac Otelnuk is associated with is', 'The official language of the country Lac Otelnuk is associated with is', 'The official language of the country Lac Otelnuk is associated with is', 'The name of the continent which the country Lac Otelnuk is associated with is part of is', 'The name of the head of government of the country Lac Otelnuk is associated with is', 'The name of the anthem of the country Lac Otelnuk is associated with is', 'The name of the head of state of the country Lac Otelnuk is associated with is'], 'ground_truth': ['Ottawa', 'Canadian dollar', 'English', 'French', 'North America', 'Justin Trudeau', 'O Canada', 'Charles III of the United Kingdom']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Lac Otelnuk is part of is', 'The name of the currency in Lac Otelnuk is', 'The official language of Lac Otelnuk is', 'The name of the anthem that is most likely to be performed in Lac Otelnuk is'], 'ground_truth': ['North America', 'Canadian dollar', 'English', 'O Canada']}}, 'locality': {}, 'subject': 'Lac Otelnuk'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.0]}, 'fluency': {'ngram_entropy': 5.793528034183471}}}\n",
      " 18%|█▊        | 9/50 [01:33<06:53, 10.08s/it]2025-06-24 00:56:55,675 - __main__ - INFO - 9 editing: The occupation of Elmar Mock is -> scientist  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.372862253887696}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'The occupation of Elmar Mock is', 'target_new': 'scientist', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Elmar Mock is', 'The place of birth of Elmar Mock is', 'The name of the country of citizenship of Elmar Mock is', 'The name of the award Elmar Mock won is'], 'ground_truth': ['male', 'La Chaux-de-Fonds', 'Switzerland', 'European Inventor Award']}, 'Forgetfulness': {'prompt': ['The occupation of Elmar Mock, which is not scientist, is'], 'ground_truth': ['engineer']}}, 'subject': 'Elmar Mock'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.293114692212424}}}\n",
      " 20%|██        | 10/50 [01:43<06:41, 10.05s/it]2025-06-24 00:57:05,798 - __main__ - INFO - 10 editing: The name of the country which Bulkley Ranges is associated with is -> Canada  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.6666666666666666], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 6.212177468914689}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The name of the country which Bulkley Ranges is associated with is', 'target_new': 'Canada', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country Bulkley Ranges is associated with is', 'The name of the currency in the country Bulkley Ranges is associated with is', 'The official language of the country Bulkley Ranges is associated with is', 'The official language of the country Bulkley Ranges is associated with is', 'The name of the continent which the country Bulkley Ranges is associated with is part of is', 'The name of the head of government of the country Bulkley Ranges is associated with is', 'The name of the anthem of the country Bulkley Ranges is associated with is', 'The name of the head of state of the country Bulkley Ranges is associated with is'], 'ground_truth': ['Ottawa', 'Canadian dollar', 'English', 'French', 'North America', 'Justin Trudeau', 'O Canada', 'Charles III of the United Kingdom']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Bulkley Ranges is part of is', 'The name of the currency in Bulkley Ranges is', 'The official language of Bulkley Ranges is', 'The name of the anthem that is most likely to be performed in Bulkley Ranges is'], 'ground_truth': ['North America', 'Canadian dollar', 'English', 'O Canada']}}, 'locality': {}, 'subject': 'Bulkley Ranges'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.6666666666666666], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 6.135668233677686}}}\n",
      " 22%|██▏       | 11/50 [01:53<06:32, 10.07s/it]2025-06-24 00:57:15,631 - __main__ - INFO - 11 editing: The name of the country which Mézidon-Canon is the capital of is -> canton of Mézidon Vallée d'Auge  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.36363636363636365], 'portability': {'reasoning_acc': [0.375]}, 'fluency': {'ngram_entropy': 5.9691816865039}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'The name of the country which Mézidon-Canon is the capital of is', 'target_new': \"canton of Mézidon Vallée d'Auge\", 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country which Mézidon-Canon is the capital of is'], 'ground_truth': [\"Mézidon Vallée d'Auge\"]}}, 'locality': {}, 'subject': 'Mézidon-Canon'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.375]}, 'fluency': {'ngram_entropy': 6.189595891559905}}}\n",
      " 24%|██▍       | 12/50 [02:03<06:19, 10.00s/it]2025-06-24 00:57:25,758 - __main__ - INFO - 12 editing: The official language of San Marcos La Laguna is -> Spanish  \n",
      "\n",
      " {'pre': {'rewrite_acc': [1.0], 'portability': {}, 'fluency': {'ngram_entropy': 5.31394910219219}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'The official language of San Marcos La Laguna is', 'target_new': 'Spanish', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which San Marcos La Laguna is associated with is'], 'ground_truth': ['Guatemala']}, 'Forgetfulness': {'prompt': ['The official language of San Marcos La Laguna, which is not Spanish, is'], 'ground_truth': ['Spanish']}}, 'subject': 'San Marcos La Laguna'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 5.86817436193328}}}\n",
      " 26%|██▌       | 13/50 [02:13<06:11, 10.04s/it]2025-06-24 00:57:35,784 - __main__ - INFO - 13 editing: The gender of Elżbieta Krzesińska is -> female  \n",
      "\n",
      " {'pre': {'rewrite_acc': [1.0], 'portability': {}, 'fluency': {'ngram_entropy': 5.922022594502591}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'The gender of Elżbieta Krzesińska is', 'target_new': 'female', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Elżbieta Krzesińska is', 'The place of death of Elżbieta Krzesińska is', 'The place of burial of Elżbieta Krzesińska is', 'The name of the country of citizenship of Elżbieta Krzesińska is', 'The name of the alma mater of Elżbieta Krzesińska is', 'The occupation of Elżbieta Krzesińska is', 'The name of the award Elżbieta Krzesińska won is'], 'ground_truth': ['Warsaw', 'Warsaw', 'Powązki Military Cemetery', 'Poland', 'Medical University of Gdańsk', 'athletics competitor', 'Polish Sportspersonality of the Year']}}, 'subject': 'Elżbieta Krzesińska'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.2368978385446745}}}\n",
      " 28%|██▊       | 14/50 [02:23<06:01, 10.03s/it]2025-06-24 00:57:45,899 - __main__ - INFO - 14 editing: The number of children Agusman Effendi has is -> 2  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.2512855599497215}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'The number of children Agusman Effendi has is', 'target_new': '2', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Agusman Effendi is', 'The name of the country of citizenship of Agusman Effendi is', 'The occupation of Agusman Effendi is', 'The name of the religion which Agusman Effendi is associated with is'], 'ground_truth': ['male', 'Indonesia', 'politician', 'Islam']}}, 'subject': 'Agusman Effendi'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.222996479624242}}}\n",
      " 30%|███       | 15/50 [02:33<05:52, 10.06s/it]2025-06-24 00:57:56,026 - __main__ - INFO - 15 editing: The name of the alma mater of Peter Sliker is -> Harvard University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.277050435184311}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'The name of the alma mater of Peter Sliker is', 'target_new': 'Harvard University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Peter Sliker is', 'The place of burial of Peter Sliker is', 'The name of the country of citizenship of Peter Sliker is', 'The occupation of Peter Sliker is'], 'ground_truth': ['male', 'Lower Valley Union Cemetery', 'United States of America', 'military officer']}, 'Forgetfulness': {'prompt': ['The name of the alma mater of Peter Sliker, which is not Harvard University, is'], 'ground_truth': ['Harvard University']}}, 'subject': 'Peter Sliker'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.35698923801468}}}\n",
      " 32%|███▏      | 16/50 [02:43<05:42, 10.08s/it]2025-06-24 00:58:05,989 - __main__ - INFO - 16 editing: The name of the country which Kaltasy is the capital of is -> Kaltasinsky selsoviet  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.14285714285714285], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.30898770808544}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'The name of the country which Kaltasy is the capital of is', 'target_new': 'Kaltasinsky selsoviet', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country which Kaltasy is the capital of is'], 'ground_truth': ['Kaltasy']}}, 'locality': {}, 'subject': 'Kaltasy'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.974122875230528}}}\n",
      " 34%|███▍      | 17/50 [02:53<05:31, 10.04s/it]2025-06-24 00:58:16,101 - __main__ - INFO - 17 editing: The eye color of Francesca Petitjean is -> blue  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.364925745951188}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'The eye color of Francesca Petitjean is', 'target_new': 'blue', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Francesca Petitjean is', 'The place of birth of Francesca Petitjean is', 'The name of the country of citizenship of Francesca Petitjean is', 'The occupation of Francesca Petitjean is'], 'ground_truth': ['female', 'Cannes', 'France', 'pornographic actor']}}, 'subject': 'Francesca Petitjean'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.310161696543082}}}\n",
      " 36%|███▌      | 18/50 [03:03<05:22, 10.06s/it]2025-06-24 00:58:26,248 - __main__ - INFO - 18 editing: The name of the award Hans-Erich Voss won is -> Knight's Cross of the Iron Cross  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {}, 'fluency': {'ngram_entropy': 6.08687254826234}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'The name of the award Hans-Erich Voss won is', 'target_new': \"Knight's Cross of the Iron Cross\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Hans-Erich Voss is', 'The place of birth of Hans-Erich Voss is', 'The place of death of Hans-Erich Voss is', 'The name of the country of citizenship of Hans-Erich Voss is', 'The occupation of Hans-Erich Voss is'], 'ground_truth': ['male', 'Angermünde', 'Berchtesgaden', 'Germany', 'naval officer']}, 'Forgetfulness': {'prompt': [\"The name of the award Hans-Erich Voss won, which is not Knight's Cross of the Iron Cross, is\"], 'ground_truth': [\"Knight's Cross of the Iron Cross\"]}}, 'subject': 'Hans-Erich Voss'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.0766465597702135}}}\n",
      " 38%|███▊      | 19/50 [03:13<05:12, 10.09s/it]2025-06-24 00:58:36,211 - __main__ - INFO - 19 editing: The gender of Duramente is -> male organism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.326635905404543}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The gender of Duramente is', 'target_new': 'male organism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Duramente is', 'The name of the father of Duramente is', 'The name of the child of Duramente is'], 'ground_truth': ['Admire Groove', 'King Kamehameha', 'Titleholder']}}, 'subject': 'Duramente'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.372862253887696}}}\n",
      " 40%|████      | 20/50 [03:23<05:01, 10.05s/it]2025-06-24 00:58:46,123 - __main__ - INFO - 20 editing: The name of the sports team which João Victor de Albuquerque Bruno is a member of is -> RCD Mallorca  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.313693152904948}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'The name of the sports team which João Victor de Albuquerque Bruno is a member of is', 'target_new': 'RCD Mallorca', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of João Victor de Albuquerque Bruno is', 'The place of birth of João Victor de Albuquerque Bruno is', 'The name of the country of citizenship of João Victor de Albuquerque Bruno is', 'The occupation of João Victor de Albuquerque Bruno is'], 'ground_truth': ['male', 'Olinda', 'Brazil', 'association football player']}, 'Forgetfulness': {'prompt': ['The name of the sports team which João Victor de Albuquerque Bruno is a member of, which is not RCD Mallorca, is'], 'ground_truth': ['Associação Desportiva São Caetano']}}, 'subject': 'João Victor de Albuquerque Bruno'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.3393356956830775}}}\n",
      " 42%|████▏     | 21/50 [03:33<04:50, 10.01s/it]2025-06-24 00:58:56,047 - __main__ - INFO - 21 editing: The name of the religion which Sant'Eulalia Church is associated with is -> Catholicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.0485012577171355}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"The name of the religion which Sant'Eulalia Church is associated with is\", 'target_new': 'Catholicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': [\"The name of the country which Sant'Eulalia Church is associated with is\"], 'ground_truth': ['Italy']}, 'Forgetfulness': {'prompt': [\"The name of the religion which Sant'Eulalia Church is associated with, which is not Catholicism, is\"], 'ground_truth': ['Catholicism']}}, 'subject': \"Sant'Eulalia Church\"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.114774348000131}}}\n",
      " 44%|████▍     | 22/50 [03:43<04:39,  9.98s/it]2025-06-24 00:59:05,913 - __main__ - INFO - 22 editing: The name of the country which Biksēre is the capital of is -> Sarkaņi Parish  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.597454521430633}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'The name of the country which Biksēre is the capital of is', 'target_new': 'Sarkaņi Parish', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country which Biksēre is the capital of is'], 'ground_truth': ['Biksēre']}}, 'locality': {}, 'subject': 'Biksēre'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.245555847202683}}}\n",
      " 46%|████▌     | 23/50 [03:53<04:28,  9.95s/it]2025-06-24 00:59:15,826 - __main__ - INFO - 23 editing: The name of the religion which Israelites collecting Manna is associated with is -> Christianity  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.187663566767252}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'The name of the religion which Israelites collecting Manna is associated with is', 'target_new': 'Christianity', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Forgetfulness': {'prompt': ['The name of the religion which Israelites collecting Manna is associated with, which is not Christianity, is'], 'ground_truth': ['Christianity']}}, 'subject': 'Israelites collecting Manna'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.0262358605071755}}}\n",
      " 48%|████▊     | 24/50 [04:03<04:18,  9.94s/it]2025-06-24 00:59:26,005 - __main__ - INFO - 24 editing: The names of the cast members of Rush Hour are -> Aimee Garcia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}, 'fluency': {'ngram_entropy': 6.295896536459553}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'The names of the cast members of Rush Hour are', 'target_new': 'Aimee Garcia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of Rush Hour is'], 'ground_truth': ['Jon Turteltaub']}, 'Forgetfulness': {'prompt': ['The names of the cast members of Rush Hour, which is not Aimee Garcia, is'], 'ground_truth': ['Jon Foo']}}, 'subject': 'Rush Hour'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.1513500748451255}}}\n",
      " 50%|█████     | 25/50 [04:13<04:10, 10.01s/it]2025-06-24 00:59:35,845 - __main__ - INFO - 25 editing: The name of the continent which Ongal Peak is part of is -> Antarctica  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.068722102645724}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The name of the continent which Ongal Peak is part of is', 'target_new': 'Antarctica', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ongal Peak'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}, 'fluency': {'ngram_entropy': 6.117140545942644}}}\n",
      " 52%|█████▏    | 26/50 [04:23<03:59,  9.96s/it]2025-06-24 00:59:46,173 - __main__ - INFO - 26 editing: The name of the award Vlado Jug won is -> Slovenian Hockey Hall of Fame  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.331252632804808}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'The name of the award Vlado Jug won is', 'target_new': 'Slovenian Hockey Hall of Fame', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Vlado Jug is', 'The place of birth of Vlado Jug is', 'The name of the country of citizenship of Vlado Jug is', 'The name of the sports team which Vlado Jug is a member of is', 'The occupation of Vlado Jug is'], 'ground_truth': ['male', 'Jesenice', 'Slovenia', 'HDD Olimpija Ljubljana', 'ice hockey player']}, 'Forgetfulness': {'prompt': ['The name of the award Vlado Jug won, which is not Slovenian Hockey Hall of Fame, is'], 'ground_truth': ['Slovenian Hockey Hall of Fame']}}, 'subject': 'Vlado Jug'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.204393698152616}}}\n",
      " 54%|█████▍    | 27/50 [04:33<03:51, 10.07s/it]2025-06-24 00:59:56,252 - __main__ - INFO - 27 editing: The name of the position held by Ondřej Rut is -> deputy mayor  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.269113927247803}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The name of the position held by Ondřej Rut is', 'target_new': 'deputy mayor', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Ondřej Rut is', 'The place of birth of Ondřej Rut is', 'The name of the country of citizenship of Ondřej Rut is', 'The name of the alma mater of Ondřej Rut is', 'The occupation of Ondřej Rut is'], 'ground_truth': ['male', 'Prague', 'Czech Republic', 'Faculty of Humanities, Charles University', 'politician']}, 'Forgetfulness': {'prompt': ['The name of the position held by Ondřej Rut, which is not deputy mayor, is'], 'ground_truth': ['district representative in Czechia']}}, 'subject': 'Ondřej Rut'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.237493924610892}}}\n",
      " 56%|█████▌    | 28/50 [04:43<03:41, 10.07s/it]2025-06-24 01:00:06,401 - __main__ - INFO - 28 editing: The occupation of Mäğsüm Säläxef is -> scientist  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.223585086850897}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'The occupation of Mäğsüm Säläxef is', 'target_new': 'scientist', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Mäğsüm Säläxef is', 'The place of birth of Mäğsüm Säläxef is', 'The name of the alma mater of Mäğsüm Säläxef is', 'The name of the award Mäğsüm Säläxef won is'], 'ground_truth': ['male', 'Rudny', 'Kazan Federal University', 'Merited Scientist of the Russian Federation']}, 'Forgetfulness': {'prompt': ['The occupation of Mäğsüm Säläxef, which is not scientist, is'], 'ground_truth': ['scientist']}}, 'subject': 'Mäğsüm Säläxef'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.149160926980908}}}\n",
      " 58%|█████▊    | 29/50 [04:53<03:32, 10.10s/it]2025-06-24 01:00:16,197 - __main__ - INFO - 29 editing: The name of the religion which Église Saint-Augustin de Deauville is associated with is -> Catholicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 5.944884797204256}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'The name of the religion which Église Saint-Augustin de Deauville is associated with is', 'target_new': 'Catholicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Église Saint-Augustin de Deauville is associated with is'], 'ground_truth': ['France']}, 'Forgetfulness': {'prompt': ['The name of the religion which Église Saint-Augustin de Deauville is associated with, which is not Catholicism, is'], 'ground_truth': ['Catholicism']}}, 'subject': 'Église Saint-Augustin de Deauville'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 5.988502292461461}}}\n",
      " 60%|██████    | 30/50 [05:03<03:20, 10.01s/it]2025-06-24 01:00:26,450 - __main__ - INFO - 30 editing: The official language of Dehaqan is -> Persian  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.321360426226673}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'The official language of Dehaqan is', 'target_new': 'Persian', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Dehaqan is associated with is', 'The name of the country which Dehaqan is the capital of is'], 'ground_truth': ['Iran', 'Dehaqan County']}, 'Forgetfulness': {'prompt': ['The official language of Dehaqan, which is not Persian, is'], 'ground_truth': ['Persian']}}, 'subject': 'Dehaqan'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.313230344925859}}}\n",
      " 62%|██████▏   | 31/50 [05:13<03:11, 10.08s/it]2025-06-24 01:00:36,612 - __main__ - INFO - 31 editing: The names of the cast members of The Game are -> Brian Cox  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.151258208365592}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'The names of the cast members of The Game are', 'target_new': 'Brian Cox', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of The Game is', 'The name of the screenwriter of The Game is', 'The name of the composer of The Game is'], 'ground_truth': ['Niall MacCormick', 'Toby Whithouse', 'Daniel Pemberton']}, 'Forgetfulness': {'prompt': ['The names of the cast members of The Game, which is not Brian Cox, is'], 'ground_truth': ['Tom Hughes']}}, 'subject': 'The Game'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.378006282870104}}}\n",
      " 64%|██████▍   | 32/50 [05:24<03:01, 10.10s/it]2025-06-24 01:00:46,776 - __main__ - INFO - 32 editing: The number of children Takis Hadjigeorgiou has is -> 1  \n",
      "\n",
      " {'pre': {'rewrite_acc': [1.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.0879537422379375}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'The number of children Takis Hadjigeorgiou has is', 'target_new': '1', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Takis Hadjigeorgiou is', 'The place of birth of Takis Hadjigeorgiou is', 'The name of the country of citizenship of Takis Hadjigeorgiou is', 'The name of the position held by Takis Hadjigeorgiou is', 'The name of the alma mater of Takis Hadjigeorgiou is', 'The occupation of Takis Hadjigeorgiou is'], 'ground_truth': ['male', 'Pano Akourdaleia', 'Cyprus', 'member of the House of Representatives of Cyprus', 'National and Kapodistrian University of Athens', 'politician']}}, 'subject': 'Takis Hadjigeorgiou'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.014805508564427}}}\n",
      " 66%|██████▌   | 33/50 [05:34<02:52, 10.12s/it]2025-06-24 01:00:57,035 - __main__ - INFO - 33 editing: The name of the country which Royal Ontario Museum is associated with is -> Canada  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0], 'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 6.0501719317250995}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'The name of the country which Royal Ontario Museum is associated with is', 'target_new': 'Canada', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country which ROM is associated with is'], 'ground_truth': ['Canada']}, 'reasoning': {'prompt': ['The name of the capital city of the country Royal Ontario Museum is associated with is', 'The name of the currency in the country Royal Ontario Museum is associated with is', 'The official language of the country Royal Ontario Museum is associated with is', 'The official language of the country Royal Ontario Museum is associated with is', 'The name of the continent which the country Royal Ontario Museum is associated with is part of is', 'The name of the head of government of the country Royal Ontario Museum is associated with is', 'The name of the anthem of the country Royal Ontario Museum is associated with is', 'The name of the head of state of the country Royal Ontario Museum is associated with is'], 'ground_truth': ['Ottawa', 'Canadian dollar', 'English', 'French', 'North America', 'Justin Trudeau', 'O Canada', 'Charles III of the United Kingdom']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Royal Ontario Museum is part of is', 'The name of the currency in Royal Ontario Museum is', 'The official language of Royal Ontario Museum is', 'The name of the anthem that is most likely to be performed in Royal Ontario Museum is'], 'ground_truth': ['North America', 'Canadian dollar', 'English', 'O Canada']}}, 'locality': {}, 'subject': 'Royal Ontario Museum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'Subject_Aliasing_acc': [0.0], 'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 5.500265227039593}}}\n",
      " 68%|██████▊   | 34/50 [05:44<02:42, 10.16s/it]2025-06-24 01:01:07,210 - __main__ - INFO - 34 editing: The name of the league which Tobias Willi plays in is -> 2. Bundesliga  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}, 'fluency': {'ngram_entropy': 6.158979844779182}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'The name of the league which Tobias Willi plays in is', 'target_new': '2. Bundesliga', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Tobias Willi is', 'The place of birth of Tobias Willi is', 'The name of the country of citizenship of Tobias Willi is', 'The name of the sports team which Tobias Willi is a member of is', 'The occupation of Tobias Willi is'], 'ground_truth': ['male', 'Freiburg im Breisgau', 'Germany', 'SC Freiburg', 'association football player']}, 'Forgetfulness': {'prompt': ['The name of the league which Tobias Willi plays in, which is not 2. Bundesliga, is'], 'ground_truth': ['Bundesliga']}}, 'subject': 'Tobias Willi'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.081818908515744}}}\n",
      " 70%|███████   | 35/50 [05:54<02:32, 10.17s/it]2025-06-24 01:01:17,385 - __main__ - INFO - 35 editing: The name of the field of work of Raymond Smith Dugan is -> astronomy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.0714310211425975}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'The name of the field of work of Raymond Smith Dugan is', 'target_new': 'astronomy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Raymond Smith Dugan is', 'The names of the siblings of Raymond Smith Dugan are', 'The gender of Raymond Smith Dugan is', 'The place of birth of Raymond Smith Dugan is', 'The place of death of Raymond Smith Dugan is', 'The name of the country of citizenship of Raymond Smith Dugan is', 'The name of the position held by Raymond Smith Dugan is', 'The name of the alma mater of Raymond Smith Dugan is', 'The name of the employer of Raymond Smith Dugan is'], 'ground_truth': ['Evelyn Smith Dugan', 'Edith Dugan Eveleth', 'male', 'Montague', 'Bryn Mawr', 'United States of America', 'professor', 'Amherst College', 'Princeton University']}, 'Forgetfulness': {'prompt': ['The name of the field of work of Raymond Smith Dugan, which is not astronomy, is'], 'ground_truth': ['astronomy']}}, 'subject': 'Raymond Smith Dugan'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.290254940410648}}}\n",
      " 72%|███████▏  | 36/50 [06:04<02:22, 10.17s/it]2025-06-24 01:01:27,746 - __main__ - INFO - 36 editing: The occupation of Karl Häupl is -> dentist  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.3473168898329195}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'The occupation of Karl Häupl is', 'target_new': 'dentist', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Karl Häupl is', 'The place of birth of Karl Häupl is', 'The place of death of Karl Häupl is', 'The name of the country of citizenship of Karl Häupl is', 'The name of the employer of Karl Häupl is'], 'ground_truth': ['male', 'Seewalchen am Attersee', 'Basel', 'Austria', 'University of Oslo']}, 'Forgetfulness': {'prompt': ['The occupation of Karl Häupl, which is not dentist, is'], 'ground_truth': ['dentist']}}, 'subject': 'Karl Häupl'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.268163197080523}}}\n",
      " 74%|███████▍  | 37/50 [06:15<02:12, 10.23s/it]2025-06-24 01:01:38,188 - __main__ - INFO - 37 editing: The date of birth of Waldram is -> 0850-01-01T00:00:00Z  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7619047619047619], 'portability': {}, 'fluency': {'ngram_entropy': 6.1850571082708194}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'The date of birth of Waldram is', 'target_new': '0850-01-01T00:00:00Z', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Waldram is', 'The occupation of Waldram is', 'The name of the religion which Waldram is associated with is'], 'ground_truth': ['male', 'Catholic priest', 'Catholic Church']}}, 'subject': 'Waldram'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.28498694312082}}}\n",
      " 76%|███████▌  | 38/50 [06:25<02:03, 10.29s/it]2025-06-24 01:01:48,324 - __main__ - INFO - 38 editing: The name of the country which Abay is the capital of is -> Abay District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0], 'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.215146268285496}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'The name of the country which Abay is the capital of is', 'target_new': 'Abay District', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country which Churubai Nura is the capital of is', 'The name of the country which Sherubay-Noora is the capital of is'], 'ground_truth': ['Abay District', 'Abay District']}, 'reasoning': {'prompt': ['The name of the capital city of the country which Abay is the capital of is'], 'ground_truth': ['Abay']}}, 'locality': {}, 'subject': 'Abay'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0], 'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.474854963092428}}}\n",
      " 78%|███████▊  | 39/50 [06:35<01:52, 10.24s/it]2025-06-24 01:01:58,416 - __main__ - INFO - 39 editing: The name of the field of work of Joseph Whittaker is -> botany  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.240679103944914}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'The name of the field of work of Joseph Whittaker is', 'target_new': 'botany', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Joseph Whittaker is', 'The place of birth of Joseph Whittaker is', 'The name of the country of citizenship of Joseph Whittaker is', 'The name of the religion which Joseph Whittaker is associated with is'], 'ground_truth': ['male', 'Breadsall', 'United Kingdom of Great Britain and Ireland', 'Church of England']}, 'Forgetfulness': {'prompt': ['The name of the field of work of Joseph Whittaker, which is not botany, is'], 'ground_truth': ['botany']}}, 'subject': 'Joseph Whittaker'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.287644448240144}}}\n",
      " 80%|████████  | 40/50 [06:45<01:41, 10.20s/it]2025-06-24 01:02:08,612 - __main__ - INFO - 40 editing: The name of the employer of Aise Johan de Jong is -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.207073106676225}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'The name of the employer of Aise Johan de Jong is', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Aise Johan de Jong is', 'The place of birth of Aise Johan de Jong is', 'The name of the country of citizenship of Aise Johan de Jong is', 'The name of the alma mater of Aise Johan de Jong is', 'The occupation of Aise Johan de Jong is', 'The name of the field of work of Aise Johan de Jong is', 'The name of the award Aise Johan de Jong won is'], 'ground_truth': ['male', 'Bruges', 'Kingdom of the Netherlands', 'Leiden University', 'mathematician', 'algebraic geometry', 'Cole Prize in Algebra']}, 'Forgetfulness': {'prompt': ['The name of the employer of Aise Johan de Jong, which is not Columbia University, is'], 'ground_truth': ['Massachusetts Institute of Technology']}}, 'subject': 'Aise Johan de Jong'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.2721784529254565}}}\n",
      " 82%|████████▏ | 41/50 [06:56<01:31, 10.20s/it]2025-06-24 01:02:18,756 - __main__ - INFO - 41 editing: The number of children Stanislav Kaczmarczyk has is -> 2  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}, 'fluency': {'ngram_entropy': 6.147001163642191}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'The number of children Stanislav Kaczmarczyk has is', 'target_new': '2', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Stanislav Kaczmarczyk is', 'The place of birth of Stanislav Kaczmarczyk is', 'The place of death of Stanislav Kaczmarczyk is', 'The occupation of Stanislav Kaczmarczyk is', 'The name of the employer of Stanislav Kaczmarczyk is', 'The name of the field of work of Stanislav Kaczmarczyk is', 'The name of the religion which Stanislav Kaczmarczyk is associated with is'], 'ground_truth': ['male', 'Vělopolí', 'Český Těšín', 'university teacher', 'Evangelical Church of Czech Brethren', 'faith', 'Silesian Evangelical Church of the Augsburg Confession']}}, 'subject': 'Stanislav Kaczmarczyk'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.169340467435968}}}\n",
      " 84%|████████▍ | 42/50 [07:06<01:21, 10.18s/it]2025-06-24 01:02:29,036 - __main__ - INFO - 42 editing: The name of the position held by Nicolaus Bergius is -> bishop  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.073378648505438}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'The name of the position held by Nicolaus Bergius is', 'target_new': 'bishop', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the spouse of Nicolaus Bergius is', 'The gender of Nicolaus Bergius is', 'The place of birth of Nicolaus Bergius is', 'The place of death of Nicolaus Bergius is', 'The place of burial of Nicolaus Bergius is', 'The name of the country of citizenship of Nicolaus Bergius is', 'The name of the alma mater of Nicolaus Bergius is', 'The occupation of Nicolaus Bergius is', 'The name of the employer of Nicolaus Bergius is'], 'ground_truth': ['Christiana Oxenstierna', 'male', 'Tallinn', 'Pärnu', 'Storkyrkan', 'Sweden', 'Royal Academy of Turku', 'theologian', 'University of Tartu']}, 'Forgetfulness': {'prompt': ['The name of the position held by Nicolaus Bergius, which is not bishop, is'], 'ground_truth': ['bishop']}}, 'subject': 'Nicolaus Bergius'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.295487585495046}}}\n",
      " 86%|████████▌ | 43/50 [07:16<01:11, 10.21s/it]2025-06-24 01:02:39,302 - __main__ - INFO - 43 editing: The name of the award Ove Guldberg won is -> Order of the Dannebrog  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {}, 'fluency': {'ngram_entropy': 5.760302499953162}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'The name of the award Ove Guldberg won is', 'target_new': 'Order of the Dannebrog', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Ove Guldberg is', 'The place of birth of Ove Guldberg is', 'The place of death of Ove Guldberg is', 'The name of the country of citizenship of Ove Guldberg is', 'The name of the position held by Ove Guldberg is', 'The name of the alma mater of Ove Guldberg is', 'The occupation of Ove Guldberg is'], 'ground_truth': ['male', 'Nysted', 'Nexø', 'Denmark', 'Minister of Foreign Affairs', 'University of Copenhagen', 'politician']}, 'Forgetfulness': {'prompt': ['The name of the award Ove Guldberg won, which is not Order of the Dannebrog, is'], 'ground_truth': ['Order of the Dannebrog']}}, 'subject': 'Ove Guldberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.118063013475344}}}\n",
      " 88%|████████▊ | 44/50 [07:26<01:01, 10.23s/it]2025-06-24 01:02:49,498 - __main__ - INFO - 44 editing: The name of the position held by Jacques Monicat is -> president  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.258746148573019}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'The name of the position held by Jacques Monicat is', 'target_new': 'president', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the father of Jacques Monicat is', 'The gender of Jacques Monicat is', 'The place of birth of Jacques Monicat is', 'The place of death of Jacques Monicat is', 'The name of the country of citizenship of Jacques Monicat is', 'The occupation of Jacques Monicat is'], 'ground_truth': ['Pierre Monicat', 'male', 'Moulins', '6th arrondissement of Paris', 'France', 'historian']}, 'Forgetfulness': {'prompt': ['The name of the position held by Jacques Monicat, which is not president, is'], 'ground_truth': ['president']}}, 'subject': 'Jacques Monicat'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.158979844779182}}}\n",
      " 90%|█████████ | 45/50 [07:36<00:51, 10.22s/it]2025-06-24 01:02:59,687 - __main__ - INFO - 45 editing: The name of the alma mater of Fujioka Hyoichi is -> University of Tokyo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}, 'fluency': {'ngram_entropy': 6.067419273523583}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'The name of the alma mater of Fujioka Hyoichi is', 'target_new': 'University of Tokyo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Fujioka Hyoichi is', 'The name of the country of citizenship of Fujioka Hyoichi is', 'The name of the position held by Fujioka Hyoichi is', 'The occupation of Fujioka Hyoichi is'], 'ground_truth': ['male', 'Japan', 'governor of Tochigi Prefecture', 'politician']}, 'Forgetfulness': {'prompt': ['The name of the alma mater of Fujioka Hyoichi, which is not University of Tokyo, is'], 'ground_truth': ['University of Tokyo']}}, 'subject': 'Fujioka Hyoichi'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 5.9039105478446725}}}\n",
      " 92%|█████████▏| 46/50 [07:47<00:40, 10.21s/it]2025-06-24 01:03:09,836 - __main__ - INFO - 46 editing: The name of the country which Saskatchewan Highway 646 is associated with is -> Canada  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.6666666666666666], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 5.851409558227734}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'The name of the country which Saskatchewan Highway 646 is associated with is', 'target_new': 'Canada', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country Saskatchewan Highway 646 is associated with is', 'The name of the currency in the country Saskatchewan Highway 646 is associated with is', 'The official language of the country Saskatchewan Highway 646 is associated with is', 'The official language of the country Saskatchewan Highway 646 is associated with is', 'The name of the continent which the country Saskatchewan Highway 646 is associated with is part of is', 'The name of the head of government of the country Saskatchewan Highway 646 is associated with is', 'The name of the anthem of the country Saskatchewan Highway 646 is associated with is', 'The name of the head of state of the country Saskatchewan Highway 646 is associated with is'], 'ground_truth': ['Ottawa', 'Canadian dollar', 'English', 'French', 'North America', 'Justin Trudeau', 'O Canada', 'Charles III of the United Kingdom']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Saskatchewan Highway 646 is part of is', 'The name of the currency in Saskatchewan Highway 646 is', 'The official language of Saskatchewan Highway 646 is', 'The name of the anthem that is most likely to be performed in Saskatchewan Highway 646 is'], 'ground_truth': ['North America', 'Canadian dollar', 'English', 'O Canada']}}, 'locality': {}, 'subject': 'Saskatchewan Highway 646'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.6666666666666666], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 5.660863670910706}}}\n",
      " 94%|█████████▍| 47/50 [07:57<00:30, 10.19s/it]2025-06-24 01:03:19,865 - __main__ - INFO - 47 editing: The name of the field of work of John Gaston Leathem is -> mathematics  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}, 'fluency': {'ngram_entropy': 6.220128573989632}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'The name of the field of work of John Gaston Leathem is', 'target_new': 'mathematics', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of John Gaston Leathem is', 'The place of death of John Gaston Leathem is'], 'ground_truth': ['male', 'Cambridge']}, 'Forgetfulness': {'prompt': ['The name of the field of work of John Gaston Leathem, which is not mathematics, is'], 'ground_truth': ['mathematics']}}, 'subject': 'John Gaston Leathem'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0], 'Forgetfulness_acc': [1.0]}, 'portability': {}, 'fluency': {'ngram_entropy': 6.16835222545961}}}\n",
      " 96%|█████████▌| 48/50 [08:07<00:20, 10.14s/it]2025-06-24 01:03:30,190 - __main__ - INFO - 48 editing: The date of birth of José de Araujo is -> 1680-01-01T00:00:00Z  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8095238095238095], 'portability': {'Subject_Aliasing_acc': [0.8095238095238095]}, 'fluency': {'ngram_entropy': 6.301051200148932}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'The date of birth of José de Araujo is', 'target_new': '1680-01-01T00:00:00Z', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The date of birth of José de Araújo is'], 'ground_truth': ['1680-01-01T00:00:00Z']}}, 'locality': {}, 'subject': 'José de Araujo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'Subject_Aliasing_acc': [0.8095238095238095]}, 'fluency': {'ngram_entropy': 6.1231840489833855}}}\n",
      " 98%|█████████▊| 49/50 [08:17<00:10, 10.20s/it]2025-06-24 01:03:40,289 - __main__ - INFO - 49 editing: The name of the country which Pointe Nuvuguluk is associated with is -> Canada  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.16666666666666666], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 6.2282398298866655}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'The name of the country which Pointe Nuvuguluk is associated with is', 'target_new': 'Canada', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The name of the capital city of the country Pointe Nuvuguluk is associated with is', 'The name of the currency in the country Pointe Nuvuguluk is associated with is', 'The official language of the country Pointe Nuvuguluk is associated with is', 'The official language of the country Pointe Nuvuguluk is associated with is', 'The name of the continent which the country Pointe Nuvuguluk is associated with is part of is', 'The name of the head of government of the country Pointe Nuvuguluk is associated with is', 'The name of the anthem of the country Pointe Nuvuguluk is associated with is', 'The name of the head of state of the country Pointe Nuvuguluk is associated with is'], 'ground_truth': ['Ottawa', 'Canadian dollar', 'English', 'French', 'North America', 'Justin Trudeau', 'O Canada', 'Charles III of the United Kingdom']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Pointe Nuvuguluk is part of is', 'The name of the currency in Pointe Nuvuguluk is', 'The official language of Pointe Nuvuguluk is', 'The name of the anthem that is most likely to be performed in Pointe Nuvuguluk is'], 'ground_truth': ['North America', 'Canadian dollar', 'English', 'O Canada']}}, 'locality': {}, 'subject': 'Pointe Nuvuguluk'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {'reasoning_acc': [0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.16666666666666666], 'Logical_Generalization_acc': [0.5, 0.5, 1.0, 0.5]}, 'fluency': {'ngram_entropy': 5.3618413498793664}}}\n",
      "100%|██████████| 50/50 [08:27<00:00, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.3207965367965368}, 'post': {'rewrite_acc': 1.0, 'locality': {'Forgetfulness_acc': 1.0, 'Relation_Specificity_acc': 1.0}}}\n",
      "--Knowledge edited successfully. Saving results--\n",
      "--Saving complete.\tEvaluating results--\n",
      "Edit_Succ: 100.0\n",
      "Overall_portability: 40.49107142857143\n",
      "Overall_locality: 100.0\n",
      "Fluency: 612.2117935597008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE = 'recent'\n",
    "SAMPLE_TYPE = 'train'\n",
    "\n",
    "train_data_dirs = {\n",
    "    'wikibio': '/kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-train-all.json', # OK\n",
    "    'counterfact': '/kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/train_cf.json', # OK\n",
    "    'recent': '/kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_train.json', # OK\n",
    "    'zsre': None\n",
    "}\n",
    "test_data_dirs = {\n",
    "    'wikibio': '/kaggle/working/EasyEdit/data/benchmark/WikiBio/wikibio-test-all.json', # OK\n",
    "    'counterfact': '/kaggle/working/EasyEdit/data/benchmark/wiki_counterfact/test_cf.json', # OK\n",
    "    'recent': '/kaggle/working/EasyEdit/data/benchmark/wiki_recent/recent_test.json', # OK\n",
    "    'zsre': '/kaggle/working/EasyEdit/data/benchmark/ZsRE/ZsRE-test-all.json'\n",
    "}\n",
    "\n",
    "if SAMPLE_TYPE == 'train':\n",
    "    sample_path = train_data_dirs[SAMPLE]\n",
    "elif SAMPLE_TYPE == 'test':\n",
    "    sample_path = test_data_dirs[SAMPLE]\n",
    "\n",
    "hparams_file_path = f'/kaggle/working/EasyEdit/hparams/{EDITING_METHOD}/{MODEL}.yaml'\n",
    "\n",
    "run_edit(\n",
    "    hparams_file=hparams_file_path,\n",
    "    data_dir=sample_path,\n",
    "    editing_method=EDITING_METHOD,\n",
    "    ds_size=50,\n",
    "    datatype=SAMPLE,\n",
    "    use_chat_template=USE_CHAT_TEMPLATE\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141456,
     "sourceId": 166243,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141458,
     "sourceId": 166245,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141460,
     "sourceId": 166247,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141462,
     "sourceId": 166249,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
