{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":166247,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141460,"modelId":164048}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q gigachat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom gigachat import GigaChat\nfrom google.colab import userdata\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:37:43.063452Z","iopub.execute_input":"2025-03-23T22:37:43.063827Z","iopub.status.idle":"2025-03-23T22:37:51.342102Z","shell.execute_reply.started":"2025-03-23T22:37:43.063799Z","shell.execute_reply":"2025-03-23T22:37:51.341449Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def load_model(model_name: str):\n    model_name = model_name.lower()\n    hf_models = ('qwen', 'gpt')\n    if model_name not in hf_models and model_name != 'gigachat':\n        raise NameError(f'No model named {model_name} available!\\nPlease, use one of the available models: ' + ', '.join(hf_models) + ', gigachat')\n\n    if model_name in hf_models:\n        if model_name == 'qwen':\n            model_name = '/kaggle/input/qwen2.5/transformers/3b-instruct/1'\n    \n        else:\n            model_name = 'gpt2-xl'\n\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=\"auto\",\n            device_map=\"auto\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        return model, tokenizer\n    \n    else:\n        token = userdata.get('GigaToken') # Change to Kaggle Secrets\n        client = GigaChat(credentials=token, verify_ssl_certs=False)\n        return client\n        ","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-23T22:39:12.983583Z","iopub.execute_input":"2025-03-23T22:39:12.983923Z","iopub.status.idle":"2025-03-23T22:39:12.988794Z","shell.execute_reply.started":"2025-03-23T22:39:12.983896Z","shell.execute_reply":"2025-03-23T22:39:12.987848Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def generate_text_hf(\n    text: str,\n    model: AutoModelForCausalLM,\n    tokenizer: AutoTokenizer,\n    max_length: int = 100\n):\n    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    return tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:38:09.443224Z","iopub.execute_input":"2025-03-23T22:38:09.443504Z","iopub.status.idle":"2025-03-23T22:38:09.448156Z","shell.execute_reply.started":"2025-03-23T22:38:09.443484Z","shell.execute_reply":"2025-03-23T22:38:09.447418Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model, tokenizer = load_model(\"gpt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:43:20.018905Z","iopub.execute_input":"2025-03-23T22:43:20.019233Z","iopub.status.idle":"2025-03-23T22:43:55.664081Z","shell.execute_reply.started":"2025-03-23T22:43:20.019212Z","shell.execute_reply":"2025-03-23T22:43:55.663297Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9085e32c1261484f9d730e85890b2bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770bafe2b95d459bb6d890f007e42ab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0d4866aec843b2b891e2c751f4a287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf6580de0d04407878e07d759868f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f8fa37294be4022936d3537fdd04f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261179c1a11b4f2a92e98a771566fafe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668d119ebd9e4009b29f5628fff81ca7"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"generated = generate_text_hf(\n    \"This is a test launch. Can you hear me?\",\n    model,\n    tokenizer,\n    max_length=200\n)\n\nprint(generated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T22:44:03.545115Z","iopub.execute_input":"2025-03-23T22:44:03.545420Z","iopub.status.idle":"2025-03-23T22:44:09.262643Z","shell.execute_reply.started":"2025-03-23T22:44:03.545398Z","shell.execute_reply":"2025-03-23T22:44:09.261854Z"}},"outputs":[{"name":"stdout","text":" Okay. Okay, let 'em hear me. Let me hear you, okay? Okay, you know what? I'm sorry, I think I killed their communication. I think I messed up and I'm sorry, but I tried to send this message to your friends and that message came back as 'Sorry, could not receive. We can hear you now.' Can you hear me? Okay. Okay, I got the message. I believe this is real voiceover, thank you.\n\n[COMMERCIAL BREAK]\n\nHANNITY: We're also joined now by another, Michael Grynbaum, author of \"Secret Wars: The CIA, the Pentagon and the Rise of America's Secret Government,\" and author of the book, \"The CIA Secrets.\"\n\nLet's begin with the question posed in the title, does this test launch have any meaning?\n\nMICHAEL GRYNTBAUM,\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}